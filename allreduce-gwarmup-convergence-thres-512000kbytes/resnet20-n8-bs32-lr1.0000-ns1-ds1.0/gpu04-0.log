2022-08-04 16:32:33,135 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=32, compressor='topk', data_dir='./data', dataset='cifar10', density=1.0, dnn='resnet20', lr=1.0, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 16:33:22,793 [dl_trainer.py:254] INFO num_batches_per_epoch: 196
2022-08-04 16:33:23,009 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 16:33:23,014 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 16:33:23,014 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 16:33:23,014 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 16:33:23,823 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 16:33:28,650 [dl_trainer.py:731] WARNING [  0][   40/  196][rank:0] loss: 1.909, average forward (0.077436) and backward (0.026005) time: 0.105948, iotime: 0.001481 
2022-08-04 16:33:28,706 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.119069, Speed: 268.752505 images/s
2022-08-04 16:33:30,420 [dl_trainer.py:731] WARNING [  0][   80/  196][rank:0] loss: 1.904, average forward (0.010917) and backward (0.020473) time: 0.033246, iotime: 0.001595 
2022-08-04 16:33:30,468 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044037, Speed: 726.657502 images/s
2022-08-04 16:33:32,191 [dl_trainer.py:731] WARNING [  0][  120/  196][rank:0] loss: 1.739, average forward (0.009962) and backward (0.021138) time: 0.032835, iotime: 0.001477 
2022-08-04 16:33:32,243 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044367, Speed: 721.256346 images/s
2022-08-04 16:33:34,009 [dl_trainer.py:731] WARNING [  0][  160/  196][rank:0] loss: 1.710, average forward (0.009566) and backward (0.018481) time: 0.029674, iotime: 0.001381 
2022-08-04 16:33:34,078 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045865, Speed: 697.694976 images/s
2022-08-04 16:33:35,594 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 1078
2022-08-04 16:33:35,594 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:33:35,802 [dl_trainer.py:634] INFO train iter: 196, num_batches_per_epoch: 196
2022-08-04 16:33:35,802 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 35.698342, lr: 0.200816, avg loss: 1.762524
2022-08-04 16:33:38,508 [dl_trainer.py:822] INFO Epoch 1, lr: 0.200816, val loss: 2.062746, val top-1 acc: 35.583067, top-5 acc: 86.142173
2022-08-04 16:33:38,703 [dl_trainer.py:731] WARNING [  1][  200/  196][rank:0] loss: 1.433, average forward (0.009434) and backward (0.020721) time: 0.103092, iotime: 0.004958 
2022-08-04 16:33:40,289 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082726, Speed: 386.820170 images/s
2022-08-04 16:33:40,452 [dl_trainer.py:731] WARNING [  1][  240/  196][rank:0] loss: 1.770, average forward (0.011238) and backward (0.018354) time: 0.031442, iotime: 0.001574 
2022-08-04 16:33:42,102 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045300, Speed: 706.400915 images/s
2022-08-04 16:33:42,281 [dl_trainer.py:731] WARNING [  1][  280/  196][rank:0] loss: 0.875, average forward (0.010279) and backward (0.020364) time: 0.032428, iotime: 0.001524 
2022-08-04 16:33:43,893 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044778, Speed: 714.642814 images/s
2022-08-04 16:33:44,044 [dl_trainer.py:731] WARNING [  1][  320/  196][rank:0] loss: 1.350, average forward (0.009343) and backward (0.019741) time: 0.030668, iotime: 0.001357 
2022-08-04 16:33:45,602 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.042718, Speed: 749.091542 images/s
2022-08-04 16:33:45,773 [dl_trainer.py:731] WARNING [  1][  360/  196][rank:0] loss: 0.999, average forward (0.009619) and backward (0.017968) time: 0.029210, iotime: 0.001384 
2022-08-04 16:33:47,169 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:33:47,169 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:33:47,432 [dl_trainer.py:634] INFO train iter: 392, num_batches_per_epoch: 196
2022-08-04 16:33:47,433 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 55.787628, lr: 0.400612, avg loss: 1.219936
2022-08-04 16:33:50,104 [dl_trainer.py:822] INFO Epoch 2, lr: 0.400612, val loss: 1.652404, val top-1 acc: 49.860224, top-5 acc: 94.219249
2022-08-04 16:33:50,500 [dl_trainer.py:731] WARNING [  2][  400/  196][rank:0] loss: 0.957, average forward (0.008933) and backward (0.020215) time: 0.102945, iotime: 0.005538 
2022-08-04 16:33:51,855 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083349, Speed: 383.928383 images/s
2022-08-04 16:33:52,215 [dl_trainer.py:731] WARNING [  2][  440/  196][rank:0] loss: 0.903, average forward (0.009382) and backward (0.018573) time: 0.029578, iotime: 0.001400 
2022-08-04 16:33:53,496 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.041014, Speed: 780.222904 images/s
2022-08-04 16:33:53,881 [dl_trainer.py:731] WARNING [  2][  480/  196][rank:0] loss: 1.255, average forward (0.009782) and backward (0.017035) time: 0.028503, iotime: 0.001454 
2022-08-04 16:33:55,199 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.042583, Speed: 751.470110 images/s
2022-08-04 16:33:55,585 [dl_trainer.py:731] WARNING [  2][  520/  196][rank:0] loss: 0.763, average forward (0.008948) and backward (0.018303) time: 0.028767, iotime: 0.001292 
2022-08-04 16:33:56,845 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.041130, Speed: 778.013687 images/s
2022-08-04 16:33:57,211 [dl_trainer.py:731] WARNING [  2][  560/  196][rank:0] loss: 1.038, average forward (0.008993) and backward (0.018389) time: 0.028898, iotime: 0.001296 
2022-08-04 16:33:58,276 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:33:58,276 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:33:58,593 [dl_trainer.py:634] INFO train iter: 588, num_batches_per_epoch: 196
2022-08-04 16:33:58,595 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 65.832270, lr: 0.600408, avg loss: 0.965237
2022-08-04 16:34:01,258 [dl_trainer.py:822] INFO Epoch 3, lr: 0.600408, val loss: 0.967861, val top-1 acc: 68.630192, top-5 acc: 96.805112
2022-08-04 16:34:01,774 [dl_trainer.py:731] WARNING [  3][  600/  196][rank:0] loss: 1.214, average forward (0.009201) and backward (0.017574) time: 0.099053, iotime: 0.005351 
2022-08-04 16:34:02,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081154, Speed: 394.311524 images/s
2022-08-04 16:34:03,523 [dl_trainer.py:731] WARNING [  3][  640/  196][rank:0] loss: 0.718, average forward (0.009197) and backward (0.019360) time: 0.030127, iotime: 0.001334 
2022-08-04 16:34:04,680 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043660, Speed: 732.940179 images/s
2022-08-04 16:34:05,228 [dl_trainer.py:731] WARNING [  3][  680/  196][rank:0] loss: 0.748, average forward (0.009939) and backward (0.019373) time: 0.030967, iotime: 0.001420 
2022-08-04 16:34:06,374 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.042349, Speed: 755.618129 images/s
2022-08-04 16:34:06,982 [dl_trainer.py:731] WARNING [  3][  720/  196][rank:0] loss: 0.741, average forward (0.008225) and backward (0.018679) time: 0.028389, iotime: 0.001267 
2022-08-04 16:34:08,210 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045886, Speed: 697.380223 images/s
2022-08-04 16:34:08,804 [dl_trainer.py:731] WARNING [  3][  760/  196][rank:0] loss: 0.816, average forward (0.008654) and backward (0.020759) time: 0.031015, iotime: 0.001373 
2022-08-04 16:34:09,680 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:34:09,681 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:34:10,027 [dl_trainer.py:634] INFO train iter: 784, num_batches_per_epoch: 196
2022-08-04 16:34:10,028 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 70.519770, lr: 0.800204, avg loss: 0.831773
2022-08-04 16:34:12,728 [dl_trainer.py:822] INFO Epoch 4, lr: 0.800204, val loss: 0.880714, val top-1 acc: 70.147764, top-5 acc: 97.873403
2022-08-04 16:34:13,452 [dl_trainer.py:731] WARNING [  4][  800/  196][rank:0] loss: 0.880, average forward (0.010453) and backward (0.019408) time: 0.104380, iotime: 0.005528 
2022-08-04 16:34:14,405 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082585, Speed: 387.481889 images/s
2022-08-04 16:34:15,226 [dl_trainer.py:731] WARNING [  4][  840/  196][rank:0] loss: 0.598, average forward (0.009744) and backward (0.019017) time: 0.030413, iotime: 0.001412 
2022-08-04 16:34:16,174 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044193, Speed: 724.102638 images/s
2022-08-04 16:34:17,036 [dl_trainer.py:731] WARNING [  4][  880/  196][rank:0] loss: 0.648, average forward (0.010532) and backward (0.019725) time: 0.032017, iotime: 0.001503 
2022-08-04 16:34:18,043 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046727, Speed: 684.829137 images/s
2022-08-04 16:34:18,914 [dl_trainer.py:731] WARNING [  4][  920/  196][rank:0] loss: 0.538, average forward (0.009135) and backward (0.021581) time: 0.032317, iotime: 0.001363 
2022-08-04 16:34:19,894 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046259, Speed: 691.763461 images/s
2022-08-04 16:34:20,749 [dl_trainer.py:731] WARNING [  4][  960/  196][rank:0] loss: 0.669, average forward (0.010462) and backward (0.019576) time: 0.031814, iotime: 0.001526 
2022-08-04 16:34:21,445 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:34:21,445 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:34:21,825 [dl_trainer.py:634] INFO train iter: 980, num_batches_per_epoch: 196
2022-08-04 16:34:21,826 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 73.469388, lr: 1.000000, avg loss: 0.771245
2022-08-04 16:34:24,508 [dl_trainer.py:822] INFO Epoch 5, lr: 1.000000, val loss: 0.930627, val top-1 acc: 69.648562, top-5 acc: 97.284345
2022-08-04 16:34:25,380 [dl_trainer.py:731] WARNING [  5][ 1000/  196][rank:0] loss: 0.915, average forward (0.010058) and backward (0.020194) time: 0.103280, iotime: 0.005637 
2022-08-04 16:34:26,107 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082821, Speed: 386.374470 images/s
2022-08-04 16:34:27,225 [dl_trainer.py:731] WARNING [  5][ 1040/  196][rank:0] loss: 0.827, average forward (0.011146) and backward (0.019926) time: 0.032957, iotime: 0.001613 
2022-08-04 16:34:27,931 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045583, Speed: 702.018156 images/s
2022-08-04 16:34:28,941 [dl_trainer.py:731] WARNING [  5][ 1080/  196][rank:0] loss: 0.944, average forward (0.009407) and backward (0.018930) time: 0.029951, iotime: 0.001387 
2022-08-04 16:34:29,695 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044092, Speed: 725.758583 images/s
2022-08-04 16:34:30,742 [dl_trainer.py:731] WARNING [  5][ 1120/  196][rank:0] loss: 0.786, average forward (0.009792) and backward (0.018310) time: 0.029774, iotime: 0.001441 
2022-08-04 16:34:31,436 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043519, Speed: 735.311480 images/s
2022-08-04 16:34:32,443 [dl_trainer.py:731] WARNING [  5][ 1160/  196][rank:0] loss: 0.516, average forward (0.009979) and backward (0.017738) time: 0.029360, iotime: 0.001409 
2022-08-04 16:34:32,904 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:34:32,904 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:34:33,354 [dl_trainer.py:634] INFO train iter: 1176, num_batches_per_epoch: 196
2022-08-04 16:34:33,355 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 75.446429, lr: 1.000000, avg loss: 0.695917
2022-08-04 16:34:36,039 [dl_trainer.py:822] INFO Epoch 6, lr: 1.000000, val loss: 0.791018, val top-1 acc: 74.011581, top-5 acc: 97.683706
2022-08-04 16:34:37,139 [dl_trainer.py:731] WARNING [  6][ 1200/  196][rank:0] loss: 0.596, average forward (0.010705) and backward (0.018324) time: 0.102718, iotime: 0.005492 
2022-08-04 16:34:37,669 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083086, Speed: 385.143483 images/s
2022-08-04 16:34:38,999 [dl_trainer.py:731] WARNING [  6][ 1240/  196][rank:0] loss: 0.427, average forward (0.010331) and backward (0.017521) time: 0.029572, iotime: 0.001464 
2022-08-04 16:34:39,531 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046552, Speed: 687.403386 images/s
2022-08-04 16:34:40,823 [dl_trainer.py:731] WARNING [  6][ 1280/  196][rank:0] loss: 0.477, average forward (0.009722) and backward (0.017891) time: 0.029268, iotime: 0.001400 
2022-08-04 16:34:41,357 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045614, Speed: 701.541321 images/s
2022-08-04 16:34:42,597 [dl_trainer.py:731] WARNING [  6][ 1320/  196][rank:0] loss: 0.874, average forward (0.009833) and backward (0.018039) time: 0.029507, iotime: 0.001403 
2022-08-04 16:34:43,084 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043186, Speed: 740.977959 images/s
2022-08-04 16:34:44,366 [dl_trainer.py:731] WARNING [  6][ 1360/  196][rank:0] loss: 0.568, average forward (0.009495) and backward (0.016779) time: 0.027886, iotime: 0.001385 
2022-08-04 16:34:44,610 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:34:44,610 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:34:45,134 [dl_trainer.py:634] INFO train iter: 1372, num_batches_per_epoch: 196
2022-08-04 16:34:45,135 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 78.093112, lr: 1.000000, avg loss: 0.644294
2022-08-04 16:34:47,781 [dl_trainer.py:822] INFO Epoch 7, lr: 1.000000, val loss: 0.672216, val top-1 acc: 76.787141, top-5 acc: 98.492412
2022-08-04 16:34:49,114 [dl_trainer.py:731] WARNING [  7][ 1400/  196][rank:0] loss: 0.577, average forward (0.011233) and backward (0.020875) time: 0.104605, iotime: 0.005973 
2022-08-04 16:34:49,423 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084487, Speed: 378.756811 images/s
2022-08-04 16:34:51,030 [dl_trainer.py:731] WARNING [  7][ 1440/  196][rank:0] loss: 0.443, average forward (0.009318) and backward (0.017329) time: 0.028303, iotime: 0.001399 
2022-08-04 16:34:51,303 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047008, Speed: 680.732578 images/s
2022-08-04 16:34:52,872 [dl_trainer.py:731] WARNING [  7][ 1480/  196][rank:0] loss: 0.710, average forward (0.009622) and backward (0.021092) time: 0.032394, iotime: 0.001436 
2022-08-04 16:34:53,184 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046998, Speed: 680.884439 images/s
2022-08-04 16:34:54,686 [dl_trainer.py:731] WARNING [  7][ 1520/  196][rank:0] loss: 0.649, average forward (0.009141) and backward (0.018501) time: 0.029322, iotime: 0.001452 
2022-08-04 16:34:54,988 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045098, Speed: 709.569949 images/s
2022-08-04 16:34:56,566 [dl_trainer.py:731] WARNING [  7][ 1560/  196][rank:0] loss: 0.473, average forward (0.009296) and backward (0.020801) time: 0.031818, iotime: 0.001484 
2022-08-04 16:34:56,578 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:34:56,578 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:34:57,140 [dl_trainer.py:634] INFO train iter: 1568, num_batches_per_epoch: 196
2022-08-04 16:34:57,141 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 78.252551, lr: 1.000000, avg loss: 0.626548
2022-08-04 16:34:59,815 [dl_trainer.py:822] INFO Epoch 8, lr: 1.000000, val loss: 0.626285, val top-1 acc: 79.193291, top-5 acc: 98.632188
2022-08-04 16:35:01,310 [dl_trainer.py:731] WARNING [  8][ 1600/  196][rank:0] loss: 0.775, average forward (0.011386) and backward (0.020585) time: 0.105665, iotime: 0.005962 
2022-08-04 16:35:01,378 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085178, Speed: 375.682978 images/s
2022-08-04 16:35:03,155 [dl_trainer.py:731] WARNING [  8][ 1640/  196][rank:0] loss: 0.428, average forward (0.009289) and backward (0.019048) time: 0.029951, iotime: 0.001370 
2022-08-04 16:35:03,204 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045640, Speed: 701.135266 images/s
2022-08-04 16:35:04,984 [dl_trainer.py:731] WARNING [  8][ 1680/  196][rank:0] loss: 0.342, average forward (0.010975) and backward (0.020129) time: 0.033045, iotime: 0.001660 
2022-08-04 16:35:05,045 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046029, Speed: 695.212784 images/s
2022-08-04 16:35:06,788 [dl_trainer.py:731] WARNING [  8][ 1720/  196][rank:0] loss: 0.323, average forward (0.009281) and backward (0.019553) time: 0.030406, iotime: 0.001335 
2022-08-04 16:35:06,853 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045182, Speed: 708.242698 images/s
2022-08-04 16:35:08,443 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:35:08,443 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:35:08,841 [dl_trainer.py:731] WARNING [  8][ 1760/  196][rank:0] loss: 0.459, average forward (0.010654) and backward (0.021033) time: 0.037810, iotime: 0.005870 
2022-08-04 16:35:09,057 [dl_trainer.py:634] INFO train iter: 1764, num_batches_per_epoch: 196
2022-08-04 16:35:09,058 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 80.022321, lr: 1.000000, avg loss: 0.585321
2022-08-04 16:35:11,732 [dl_trainer.py:822] INFO Epoch 9, lr: 1.000000, val loss: 0.874767, val top-1 acc: 75.139776, top-5 acc: 97.623802
2022-08-04 16:35:13,260 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085407, Speed: 374.676258 images/s
2022-08-04 16:35:13,415 [dl_trainer.py:731] WARNING [  9][ 1800/  196][rank:0] loss: 0.829, average forward (0.009502) and backward (0.020199) time: 0.098380, iotime: 0.001479 
2022-08-04 16:35:15,131 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046761, Speed: 684.326421 images/s
2022-08-04 16:35:15,321 [dl_trainer.py:731] WARNING [  9][ 1840/  196][rank:0] loss: 0.950, average forward (0.010694) and backward (0.020843) time: 0.033457, iotime: 0.001646 
2022-08-04 16:35:16,972 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045995, Speed: 695.720809 images/s
2022-08-04 16:35:17,140 [dl_trainer.py:731] WARNING [  9][ 1880/  196][rank:0] loss: 0.396, average forward (0.010101) and backward (0.018700) time: 0.030566, iotime: 0.001523 
2022-08-04 16:35:18,751 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044476, Speed: 719.482181 images/s
2022-08-04 16:35:18,916 [dl_trainer.py:731] WARNING [  9][ 1920/  196][rank:0] loss: 0.476, average forward (0.010740) and backward (0.019404) time: 0.031986, iotime: 0.001582 
2022-08-04 16:35:20,355 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:35:20,355 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:35:20,962 [dl_trainer.py:731] WARNING [  9][ 1960/  196][rank:0] loss: 0.294, average forward (0.009783) and backward (0.020463) time: 0.036543, iotime: 0.006048 
2022-08-04 16:35:20,979 [dl_trainer.py:634] INFO train iter: 1960, num_batches_per_epoch: 196
2022-08-04 16:35:20,980 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 80.739796, lr: 1.000000, avg loss: 0.551681
2022-08-04 16:35:23,630 [dl_trainer.py:822] INFO Epoch 10, lr: 1.000000, val loss: 0.733598, val top-1 acc: 75.309505, top-5 acc: 98.632188
2022-08-04 16:35:25,121 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084900, Speed: 376.911825 images/s
2022-08-04 16:35:25,512 [dl_trainer.py:731] WARNING [ 10][ 2000/  196][rank:0] loss: 0.708, average forward (0.010049) and backward (0.020360) time: 0.098926, iotime: 0.001503 
2022-08-04 16:35:27,026 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047621, Speed: 671.973209 images/s
2022-08-04 16:35:27,416 [dl_trainer.py:731] WARNING [ 10][ 2040/  196][rank:0] loss: 0.498, average forward (0.009208) and backward (0.019191) time: 0.030115, iotime: 0.001477 
2022-08-04 16:35:28,843 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045412, Speed: 704.660794 images/s
2022-08-04 16:35:29,251 [dl_trainer.py:731] WARNING [ 10][ 2080/  196][rank:0] loss: 0.887, average forward (0.008827) and backward (0.019106) time: 0.029571, iotime: 0.001404 
2022-08-04 16:35:30,704 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046519, Speed: 687.895298 images/s
2022-08-04 16:35:31,090 [dl_trainer.py:731] WARNING [ 10][ 2120/  196][rank:0] loss: 0.422, average forward (0.009269) and backward (0.020557) time: 0.031438, iotime: 0.001380 
2022-08-04 16:35:32,281 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:35:32,282 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:35:32,975 [dl_trainer.py:634] INFO train iter: 2156, num_batches_per_epoch: 196
2022-08-04 16:35:32,977 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 82.079082, lr: 1.000000, avg loss: 0.522049
2022-08-04 16:35:35,628 [dl_trainer.py:822] INFO Epoch 11, lr: 1.000000, val loss: 0.638218, val top-1 acc: 78.674121, top-5 acc: 99.041534
2022-08-04 16:35:35,801 [dl_trainer.py:731] WARNING [ 11][ 2160/  196][rank:0] loss: 0.458, average forward (0.010619) and backward (0.020679) time: 0.103774, iotime: 0.005837 
2022-08-04 16:35:37,025 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084258, Speed: 379.785337 images/s
2022-08-04 16:35:37,645 [dl_trainer.py:731] WARNING [ 11][ 2200/  196][rank:0] loss: 0.798, average forward (0.008880) and backward (0.017998) time: 0.028526, iotime: 0.001413 
2022-08-04 16:35:38,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045529, Speed: 702.842079 images/s
2022-08-04 16:35:39,466 [dl_trainer.py:731] WARNING [ 11][ 2240/  196][rank:0] loss: 0.704, average forward (0.010538) and backward (0.019518) time: 0.031852, iotime: 0.001540 
2022-08-04 16:35:40,704 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046409, Speed: 689.522950 images/s
2022-08-04 16:35:41,339 [dl_trainer.py:731] WARNING [ 11][ 2280/  196][rank:0] loss: 0.489, average forward (0.011177) and backward (0.021281) time: 0.034349, iotime: 0.001620 
2022-08-04 16:35:42,600 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047374, Speed: 675.482740 images/s
2022-08-04 16:35:43,244 [dl_trainer.py:731] WARNING [ 11][ 2320/  196][rank:0] loss: 0.424, average forward (0.010309) and backward (0.022021) time: 0.034041, iotime: 0.001468 
2022-08-04 16:35:44,180 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:35:44,181 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:35:44,939 [dl_trainer.py:634] INFO train iter: 2352, num_batches_per_epoch: 196
2022-08-04 16:35:44,940 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 81.361607, lr: 1.000000, avg loss: 0.532447
2022-08-04 16:35:47,619 [dl_trainer.py:822] INFO Epoch 12, lr: 1.000000, val loss: 0.646972, val top-1 acc: 78.115016, top-5 acc: 98.851837
2022-08-04 16:35:47,975 [dl_trainer.py:731] WARNING [ 12][ 2360/  196][rank:0] loss: 0.361, average forward (0.009594) and backward (0.021275) time: 0.104154, iotime: 0.005675 
2022-08-04 16:35:48,961 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084795, Speed: 377.381938 images/s
2022-08-04 16:35:49,860 [dl_trainer.py:731] WARNING [ 12][ 2400/  196][rank:0] loss: 0.515, average forward (0.010197) and backward (0.019598) time: 0.031581, iotime: 0.001531 
2022-08-04 16:35:50,834 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046831, Speed: 683.302936 images/s
2022-08-04 16:35:51,697 [dl_trainer.py:731] WARNING [ 12][ 2440/  196][rank:0] loss: 0.674, average forward (0.010217) and backward (0.018253) time: 0.030212, iotime: 0.001494 
2022-08-04 16:35:52,710 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046896, Speed: 682.356998 images/s
2022-08-04 16:35:53,576 [dl_trainer.py:731] WARNING [ 12][ 2480/  196][rank:0] loss: 0.570, average forward (0.011192) and backward (0.020145) time: 0.033246, iotime: 0.001636 
2022-08-04 16:35:54,589 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046956, Speed: 681.490139 images/s
2022-08-04 16:35:55,452 [dl_trainer.py:731] WARNING [ 12][ 2520/  196][rank:0] loss: 0.488, average forward (0.010218) and backward (0.019970) time: 0.031918, iotime: 0.001483 
2022-08-04 16:35:56,188 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:35:56,188 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:35:56,941 [dl_trainer.py:634] INFO train iter: 2548, num_batches_per_epoch: 196
2022-08-04 16:35:56,942 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 82.318240, lr: 1.000000, avg loss: 0.523279
2022-08-04 16:35:59,624 [dl_trainer.py:822] INFO Epoch 13, lr: 1.000000, val loss: 0.689122, val top-1 acc: 77.216454, top-5 acc: 98.662141
2022-08-04 16:36:00,138 [dl_trainer.py:731] WARNING [ 13][ 2560/  196][rank:0] loss: 0.754, average forward (0.010176) and backward (0.020128) time: 0.103266, iotime: 0.005547 
2022-08-04 16:36:00,874 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083783, Speed: 381.938766 images/s
2022-08-04 16:36:01,946 [dl_trainer.py:731] WARNING [ 13][ 2600/  196][rank:0] loss: 0.321, average forward (0.010358) and backward (0.018377) time: 0.030522, iotime: 0.001534 
2022-08-04 16:36:02,708 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045834, Speed: 698.164053 images/s
2022-08-04 16:36:03,822 [dl_trainer.py:731] WARNING [ 13][ 2640/  196][rank:0] loss: 0.805, average forward (0.009483) and backward (0.020232) time: 0.031436, iotime: 0.001468 
2022-08-04 16:36:04,608 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047485, Speed: 673.892115 images/s
2022-08-04 16:36:05,729 [dl_trainer.py:731] WARNING [ 13][ 2680/  196][rank:0] loss: 0.433, average forward (0.011027) and backward (0.021283) time: 0.034230, iotime: 0.001645 
2022-08-04 16:36:06,488 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046993, Speed: 680.957588 images/s
2022-08-04 16:36:07,623 [dl_trainer.py:731] WARNING [ 13][ 2720/  196][rank:0] loss: 0.563, average forward (0.011245) and backward (0.021583) time: 0.034730, iotime: 0.001635 
2022-08-04 16:36:08,126 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:36:08,126 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:36:08,949 [dl_trainer.py:634] INFO train iter: 2744, num_batches_per_epoch: 196
2022-08-04 16:36:08,950 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 83.131378, lr: 1.000000, avg loss: 0.514122
2022-08-04 16:36:11,618 [dl_trainer.py:822] INFO Epoch 14, lr: 1.000000, val loss: 0.647430, val top-1 acc: 79.323083, top-5 acc: 98.781949
2022-08-04 16:36:12,405 [dl_trainer.py:731] WARNING [ 14][ 2760/  196][rank:0] loss: 0.441, average forward (0.009841) and backward (0.019086) time: 0.102744, iotime: 0.005683 
2022-08-04 16:36:12,919 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085726, Speed: 373.283673 images/s
2022-08-04 16:36:14,290 [dl_trainer.py:731] WARNING [ 14][ 2800/  196][rank:0] loss: 0.519, average forward (0.012121) and backward (0.021547) time: 0.035652, iotime: 0.001691 
2022-08-04 16:36:14,825 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047629, Speed: 671.859515 images/s
2022-08-04 16:36:16,179 [dl_trainer.py:731] WARNING [ 14][ 2840/  196][rank:0] loss: 0.545, average forward (0.010636) and backward (0.021934) time: 0.034311, iotime: 0.001483 
2022-08-04 16:36:16,704 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046978, Speed: 681.167017 images/s
2022-08-04 16:36:18,063 [dl_trainer.py:731] WARNING [ 14][ 2880/  196][rank:0] loss: 0.411, average forward (0.009925) and backward (0.021695) time: 0.033360, iotime: 0.001477 
2022-08-04 16:36:18,616 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047780, Speed: 669.733685 images/s
2022-08-04 16:36:19,968 [dl_trainer.py:731] WARNING [ 14][ 2920/  196][rank:0] loss: 0.499, average forward (0.010553) and backward (0.019697) time: 0.031958, iotime: 0.001446 
2022-08-04 16:36:20,241 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:36:20,241 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:36:21,115 [dl_trainer.py:634] INFO train iter: 2940, num_batches_per_epoch: 196
2022-08-04 16:36:21,116 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 82.445791, lr: 1.000000, avg loss: 0.528027
2022-08-04 16:36:23,793 [dl_trainer.py:822] INFO Epoch 15, lr: 1.000000, val loss: 0.685959, val top-1 acc: 77.116613, top-5 acc: 98.781949
2022-08-04 16:36:24,703 [dl_trainer.py:731] WARNING [ 15][ 2960/  196][rank:0] loss: 0.652, average forward (0.009320) and backward (0.021293) time: 0.103292, iotime: 0.005438 
2022-08-04 16:36:24,996 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085051, Speed: 376.244605 images/s
2022-08-04 16:36:26,601 [dl_trainer.py:731] WARNING [ 15][ 3000/  196][rank:0] loss: 0.138, average forward (0.010306) and backward (0.020992) time: 0.033110, iotime: 0.001550 
2022-08-04 16:36:26,900 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047581, Speed: 672.530461 images/s
2022-08-04 16:36:28,449 [dl_trainer.py:731] WARNING [ 15][ 3040/  196][rank:0] loss: 0.678, average forward (0.009602) and backward (0.020521) time: 0.031849, iotime: 0.001489 
2022-08-04 16:36:28,755 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046362, Speed: 690.219453 images/s
2022-08-04 16:36:30,302 [dl_trainer.py:731] WARNING [ 15][ 3080/  196][rank:0] loss: 0.385, average forward (0.009142) and backward (0.018547) time: 0.029345, iotime: 0.001419 
2022-08-04 16:36:30,591 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045888, Speed: 697.344352 images/s
2022-08-04 16:36:32,173 [dl_trainer.py:731] WARNING [ 15][ 3120/  196][rank:0] loss: 0.430, average forward (0.008663) and backward (0.019082) time: 0.029348, iotime: 0.001373 
2022-08-04 16:36:32,190 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:36:32,190 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:36:33,145 [dl_trainer.py:634] INFO train iter: 3136, num_batches_per_epoch: 196
2022-08-04 16:36:33,146 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 82.573342, lr: 1.000000, avg loss: 0.515175
2022-08-04 16:36:35,858 [dl_trainer.py:822] INFO Epoch 16, lr: 1.000000, val loss: 0.734449, val top-1 acc: 77.456070, top-5 acc: 98.252796
2022-08-04 16:36:37,080 [dl_trainer.py:731] WARNING [ 16][ 3160/  196][rank:0] loss: 0.278, average forward (0.009330) and backward (0.016806) time: 0.102688, iotime: 0.005659 
2022-08-04 16:36:37,144 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087356, Speed: 366.316026 images/s
2022-08-04 16:36:38,980 [dl_trainer.py:731] WARNING [ 16][ 3200/  196][rank:0] loss: 0.442, average forward (0.009565) and backward (0.020632) time: 0.031958, iotime: 0.001499 
2022-08-04 16:36:39,035 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047256, Speed: 677.162107 images/s
2022-08-04 16:36:40,786 [dl_trainer.py:731] WARNING [ 16][ 3240/  196][rank:0] loss: 0.340, average forward (0.011699) and backward (0.018897) time: 0.032537, iotime: 0.001670 
2022-08-04 16:36:40,858 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045582, Speed: 702.033670 images/s
2022-08-04 16:36:42,742 [dl_trainer.py:731] WARNING [ 16][ 3280/  196][rank:0] loss: 0.408, average forward (0.009908) and backward (0.021801) time: 0.033516, iotime: 0.001548 
2022-08-04 16:36:42,802 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048564, Speed: 658.924240 images/s
2022-08-04 16:36:44,449 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:36:44,449 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:36:44,838 [dl_trainer.py:731] WARNING [ 16][ 3320/  196][rank:0] loss: 0.507, average forward (0.011873) and backward (0.020904) time: 0.038985, iotime: 0.005913 
2022-08-04 16:36:45,452 [dl_trainer.py:634] INFO train iter: 3332, num_batches_per_epoch: 196
2022-08-04 16:36:45,452 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 82.748724, lr: 1.000000, avg loss: 0.503161
2022-08-04 16:36:48,145 [dl_trainer.py:822] INFO Epoch 17, lr: 1.000000, val loss: 0.938057, val top-1 acc: 72.374201, top-5 acc: 98.322684
2022-08-04 16:36:49,265 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086154, Speed: 371.427004 images/s
2022-08-04 16:36:49,428 [dl_trainer.py:731] WARNING [ 17][ 3360/  196][rank:0] loss: 0.521, average forward (0.009175) and backward (0.019636) time: 0.097817, iotime: 0.001395 
2022-08-04 16:36:51,123 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046430, Speed: 689.212075 images/s
2022-08-04 16:36:51,298 [dl_trainer.py:731] WARNING [ 17][ 3400/  196][rank:0] loss: 0.374, average forward (0.009885) and backward (0.020012) time: 0.031574, iotime: 0.001443 
2022-08-04 16:36:52,939 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045392, Speed: 704.964196 images/s
2022-08-04 16:36:53,118 [dl_trainer.py:731] WARNING [ 17][ 3440/  196][rank:0] loss: 0.290, average forward (0.009266) and backward (0.016923) time: 0.027948, iotime: 0.001515 
2022-08-04 16:36:54,702 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044063, Speed: 726.226183 images/s
2022-08-04 16:36:54,878 [dl_trainer.py:731] WARNING [ 17][ 3480/  196][rank:0] loss: 0.515, average forward (0.010066) and backward (0.018270) time: 0.030053, iotime: 0.001480 
2022-08-04 16:36:56,304 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:36:56,304 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:36:56,986 [dl_trainer.py:731] WARNING [ 17][ 3520/  196][rank:0] loss: 0.362, average forward (0.010888) and backward (0.019359) time: 0.036242, iotime: 0.005736 
2022-08-04 16:36:57,373 [dl_trainer.py:634] INFO train iter: 3528, num_batches_per_epoch: 196
2022-08-04 16:36:57,373 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 82.605230, lr: 1.000000, avg loss: 0.513142
2022-08-04 16:37:00,027 [dl_trainer.py:822] INFO Epoch 18, lr: 1.000000, val loss: 0.532041, val top-1 acc: 81.918930, top-5 acc: 98.921725
2022-08-04 16:37:01,205 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086688, Speed: 369.138982 images/s
2022-08-04 16:37:01,620 [dl_trainer.py:731] WARNING [ 18][ 3560/  196][rank:0] loss: 0.257, average forward (0.009810) and backward (0.022134) time: 0.101507, iotime: 0.001484 
2022-08-04 16:37:03,076 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046768, Speed: 684.235106 images/s
2022-08-04 16:37:03,474 [dl_trainer.py:731] WARNING [ 18][ 3600/  196][rank:0] loss: 0.301, average forward (0.009906) and backward (0.017185) time: 0.028956, iotime: 0.001606 
2022-08-04 16:37:04,972 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047395, Speed: 675.182864 images/s
2022-08-04 16:37:05,362 [dl_trainer.py:731] WARNING [ 18][ 3640/  196][rank:0] loss: 1.123, average forward (0.009473) and backward (0.019882) time: 0.030992, iotime: 0.001403 
2022-08-04 16:37:06,845 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046797, Speed: 683.810159 images/s
2022-08-04 16:37:07,237 [dl_trainer.py:731] WARNING [ 18][ 3680/  196][rank:0] loss: 0.795, average forward (0.011490) and backward (0.020959) time: 0.034387, iotime: 0.001661 
2022-08-04 16:37:08,574 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:37:08,575 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:37:09,429 [dl_trainer.py:731] WARNING [ 18][ 3720/  196][rank:0] loss: 0.486, average forward (0.011028) and backward (0.021017) time: 0.039155, iotime: 0.006845 
2022-08-04 16:37:09,630 [dl_trainer.py:634] INFO train iter: 3724, num_batches_per_epoch: 196
2022-08-04 16:37:09,631 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 83.306760, lr: 1.000000, avg loss: 0.484480
2022-08-04 16:37:12,309 [dl_trainer.py:822] INFO Epoch 19, lr: 1.000000, val loss: 0.685678, val top-1 acc: 78.234824, top-5 acc: 98.392572
2022-08-04 16:37:13,322 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086337, Speed: 370.639972 images/s
2022-08-04 16:37:13,938 [dl_trainer.py:731] WARNING [ 19][ 3760/  196][rank:0] loss: 0.456, average forward (0.010109) and backward (0.020916) time: 0.099758, iotime: 0.001435 
2022-08-04 16:37:15,173 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046265, Speed: 691.661328 images/s
2022-08-04 16:37:15,848 [dl_trainer.py:731] WARNING [ 19][ 3800/  196][rank:0] loss: 0.506, average forward (0.010495) and backward (0.020178) time: 0.032496, iotime: 0.001560 
2022-08-04 16:37:17,084 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047770, Speed: 669.882767 images/s
2022-08-04 16:37:17,740 [dl_trainer.py:731] WARNING [ 19][ 3840/  196][rank:0] loss: 0.477, average forward (0.009771) and backward (0.020400) time: 0.031837, iotime: 0.001431 
2022-08-04 16:37:19,000 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047889, Speed: 668.213651 images/s
2022-08-04 16:37:19,622 [dl_trainer.py:731] WARNING [ 19][ 3880/  196][rank:0] loss: 0.272, average forward (0.009096) and backward (0.019983) time: 0.030690, iotime: 0.001378 
2022-08-04 16:37:20,638 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:37:20,638 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:37:21,761 [dl_trainer.py:731] WARNING [ 19][ 3920/  196][rank:0] loss: 0.334, average forward (0.009235) and backward (0.020769) time: 0.035893, iotime: 0.005646 
2022-08-04 16:37:21,774 [dl_trainer.py:634] INFO train iter: 3920, num_batches_per_epoch: 196
2022-08-04 16:37:21,774 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 83.163265, lr: 1.000000, avg loss: 0.493371
2022-08-04 16:37:24,400 [dl_trainer.py:822] INFO Epoch 20, lr: 1.000000, val loss: 0.621537, val top-1 acc: 80.531150, top-5 acc: 98.532348
2022-08-04 16:37:25,348 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084616, Speed: 378.178600 images/s
2022-08-04 16:37:26,203 [dl_trainer.py:731] WARNING [ 20][ 3960/  196][rank:0] loss: 0.336, average forward (0.009670) and backward (0.018651) time: 0.096151, iotime: 0.001415 
2022-08-04 16:37:27,219 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046780, Speed: 684.059607 images/s
2022-08-04 16:37:28,116 [dl_trainer.py:731] WARNING [ 20][ 4000/  196][rank:0] loss: 0.337, average forward (0.010172) and backward (0.020842) time: 0.032865, iotime: 0.001592 
2022-08-04 16:37:29,106 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047161, Speed: 678.531795 images/s
2022-08-04 16:37:30,011 [dl_trainer.py:731] WARNING [ 20][ 4040/  196][rank:0] loss: 0.327, average forward (0.011569) and backward (0.018971) time: 0.032519, iotime: 0.001689 
2022-08-04 16:37:30,974 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046676, Speed: 685.570196 images/s
2022-08-04 16:37:31,844 [dl_trainer.py:731] WARNING [ 20][ 4080/  196][rank:0] loss: 0.560, average forward (0.010386) and backward (0.020236) time: 0.032415, iotime: 0.001554 
2022-08-04 16:37:32,583 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:37:32,583 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:37:33,791 [dl_trainer.py:634] INFO train iter: 4116, num_batches_per_epoch: 196
2022-08-04 16:37:33,792 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 83.785077, lr: 1.000000, avg loss: 0.462887
2022-08-04 16:37:36,487 [dl_trainer.py:822] INFO Epoch 21, lr: 1.000000, val loss: 0.583513, val top-1 acc: 80.511182, top-5 acc: 98.961661
2022-08-04 16:37:36,669 [dl_trainer.py:731] WARNING [ 21][ 4120/  196][rank:0] loss: 0.336, average forward (0.011357) and backward (0.021255) time: 0.106356, iotime: 0.005997 
2022-08-04 16:37:37,435 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086135, Speed: 371.509142 images/s
2022-08-04 16:37:38,517 [dl_trainer.py:731] WARNING [ 21][ 4160/  196][rank:0] loss: 0.400, average forward (0.009208) and backward (0.020689) time: 0.031508, iotime: 0.001383 
2022-08-04 16:37:39,275 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045989, Speed: 695.823964 images/s
2022-08-04 16:37:40,377 [dl_trainer.py:731] WARNING [ 21][ 4200/  196][rank:0] loss: 0.219, average forward (0.008910) and backward (0.020886) time: 0.031476, iotime: 0.001438 
2022-08-04 16:37:41,148 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046821, Speed: 683.455859 images/s
2022-08-04 16:37:42,245 [dl_trainer.py:731] WARNING [ 21][ 4240/  196][rank:0] loss: 0.375, average forward (0.010378) and backward (0.019174) time: 0.031386, iotime: 0.001583 
2022-08-04 16:37:43,022 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046826, Speed: 683.379737 images/s
2022-08-04 16:37:44,118 [dl_trainer.py:731] WARNING [ 21][ 4280/  196][rank:0] loss: 0.341, average forward (0.011072) and backward (0.020659) time: 0.033540, iotime: 0.001551 
2022-08-04 16:37:44,610 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:37:44,610 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:37:45,795 [dl_trainer.py:634] INFO train iter: 4312, num_batches_per_epoch: 196
2022-08-04 16:37:45,796 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 83.816964, lr: 1.000000, avg loss: 0.463911
2022-08-04 16:37:48,450 [dl_trainer.py:822] INFO Epoch 22, lr: 1.000000, val loss: 0.668976, val top-1 acc: 78.983626, top-5 acc: 98.352636
2022-08-04 16:37:48,849 [dl_trainer.py:731] WARNING [ 22][ 4320/  196][rank:0] loss: 0.254, average forward (0.011250) and backward (0.018174) time: 0.103271, iotime: 0.005799 
2022-08-04 16:37:49,375 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084688, Speed: 377.858871 images/s
2022-08-04 16:37:50,698 [dl_trainer.py:731] WARNING [ 22][ 4360/  196][rank:0] loss: 0.187, average forward (0.009085) and backward (0.019889) time: 0.030709, iotime: 0.001495 
2022-08-04 16:37:51,213 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045946, Speed: 696.476605 images/s
2022-08-04 16:37:52,594 [dl_trainer.py:731] WARNING [ 22][ 4400/  196][rank:0] loss: 0.621, average forward (0.009730) and backward (0.020832) time: 0.032318, iotime: 0.001515 
2022-08-04 16:37:53,108 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047367, Speed: 675.581935 images/s
2022-08-04 16:37:54,425 [dl_trainer.py:731] WARNING [ 22][ 4440/  196][rank:0] loss: 0.263, average forward (0.008929) and backward (0.021253) time: 0.031736, iotime: 0.001335 
2022-08-04 16:37:54,969 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046523, Speed: 687.834134 images/s
2022-08-04 16:37:56,301 [dl_trainer.py:731] WARNING [ 22][ 4480/  196][rank:0] loss: 0.351, average forward (0.010478) and backward (0.020663) time: 0.032908, iotime: 0.001521 
2022-08-04 16:37:56,549 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:37:56,550 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:37:57,835 [dl_trainer.py:634] INFO train iter: 4508, num_batches_per_epoch: 196
2022-08-04 16:37:57,836 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 83.737245, lr: 1.000000, avg loss: 0.471299
2022-08-04 16:38:00,520 [dl_trainer.py:822] INFO Epoch 23, lr: 1.000000, val loss: 0.578736, val top-1 acc: 81.309904, top-5 acc: 98.861821
2022-08-04 16:38:01,072 [dl_trainer.py:731] WARNING [ 23][ 4520/  196][rank:0] loss: 0.407, average forward (0.010490) and backward (0.019592) time: 0.103418, iotime: 0.005874 
2022-08-04 16:38:01,356 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085143, Speed: 375.837607 images/s
2022-08-04 16:38:02,948 [dl_trainer.py:731] WARNING [ 23][ 4560/  196][rank:0] loss: 0.300, average forward (0.009735) and backward (0.019371) time: 0.030851, iotime: 0.001486 
2022-08-04 16:38:03,265 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047704, Speed: 670.809751 images/s
2022-08-04 16:38:04,815 [dl_trainer.py:731] WARNING [ 23][ 4600/  196][rank:0] loss: 0.437, average forward (0.008941) and backward (0.020456) time: 0.031054, iotime: 0.001420 
2022-08-04 16:38:05,122 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046417, Speed: 689.399965 images/s
2022-08-04 16:38:06,664 [dl_trainer.py:731] WARNING [ 23][ 4640/  196][rank:0] loss: 0.460, average forward (0.009968) and backward (0.021309) time: 0.033006, iotime: 0.001485 
2022-08-04 16:38:06,954 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045787, Speed: 698.885591 images/s
2022-08-04 16:38:08,538 [dl_trainer.py:731] WARNING [ 23][ 4680/  196][rank:0] loss: 0.260, average forward (0.010291) and backward (0.021154) time: 0.033218, iotime: 0.001527 
2022-08-04 16:38:08,552 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:38:08,552 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:38:09,860 [dl_trainer.py:634] INFO train iter: 4704, num_batches_per_epoch: 196
2022-08-04 16:38:09,862 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 84.821429, lr: 1.000000, avg loss: 0.450752
2022-08-04 16:38:12,503 [dl_trainer.py:822] INFO Epoch 24, lr: 1.000000, val loss: 0.647269, val top-1 acc: 77.925319, top-5 acc: 98.642173
2022-08-04 16:38:13,239 [dl_trainer.py:731] WARNING [ 24][ 4720/  196][rank:0] loss: 0.407, average forward (0.010521) and backward (0.017485) time: 0.100562, iotime: 0.005705 
2022-08-04 16:38:13,300 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084600, Speed: 378.250504 images/s
2022-08-04 16:38:15,131 [dl_trainer.py:731] WARNING [ 24][ 4760/  196][rank:0] loss: 0.768, average forward (0.010140) and backward (0.019195) time: 0.031058, iotime: 0.001476 
2022-08-04 16:38:15,187 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047160, Speed: 678.547831 images/s
2022-08-04 16:38:16,942 [dl_trainer.py:731] WARNING [ 24][ 4800/  196][rank:0] loss: 0.312, average forward (0.010270) and backward (0.021101) time: 0.033105, iotime: 0.001488 
2022-08-04 16:38:17,002 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045366, Speed: 705.378222 images/s
2022-08-04 16:38:18,785 [dl_trainer.py:731] WARNING [ 24][ 4840/  196][rank:0] loss: 0.313, average forward (0.009955) and backward (0.019411) time: 0.031044, iotime: 0.001429 
2022-08-04 16:38:18,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046106, Speed: 694.047021 images/s
2022-08-04 16:38:20,435 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:38:20,435 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:38:20,815 [dl_trainer.py:731] WARNING [ 24][ 4880/  196][rank:0] loss: 0.401, average forward (0.009599) and backward (0.019413) time: 0.035046, iotime: 0.005797 
2022-08-04 16:38:21,727 [dl_trainer.py:634] INFO train iter: 4900, num_batches_per_epoch: 196
2022-08-04 16:38:21,727 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 84.454719, lr: 1.000000, avg loss: 0.449140
2022-08-04 16:38:24,439 [dl_trainer.py:822] INFO Epoch 25, lr: 1.000000, val loss: 0.519786, val top-1 acc: 82.478035, top-5 acc: 99.181310
2022-08-04 16:38:25,199 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084685, Speed: 377.869239 images/s
2022-08-04 16:38:25,358 [dl_trainer.py:731] WARNING [ 25][ 4920/  196][rank:0] loss: 0.338, average forward (0.010921) and backward (0.019833) time: 0.100401, iotime: 0.001522 
2022-08-04 16:38:27,086 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047167, Speed: 678.434817 images/s
2022-08-04 16:38:27,260 [dl_trainer.py:731] WARNING [ 25][ 4960/  196][rank:0] loss: 0.464, average forward (0.009722) and backward (0.020505) time: 0.031943, iotime: 0.001478 
2022-08-04 16:38:28,985 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047455, Speed: 674.317610 images/s
2022-08-04 16:38:29,152 [dl_trainer.py:731] WARNING [ 25][ 5000/  196][rank:0] loss: 0.372, average forward (0.010452) and backward (0.020381) time: 0.032602, iotime: 0.001521 
2022-08-04 16:38:30,877 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047299, Speed: 676.545828 images/s
2022-08-04 16:38:31,060 [dl_trainer.py:731] WARNING [ 25][ 5040/  196][rank:0] loss: 0.370, average forward (0.009952) and backward (0.019881) time: 0.031621, iotime: 0.001552 
2022-08-04 16:38:32,499 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:38:32,499 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:38:33,120 [dl_trainer.py:731] WARNING [ 25][ 5080/  196][rank:0] loss: 0.471, average forward (0.009125) and backward (0.020357) time: 0.035364, iotime: 0.005650 
2022-08-04 16:38:33,894 [dl_trainer.py:634] INFO train iter: 5096, num_batches_per_epoch: 196
2022-08-04 16:38:33,895 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 84.295281, lr: 1.000000, avg loss: 0.446863
2022-08-04 16:38:36,573 [dl_trainer.py:822] INFO Epoch 26, lr: 1.000000, val loss: 0.552530, val top-1 acc: 81.878994, top-5 acc: 98.921725
2022-08-04 16:38:37,279 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085337, Speed: 374.985690 images/s
2022-08-04 16:38:37,713 [dl_trainer.py:731] WARNING [ 26][ 5120/  196][rank:0] loss: 0.463, average forward (0.010595) and backward (0.020934) time: 0.100872, iotime: 0.001578 
2022-08-04 16:38:39,221 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048535, Speed: 659.316057 images/s
2022-08-04 16:38:39,631 [dl_trainer.py:731] WARNING [ 26][ 5160/  196][rank:0] loss: 0.588, average forward (0.009859) and backward (0.021132) time: 0.032778, iotime: 0.001546 
2022-08-04 16:38:41,147 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048143, Speed: 664.690249 images/s
2022-08-04 16:38:41,567 [dl_trainer.py:731] WARNING [ 26][ 5200/  196][rank:0] loss: 0.236, average forward (0.011045) and backward (0.022296) time: 0.035216, iotime: 0.001609 
2022-08-04 16:38:43,010 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046572, Speed: 687.112006 images/s
2022-08-04 16:38:43,423 [dl_trainer.py:731] WARNING [ 26][ 5240/  196][rank:0] loss: 0.448, average forward (0.010097) and backward (0.020550) time: 0.032417, iotime: 0.001518 
2022-08-04 16:38:44,632 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:38:44,633 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:38:45,473 [dl_trainer.py:731] WARNING [ 26][ 5280/  196][rank:0] loss: 0.319, average forward (0.010751) and backward (0.022262) time: 0.038865, iotime: 0.005597 
2022-08-04 16:38:46,038 [dl_trainer.py:634] INFO train iter: 5292, num_batches_per_epoch: 196
2022-08-04 16:38:46,038 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 84.725765, lr: 1.000000, avg loss: 0.452126
2022-08-04 16:38:48,703 [dl_trainer.py:822] INFO Epoch 27, lr: 1.000000, val loss: 0.620967, val top-1 acc: 79.522764, top-5 acc: 98.742013
2022-08-04 16:38:49,380 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084904, Speed: 376.896979 images/s
2022-08-04 16:38:50,021 [dl_trainer.py:731] WARNING [ 27][ 5320/  196][rank:0] loss: 0.354, average forward (0.009332) and backward (0.020517) time: 0.098188, iotime: 0.001390 
2022-08-04 16:38:51,246 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046635, Speed: 686.178564 images/s
2022-08-04 16:38:51,881 [dl_trainer.py:731] WARNING [ 27][ 5360/  196][rank:0] loss: 0.252, average forward (0.009886) and backward (0.020496) time: 0.032083, iotime: 0.001463 
2022-08-04 16:38:53,094 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046208, Speed: 692.520416 images/s
2022-08-04 16:38:53,763 [dl_trainer.py:731] WARNING [ 27][ 5400/  196][rank:0] loss: 0.522, average forward (0.011109) and backward (0.018553) time: 0.031513, iotime: 0.001593 
2022-08-04 16:38:54,998 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047569, Speed: 672.708101 images/s
2022-08-04 16:38:55,647 [dl_trainer.py:731] WARNING [ 27][ 5440/  196][rank:0] loss: 0.380, average forward (0.008937) and backward (0.020281) time: 0.030925, iotime: 0.001478 
2022-08-04 16:38:56,619 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:38:56,619 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:38:57,736 [dl_trainer.py:731] WARNING [ 27][ 5480/  196][rank:0] loss: 0.538, average forward (0.009436) and backward (0.019679) time: 0.034969, iotime: 0.005618 
2022-08-04 16:38:58,130 [dl_trainer.py:634] INFO train iter: 5488, num_batches_per_epoch: 196
2022-08-04 16:38:58,131 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 86.096939, lr: 1.000000, avg loss: 0.413985
2022-08-04 16:39:00,798 [dl_trainer.py:822] INFO Epoch 28, lr: 1.000000, val loss: 0.740956, val top-1 acc: 78.125000, top-5 acc: 98.312700
2022-08-04 16:39:01,473 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086316, Speed: 370.729503 images/s
2022-08-04 16:39:02,374 [dl_trainer.py:731] WARNING [ 28][ 5520/  196][rank:0] loss: 0.361, average forward (0.010677) and backward (0.022073) time: 0.102537, iotime: 0.001575 
2022-08-04 16:39:03,402 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048228, Speed: 663.521509 images/s
2022-08-04 16:39:04,326 [dl_trainer.py:731] WARNING [ 28][ 5560/  196][rank:0] loss: 0.778, average forward (0.011385) and backward (0.021795) time: 0.035140, iotime: 0.001683 
2022-08-04 16:39:05,306 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047575, Speed: 672.616910 images/s
2022-08-04 16:39:06,216 [dl_trainer.py:731] WARNING [ 28][ 5600/  196][rank:0] loss: 0.416, average forward (0.010235) and backward (0.020003) time: 0.032039, iotime: 0.001538 
2022-08-04 16:39:07,222 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047897, Speed: 668.093826 images/s
2022-08-04 16:39:08,137 [dl_trainer.py:731] WARNING [ 28][ 5640/  196][rank:0] loss: 0.467, average forward (0.009950) and backward (0.021713) time: 0.033398, iotime: 0.001482 
2022-08-04 16:39:08,869 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:39:08,869 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:39:10,163 [dl_trainer.py:731] WARNING [ 28][ 5680/  196][rank:0] loss: 0.349, average forward (0.009559) and backward (0.020040) time: 0.035424, iotime: 0.005583 
2022-08-04 16:39:10,367 [dl_trainer.py:634] INFO train iter: 5684, num_batches_per_epoch: 196
2022-08-04 16:39:10,368 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 83.498087, lr: 1.000000, avg loss: 0.473782
2022-08-04 16:39:13,062 [dl_trainer.py:822] INFO Epoch 29, lr: 1.000000, val loss: 0.600606, val top-1 acc: 81.339856, top-5 acc: 98.482428
2022-08-04 16:39:13,623 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085333, Speed: 375.003054 images/s
2022-08-04 16:39:14,708 [dl_trainer.py:731] WARNING [ 29][ 5720/  196][rank:0] loss: 0.760, average forward (0.010016) and backward (0.021110) time: 0.100346, iotime: 0.001531 
2022-08-04 16:39:15,443 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045479, Speed: 703.617123 images/s
2022-08-04 16:39:16,579 [dl_trainer.py:731] WARNING [ 29][ 5760/  196][rank:0] loss: 0.596, average forward (0.009635) and backward (0.019925) time: 0.031272, iotime: 0.001467 
2022-08-04 16:39:17,372 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048211, Speed: 663.749069 images/s
2022-08-04 16:39:18,568 [dl_trainer.py:731] WARNING [ 29][ 5800/  196][rank:0] loss: 0.486, average forward (0.009839) and backward (0.017579) time: 0.029119, iotime: 0.001452 
2022-08-04 16:39:19,335 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049075, Speed: 652.061486 images/s
2022-08-04 16:39:20,412 [dl_trainer.py:731] WARNING [ 29][ 5840/  196][rank:0] loss: 0.603, average forward (0.009928) and backward (0.019889) time: 0.031552, iotime: 0.001498 
2022-08-04 16:39:20,888 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:39:20,888 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:39:22,468 [dl_trainer.py:731] WARNING [ 29][ 5880/  196][rank:0] loss: 0.348, average forward (0.009134) and backward (0.020824) time: 0.035886, iotime: 0.005679 
2022-08-04 16:39:22,486 [dl_trainer.py:634] INFO train iter: 5880, num_batches_per_epoch: 196
2022-08-04 16:39:22,486 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 84.661990, lr: 1.000000, avg loss: 0.441128
2022-08-04 16:39:25,142 [dl_trainer.py:822] INFO Epoch 30, lr: 1.000000, val loss: 0.510225, val top-1 acc: 83.476438, top-5 acc: 99.101438
2022-08-04 16:39:25,705 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084916, Speed: 376.842178 images/s
2022-08-04 16:39:27,028 [dl_trainer.py:731] WARNING [ 30][ 5920/  196][rank:0] loss: 0.563, average forward (0.009547) and backward (0.019461) time: 0.098300, iotime: 0.001472 
2022-08-04 16:39:27,585 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046990, Speed: 680.994384 images/s
2022-08-04 16:39:28,957 [dl_trainer.py:731] WARNING [ 30][ 5960/  196][rank:0] loss: 0.275, average forward (0.011161) and backward (0.021114) time: 0.034171, iotime: 0.001620 
2022-08-04 16:39:29,485 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047491, Speed: 673.814049 images/s
2022-08-04 16:39:30,900 [dl_trainer.py:731] WARNING [ 30][ 6000/  196][rank:0] loss: 0.540, average forward (0.010262) and backward (0.021421) time: 0.033565, iotime: 0.001621 
2022-08-04 16:39:31,416 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048257, Speed: 663.111000 images/s
2022-08-04 16:39:32,801 [dl_trainer.py:731] WARNING [ 30][ 6040/  196][rank:0] loss: 0.324, average forward (0.011637) and backward (0.021742) time: 0.035329, iotime: 0.001674 
2022-08-04 16:39:33,053 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:39:33,053 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:39:34,643 [dl_trainer.py:634] INFO train iter: 6076, num_batches_per_epoch: 196
2022-08-04 16:39:34,644 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 84.709821, lr: 1.000000, avg loss: 0.444283
2022-08-04 16:39:37,338 [dl_trainer.py:822] INFO Epoch 31, lr: 1.000000, val loss: 0.793730, val top-1 acc: 76.477636, top-5 acc: 97.484026
2022-08-04 16:39:37,517 [dl_trainer.py:731] WARNING [ 31][ 6080/  196][rank:0] loss: 0.418, average forward (0.010276) and backward (0.019160) time: 0.102920, iotime: 0.005756 
2022-08-04 16:39:37,815 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085304, Speed: 375.130686 images/s
2022-08-04 16:39:39,447 [dl_trainer.py:731] WARNING [ 31][ 6120/  196][rank:0] loss: 0.423, average forward (0.011687) and backward (0.022007) time: 0.035558, iotime: 0.001591 
2022-08-04 16:39:39,746 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048261, Speed: 663.061452 images/s
2022-08-04 16:39:41,386 [dl_trainer.py:731] WARNING [ 31][ 6160/  196][rank:0] loss: 0.396, average forward (0.010611) and backward (0.023203) time: 0.035651, iotime: 0.001571 
2022-08-04 16:39:41,652 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047643, Speed: 671.656021 images/s
2022-08-04 16:39:43,313 [dl_trainer.py:731] WARNING [ 31][ 6200/  196][rank:0] loss: 0.261, average forward (0.010513) and backward (0.021768) time: 0.034075, iotime: 0.001535 
2022-08-04 16:39:43,619 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049182, Speed: 650.638818 images/s
2022-08-04 16:39:45,217 [dl_trainer.py:731] WARNING [ 31][ 6240/  196][rank:0] loss: 0.540, average forward (0.011718) and backward (0.022243) time: 0.036044, iotime: 0.001811 
2022-08-04 16:39:45,232 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:39:45,232 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:39:46,885 [dl_trainer.py:634] INFO train iter: 6272, num_batches_per_epoch: 196
2022-08-04 16:39:46,886 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 84.741709, lr: 1.000000, avg loss: 0.439987
2022-08-04 16:39:49,617 [dl_trainer.py:822] INFO Epoch 32, lr: 1.000000, val loss: 0.745591, val top-1 acc: 76.357827, top-5 acc: 98.262780
2022-08-04 16:39:50,001 [dl_trainer.py:731] WARNING [ 32][ 6280/  196][rank:0] loss: 0.194, average forward (0.010126) and backward (0.018435) time: 0.103736, iotime: 0.005716 
2022-08-04 16:39:50,078 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086093, Speed: 371.689103 images/s
2022-08-04 16:39:51,962 [dl_trainer.py:731] WARNING [ 32][ 6320/  196][rank:0] loss: 0.313, average forward (0.008505) and backward (0.020076) time: 0.030236, iotime: 0.001433 
2022-08-04 16:39:52,023 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048632, Speed: 658.001807 images/s
2022-08-04 16:39:53,901 [dl_trainer.py:731] WARNING [ 32][ 6360/  196][rank:0] loss: 0.525, average forward (0.009153) and backward (0.018659) time: 0.029448, iotime: 0.001408 
2022-08-04 16:39:53,961 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048427, Speed: 660.783393 images/s
2022-08-04 16:39:55,777 [dl_trainer.py:731] WARNING [ 32][ 6400/  196][rank:0] loss: 0.571, average forward (0.009296) and backward (0.018460) time: 0.029420, iotime: 0.001428 
2022-08-04 16:39:55,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047138, Speed: 678.851906 images/s
2022-08-04 16:39:57,428 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:39:57,428 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:39:57,812 [dl_trainer.py:731] WARNING [ 32][ 6440/  196][rank:0] loss: 0.483, average forward (0.009432) and backward (0.019916) time: 0.035201, iotime: 0.005621 
2022-08-04 16:39:59,035 [dl_trainer.py:634] INFO train iter: 6468, num_batches_per_epoch: 196
2022-08-04 16:39:59,036 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 84.773597, lr: 1.000000, avg loss: 0.442539
2022-08-04 16:40:01,748 [dl_trainer.py:822] INFO Epoch 33, lr: 1.000000, val loss: 0.611357, val top-1 acc: 80.171725, top-5 acc: 98.742013
2022-08-04 16:40:02,139 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083879, Speed: 381.504006 images/s
2022-08-04 16:40:02,304 [dl_trainer.py:731] WARNING [ 33][ 6480/  196][rank:0] loss: 0.672, average forward (0.010410) and backward (0.018318) time: 0.098474, iotime: 0.001607 
2022-08-04 16:40:04,031 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047272, Speed: 676.926539 images/s
2022-08-04 16:40:04,187 [dl_trainer.py:731] WARNING [ 33][ 6520/  196][rank:0] loss: 0.747, average forward (0.009817) and backward (0.020440) time: 0.031998, iotime: 0.001489 
2022-08-04 16:40:05,952 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048008, Speed: 666.556059 images/s
2022-08-04 16:40:06,125 [dl_trainer.py:731] WARNING [ 33][ 6560/  196][rank:0] loss: 0.550, average forward (0.009608) and backward (0.021123) time: 0.032413, iotime: 0.001426 
2022-08-04 16:40:07,857 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047598, Speed: 672.300546 images/s
2022-08-04 16:40:08,029 [dl_trainer.py:731] WARNING [ 33][ 6600/  196][rank:0] loss: 0.286, average forward (0.009906) and backward (0.020131) time: 0.031795, iotime: 0.001506 
2022-08-04 16:40:09,445 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:40:09,446 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:40:10,062 [dl_trainer.py:731] WARNING [ 33][ 6640/  196][rank:0] loss: 0.214, average forward (0.010947) and backward (0.019735) time: 0.036703, iotime: 0.005760 
2022-08-04 16:40:11,184 [dl_trainer.py:634] INFO train iter: 6664, num_batches_per_epoch: 196
2022-08-04 16:40:11,185 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 85.363520, lr: 1.000000, avg loss: 0.430902
2022-08-04 16:40:13,885 [dl_trainer.py:822] INFO Epoch 34, lr: 1.000000, val loss: 0.680835, val top-1 acc: 78.554313, top-5 acc: 98.152955
2022-08-04 16:40:14,247 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085186, Speed: 375.649359 images/s
2022-08-04 16:40:14,655 [dl_trainer.py:731] WARNING [ 34][ 6680/  196][rank:0] loss: 0.380, average forward (0.009235) and backward (0.021904) time: 0.101186, iotime: 0.001487 
2022-08-04 16:40:16,150 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047546, Speed: 673.038095 images/s
2022-08-04 16:40:16,569 [dl_trainer.py:731] WARNING [ 34][ 6720/  196][rank:0] loss: 0.761, average forward (0.011410) and backward (0.020909) time: 0.034279, iotime: 0.001675 
2022-08-04 16:40:18,073 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048059, Speed: 665.849239 images/s
2022-08-04 16:40:18,478 [dl_trainer.py:731] WARNING [ 34][ 6760/  196][rank:0] loss: 0.426, average forward (0.009830) and backward (0.021211) time: 0.032744, iotime: 0.001442 
2022-08-04 16:40:19,980 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047668, Speed: 671.303121 images/s
2022-08-04 16:40:20,385 [dl_trainer.py:731] WARNING [ 34][ 6800/  196][rank:0] loss: 0.215, average forward (0.009679) and backward (0.019731) time: 0.031055, iotime: 0.001404 
2022-08-04 16:40:21,599 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:40:21,599 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:40:22,487 [dl_trainer.py:731] WARNING [ 34][ 6840/  196][rank:0] loss: 0.330, average forward (0.010530) and backward (0.021303) time: 0.038373, iotime: 0.006276 
2022-08-04 16:40:23,445 [dl_trainer.py:634] INFO train iter: 6860, num_batches_per_epoch: 196
2022-08-04 16:40:23,445 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 85.985332, lr: 1.000000, avg loss: 0.405532
2022-08-04 16:40:26,152 [dl_trainer.py:822] INFO Epoch 35, lr: 1.000000, val loss: 0.512347, val top-1 acc: 82.867412, top-5 acc: 99.101438
2022-08-04 16:40:26,440 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086113, Speed: 371.602962 images/s
2022-08-04 16:40:27,060 [dl_trainer.py:731] WARNING [ 35][ 6880/  196][rank:0] loss: 0.228, average forward (0.009436) and backward (0.019533) time: 0.098398, iotime: 0.001444 
2022-08-04 16:40:28,302 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046561, Speed: 687.267079 images/s
2022-08-04 16:40:28,936 [dl_trainer.py:731] WARNING [ 35][ 6920/  196][rank:0] loss: 0.477, average forward (0.009058) and backward (0.020025) time: 0.030722, iotime: 0.001414 
2022-08-04 16:40:30,179 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046905, Speed: 682.226152 images/s
2022-08-04 16:40:30,847 [dl_trainer.py:731] WARNING [ 35][ 6960/  196][rank:0] loss: 0.201, average forward (0.010817) and backward (0.021643) time: 0.034341, iotime: 0.001624 
2022-08-04 16:40:32,087 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047694, Speed: 670.948328 images/s
2022-08-04 16:40:32,720 [dl_trainer.py:731] WARNING [ 35][ 7000/  196][rank:0] loss: 0.808, average forward (0.009323) and backward (0.019893) time: 0.030875, iotime: 0.001416 
2022-08-04 16:40:33,707 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:40:33,708 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:40:34,839 [dl_trainer.py:731] WARNING [ 35][ 7040/  196][rank:0] loss: 0.397, average forward (0.011203) and backward (0.022121) time: 0.039192, iotime: 0.005583 
2022-08-04 16:40:35,603 [dl_trainer.py:634] INFO train iter: 7056, num_batches_per_epoch: 196
2022-08-04 16:40:35,603 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 85.108418, lr: 1.000000, avg loss: 0.423215
2022-08-04 16:40:38,404 [dl_trainer.py:822] INFO Epoch 36, lr: 1.000000, val loss: 0.641249, val top-1 acc: 79.902157, top-5 acc: 98.911741
2022-08-04 16:40:38,726 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088496, Speed: 361.598219 images/s
2022-08-04 16:40:39,603 [dl_trainer.py:731] WARNING [ 36][ 7080/  196][rank:0] loss: 0.599, average forward (0.010740) and backward (0.019601) time: 0.104537, iotime: 0.001529 
2022-08-04 16:40:40,617 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047261, Speed: 677.086356 images/s
2022-08-04 16:40:41,492 [dl_trainer.py:731] WARNING [ 36][ 7120/  196][rank:0] loss: 0.128, average forward (0.010813) and backward (0.018623) time: 0.031319, iotime: 0.001614 
2022-08-04 16:40:42,520 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047564, Speed: 672.772843 images/s
2022-08-04 16:40:43,380 [dl_trainer.py:731] WARNING [ 36][ 7160/  196][rank:0] loss: 0.219, average forward (0.010653) and backward (0.019622) time: 0.032138, iotime: 0.001613 
2022-08-04 16:40:44,380 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046489, Speed: 688.335311 images/s
2022-08-04 16:40:45,282 [dl_trainer.py:731] WARNING [ 36][ 7200/  196][rank:0] loss: 0.267, average forward (0.011193) and backward (0.020826) time: 0.033936, iotime: 0.001650 
2022-08-04 16:40:45,966 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:40:45,967 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:40:47,321 [dl_trainer.py:731] WARNING [ 36][ 7240/  196][rank:0] loss: 0.400, average forward (0.010092) and backward (0.020521) time: 0.036563, iotime: 0.005704 
2022-08-04 16:40:47,908 [dl_trainer.py:634] INFO train iter: 7252, num_batches_per_epoch: 196
2022-08-04 16:40:47,909 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 85.379464, lr: 1.000000, avg loss: 0.428716
2022-08-04 16:40:50,554 [dl_trainer.py:822] INFO Epoch 37, lr: 1.000000, val loss: 0.533430, val top-1 acc: 82.288339, top-5 acc: 98.971645
2022-08-04 16:40:50,756 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085002, Speed: 376.463253 images/s
2022-08-04 16:40:51,844 [dl_trainer.py:731] WARNING [ 37][ 7280/  196][rank:0] loss: 0.407, average forward (0.010918) and backward (0.021733) time: 0.100717, iotime: 0.001608 
2022-08-04 16:40:52,598 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046040, Speed: 695.045377 images/s
2022-08-04 16:40:53,686 [dl_trainer.py:731] WARNING [ 37][ 7320/  196][rank:0] loss: 0.544, average forward (0.011911) and backward (0.020291) time: 0.034112, iotime: 0.001632 
2022-08-04 16:40:54,435 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045900, Speed: 697.174740 images/s
2022-08-04 16:40:55,486 [dl_trainer.py:731] WARNING [ 37][ 7360/  196][rank:0] loss: 0.335, average forward (0.009197) and backward (0.017367) time: 0.028153, iotime: 0.001361 
2022-08-04 16:40:56,246 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045260, Speed: 707.020854 images/s
2022-08-04 16:40:57,379 [dl_trainer.py:731] WARNING [ 37][ 7400/  196][rank:0] loss: 0.434, average forward (0.009385) and backward (0.020605) time: 0.031682, iotime: 0.001461 
2022-08-04 16:40:57,909 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:40:57,909 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:40:59,471 [dl_trainer.py:731] WARNING [ 37][ 7440/  196][rank:0] loss: 0.248, average forward (0.011146) and backward (0.019201) time: 0.036461, iotime: 0.005845 
2022-08-04 16:40:59,870 [dl_trainer.py:634] INFO train iter: 7448, num_batches_per_epoch: 196
2022-08-04 16:40:59,870 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 85.602679, lr: 1.000000, avg loss: 0.409429
2022-08-04 16:41:02,550 [dl_trainer.py:822] INFO Epoch 38, lr: 1.000000, val loss: 0.478299, val top-1 acc: 83.546326, top-5 acc: 99.311102
2022-08-04 16:41:02,713 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086215, Speed: 371.167122 images/s
2022-08-04 16:41:04,067 [dl_trainer.py:731] WARNING [ 38][ 7480/  196][rank:0] loss: 0.710, average forward (0.008819) and backward (0.021222) time: 0.099222, iotime: 0.001379 
2022-08-04 16:41:04,600 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047182, Speed: 678.223980 images/s
2022-08-04 16:41:05,939 [dl_trainer.py:731] WARNING [ 38][ 7520/  196][rank:0] loss: 0.376, average forward (0.009335) and backward (0.019519) time: 0.030484, iotime: 0.001400 
2022-08-04 16:41:06,483 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047045, Speed: 680.200866 images/s
2022-08-04 16:41:07,853 [dl_trainer.py:731] WARNING [ 38][ 7560/  196][rank:0] loss: 0.634, average forward (0.009115) and backward (0.019290) time: 0.030049, iotime: 0.001409 
2022-08-04 16:41:08,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046240, Speed: 692.044973 images/s
2022-08-04 16:41:09,679 [dl_trainer.py:731] WARNING [ 38][ 7600/  196][rank:0] loss: 0.547, average forward (0.008367) and backward (0.020410) time: 0.030325, iotime: 0.001336 
2022-08-04 16:41:09,945 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:41:09,946 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:41:11,775 [dl_trainer.py:731] WARNING [ 38][ 7640/  196][rank:0] loss: 0.532, average forward (0.010148) and backward (0.020713) time: 0.036839, iotime: 0.005711 
2022-08-04 16:41:11,983 [dl_trainer.py:634] INFO train iter: 7644, num_batches_per_epoch: 196
2022-08-04 16:41:11,983 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 84.837372, lr: 1.000000, avg loss: 0.454561
2022-08-04 16:41:14,635 [dl_trainer.py:822] INFO Epoch 39, lr: 1.000000, val loss: 0.481431, val top-1 acc: 83.676118, top-5 acc: 99.371006
2022-08-04 16:41:14,742 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085447, Speed: 374.502477 images/s
2022-08-04 16:41:16,309 [dl_trainer.py:731] WARNING [ 39][ 7680/  196][rank:0] loss: 0.298, average forward (0.009959) and backward (0.019476) time: 0.097534, iotime: 0.001485 
2022-08-04 16:41:16,602 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046494, Speed: 688.263657 images/s
2022-08-04 16:41:18,149 [dl_trainer.py:731] WARNING [ 39][ 7720/  196][rank:0] loss: 0.290, average forward (0.008797) and backward (0.020497) time: 0.030914, iotime: 0.001403 
2022-08-04 16:41:18,469 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046664, Speed: 685.747785 images/s
2022-08-04 16:41:20,059 [dl_trainer.py:731] WARNING [ 39][ 7760/  196][rank:0] loss: 0.313, average forward (0.010010) and backward (0.018988) time: 0.030744, iotime: 0.001482 
2022-08-04 16:41:20,372 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047559, Speed: 672.851595 images/s
2022-08-04 16:41:21,922 [dl_trainer.py:731] WARNING [ 39][ 7800/  196][rank:0] loss: 0.423, average forward (0.009851) and backward (0.020522) time: 0.032123, iotime: 0.001520 
2022-08-04 16:41:21,935 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:41:21,935 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:41:23,999 [dl_trainer.py:731] WARNING [ 39][ 7840/  196][rank:0] loss: 0.406, average forward (0.009100) and backward (0.020608) time: 0.035567, iotime: 0.005637 
2022-08-04 16:41:24,009 [dl_trainer.py:634] INFO train iter: 7840, num_batches_per_epoch: 196
2022-08-04 16:41:24,009 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 85.921556, lr: 1.000000, avg loss: 0.415732
2022-08-04 16:41:26,700 [dl_trainer.py:822] INFO Epoch 40, lr: 1.000000, val loss: 0.619878, val top-1 acc: 80.071885, top-5 acc: 98.751997
2022-08-04 16:41:26,765 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085223, Speed: 375.484367 images/s
2022-08-04 16:41:28,578 [dl_trainer.py:731] WARNING [ 40][ 7880/  196][rank:0] loss: 0.458, average forward (0.009709) and backward (0.021248) time: 0.100386, iotime: 0.001407 
2022-08-04 16:41:28,645 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046990, Speed: 680.992052 images/s
2022-08-04 16:41:30,476 [dl_trainer.py:731] WARNING [ 40][ 7920/  196][rank:0] loss: 0.642, average forward (0.009035) and backward (0.019678) time: 0.030293, iotime: 0.001355 
2022-08-04 16:41:30,542 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047421, Speed: 674.803178 images/s
2022-08-04 16:41:32,453 [dl_trainer.py:731] WARNING [ 40][ 7960/  196][rank:0] loss: 0.269, average forward (0.010380) and backward (0.021415) time: 0.033642, iotime: 0.001582 
2022-08-04 16:41:32,516 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049338, Speed: 648.593159 images/s
2022-08-04 16:41:34,103 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:41:34,103 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:41:34,499 [dl_trainer.py:731] WARNING [ 40][ 8000/  196][rank:0] loss: 0.417, average forward (0.009539) and backward (0.019078) time: 0.034539, iotime: 0.005678 
2022-08-04 16:41:36,191 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048984, Speed: 653.278101 images/s
2022-08-04 16:41:36,192 [dl_trainer.py:634] INFO train iter: 8036, num_batches_per_epoch: 196
2022-08-04 16:41:36,192 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 85.188138, lr: 1.000000, avg loss: 0.418644
2022-08-04 16:41:38,825 [dl_trainer.py:822] INFO Epoch 41, lr: 1.000000, val loss: 0.564519, val top-1 acc: 81.389776, top-5 acc: 99.051518
2022-08-04 16:41:39,013 [dl_trainer.py:731] WARNING [ 41][ 8040/  196][rank:0] loss: 0.217, average forward (0.011163) and backward (0.020198) time: 0.099122, iotime: 0.001619 
2022-08-04 16:41:40,716 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113120, Speed: 282.885428 images/s
2022-08-04 16:41:40,890 [dl_trainer.py:731] WARNING [ 41][ 8080/  196][rank:0] loss: 0.336, average forward (0.009380) and backward (0.019670) time: 0.030790, iotime: 0.001495 
2022-08-04 16:41:42,616 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047490, Speed: 673.827834 images/s
2022-08-04 16:41:42,780 [dl_trainer.py:731] WARNING [ 41][ 8120/  196][rank:0] loss: 0.316, average forward (0.009785) and backward (0.021647) time: 0.033191, iotime: 0.001516 
2022-08-04 16:41:44,455 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045957, Speed: 696.309311 images/s
2022-08-04 16:41:44,628 [dl_trainer.py:731] WARNING [ 41][ 8160/  196][rank:0] loss: 0.480, average forward (0.010074) and backward (0.021639) time: 0.033449, iotime: 0.001496 
2022-08-04 16:41:46,004 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:41:46,004 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:41:46,655 [dl_trainer.py:731] WARNING [ 41][ 8200/  196][rank:0] loss: 0.276, average forward (0.010126) and backward (0.019326) time: 0.035124, iotime: 0.005429 
2022-08-04 16:41:48,126 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048928, Speed: 654.022320 images/s
2022-08-04 16:41:48,178 [dl_trainer.py:634] INFO train iter: 8232, num_batches_per_epoch: 196
2022-08-04 16:41:48,179 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 86.192602, lr: 1.000000, avg loss: 0.396721
2022-08-04 16:41:50,882 [dl_trainer.py:822] INFO Epoch 42, lr: 1.000000, val loss: 0.823159, val top-1 acc: 74.051518, top-5 acc: 98.342652
2022-08-04 16:41:51,280 [dl_trainer.py:731] WARNING [ 42][ 8240/  196][rank:0] loss: 0.419, average forward (0.011515) and backward (0.020918) time: 0.102491, iotime: 0.001644 
2022-08-04 16:41:52,740 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115357, Speed: 277.400815 images/s
2022-08-04 16:41:53,177 [dl_trainer.py:731] WARNING [ 42][ 8280/  196][rank:0] loss: 0.400, average forward (0.009770) and backward (0.021397) time: 0.032885, iotime: 0.001480 
2022-08-04 16:41:54,681 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048490, Speed: 659.928101 images/s
2022-08-04 16:41:55,106 [dl_trainer.py:731] WARNING [ 42][ 8320/  196][rank:0] loss: 0.659, average forward (0.009411) and backward (0.020528) time: 0.031657, iotime: 0.001466 
2022-08-04 16:41:56,538 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046429, Speed: 689.226674 images/s
2022-08-04 16:41:56,942 [dl_trainer.py:731] WARNING [ 42][ 8360/  196][rank:0] loss: 0.393, average forward (0.009127) and backward (0.021072) time: 0.031823, iotime: 0.001404 
2022-08-04 16:41:58,125 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:41:58,125 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:41:58,951 [dl_trainer.py:731] WARNING [ 42][ 8400/  196][rank:0] loss: 0.382, average forward (0.010363) and backward (0.020861) time: 0.036984, iotime: 0.005514 
2022-08-04 16:42:00,167 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048366, Speed: 661.617932 images/s
2022-08-04 16:42:00,271 [dl_trainer.py:634] INFO train iter: 8428, num_batches_per_epoch: 196
2022-08-04 16:42:00,272 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 85.841837, lr: 1.000000, avg loss: 0.416216
2022-08-04 16:42:02,896 [dl_trainer.py:822] INFO Epoch 43, lr: 1.000000, val loss: 0.472594, val top-1 acc: 84.464856, top-5 acc: 99.041534
2022-08-04 16:42:03,423 [dl_trainer.py:731] WARNING [ 43][ 8440/  196][rank:0] loss: 0.453, average forward (0.009463) and backward (0.019312) time: 0.096076, iotime: 0.001401 
2022-08-04 16:42:04,606 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.110961, Speed: 288.390229 images/s
2022-08-04 16:42:05,214 [dl_trainer.py:731] WARNING [ 43][ 8480/  196][rank:0] loss: 0.169, average forward (0.008995) and backward (0.018644) time: 0.029274, iotime: 0.001403 
2022-08-04 16:42:06,460 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046353, Speed: 690.352850 images/s
2022-08-04 16:42:07,070 [dl_trainer.py:731] WARNING [ 43][ 8520/  196][rank:0] loss: 0.259, average forward (0.009436) and backward (0.020634) time: 0.031775, iotime: 0.001469 
2022-08-04 16:42:08,363 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047548, Speed: 672.999960 images/s
2022-08-04 16:42:08,975 [dl_trainer.py:731] WARNING [ 43][ 8560/  196][rank:0] loss: 0.382, average forward (0.010124) and backward (0.018675) time: 0.030552, iotime: 0.001494 
2022-08-04 16:42:09,947 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:42:09,947 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:42:11,010 [dl_trainer.py:731] WARNING [ 43][ 8600/  196][rank:0] loss: 0.280, average forward (0.009569) and backward (0.020747) time: 0.036411, iotime: 0.005867 
2022-08-04 16:42:11,972 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048105, Speed: 665.213519 images/s
2022-08-04 16:42:12,118 [dl_trainer.py:634] INFO train iter: 8624, num_batches_per_epoch: 196
2022-08-04 16:42:12,119 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 85.650510, lr: 1.000000, avg loss: 0.417613
2022-08-04 16:42:14,779 [dl_trainer.py:822] INFO Epoch 44, lr: 1.000000, val loss: 0.538702, val top-1 acc: 82.078674, top-5 acc: 99.161342
2022-08-04 16:42:15,535 [dl_trainer.py:731] WARNING [ 44][ 8640/  196][rank:0] loss: 0.167, average forward (0.009815) and backward (0.020331) time: 0.099200, iotime: 0.001506 
2022-08-04 16:42:16,525 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113824, Speed: 281.135514 images/s
2022-08-04 16:42:17,433 [dl_trainer.py:731] WARNING [ 44][ 8680/  196][rank:0] loss: 0.321, average forward (0.008343) and backward (0.021721) time: 0.031592, iotime: 0.001308 
2022-08-04 16:42:18,411 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047144, Speed: 678.770197 images/s
2022-08-04 16:42:19,268 [dl_trainer.py:731] WARNING [ 44][ 8720/  196][rank:0] loss: 0.395, average forward (0.009465) and backward (0.020361) time: 0.031549, iotime: 0.001493 
2022-08-04 16:42:20,234 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045545, Speed: 702.598331 images/s
2022-08-04 16:42:21,080 [dl_trainer.py:731] WARNING [ 44][ 8760/  196][rank:0] loss: 0.282, average forward (0.009307) and backward (0.020068) time: 0.030970, iotime: 0.001367 
2022-08-04 16:42:21,813 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:42:21,813 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:42:23,131 [dl_trainer.py:731] WARNING [ 44][ 8800/  196][rank:0] loss: 0.363, average forward (0.009407) and backward (0.020297) time: 0.035181, iotime: 0.005238 
2022-08-04 16:42:23,881 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048609, Speed: 658.320987 images/s
2022-08-04 16:42:24,063 [dl_trainer.py:634] INFO train iter: 8820, num_batches_per_epoch: 196
2022-08-04 16:42:24,064 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 86.128827, lr: 1.000000, avg loss: 0.406304
2022-08-04 16:42:26,707 [dl_trainer.py:822] INFO Epoch 45, lr: 1.000000, val loss: 0.527228, val top-1 acc: 82.927316, top-5 acc: 99.131390
2022-08-04 16:42:27,585 [dl_trainer.py:731] WARNING [ 45][ 8840/  196][rank:0] loss: 0.451, average forward (0.009700) and backward (0.020420) time: 0.097956, iotime: 0.001425 
2022-08-04 16:42:28,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.111245, Speed: 287.654209 images/s
2022-08-04 16:42:29,440 [dl_trainer.py:731] WARNING [ 45][ 8880/  196][rank:0] loss: 0.121, average forward (0.009058) and backward (0.018748) time: 0.029445, iotime: 0.001396 
2022-08-04 16:42:30,198 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046644, Speed: 686.054665 images/s
2022-08-04 16:42:31,230 [dl_trainer.py:731] WARNING [ 45][ 8920/  196][rank:0] loss: 0.153, average forward (0.008815) and backward (0.019340) time: 0.029706, iotime: 0.001334 
2022-08-04 16:42:31,957 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043964, Speed: 727.868082 images/s
2022-08-04 16:42:33,094 [dl_trainer.py:731] WARNING [ 45][ 8960/  196][rank:0] loss: 0.285, average forward (0.009551) and backward (0.019909) time: 0.031116, iotime: 0.001428 
2022-08-04 16:42:33,561 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:42:33,562 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:42:35,101 [dl_trainer.py:731] WARNING [ 45][ 9000/  196][rank:0] loss: 0.889, average forward (0.009621) and backward (0.018243) time: 0.033770, iotime: 0.005664 
2022-08-04 16:42:35,598 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048525, Speed: 659.458085 images/s
2022-08-04 16:42:35,815 [dl_trainer.py:634] INFO train iter: 9016, num_batches_per_epoch: 196
2022-08-04 16:42:35,816 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 85.156250, lr: 1.000000, avg loss: 0.435216
2022-08-04 16:42:38,458 [dl_trainer.py:822] INFO Epoch 46, lr: 1.000000, val loss: 0.541163, val top-1 acc: 81.829073, top-5 acc: 99.011581
2022-08-04 16:42:39,562 [dl_trainer.py:731] WARNING [ 46][ 9040/  196][rank:0] loss: 0.201, average forward (0.009261) and backward (0.019688) time: 0.097162, iotime: 0.001394 
2022-08-04 16:42:40,107 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112727, Speed: 283.871118 images/s
2022-08-04 16:42:41,411 [dl_trainer.py:731] WARNING [ 46][ 9080/  196][rank:0] loss: 0.427, average forward (0.009081) and backward (0.020152) time: 0.030849, iotime: 0.001395 
2022-08-04 16:42:41,958 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046255, Speed: 691.813380 images/s
2022-08-04 16:42:43,334 [dl_trainer.py:731] WARNING [ 46][ 9120/  196][rank:0] loss: 0.331, average forward (0.010423) and backward (0.020937) time: 0.033165, iotime: 0.001549 
2022-08-04 16:42:43,869 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047782, Speed: 669.704695 images/s
2022-08-04 16:42:45,202 [dl_trainer.py:731] WARNING [ 46][ 9160/  196][rank:0] loss: 0.325, average forward (0.009618) and backward (0.021245) time: 0.032534, iotime: 0.001444 
2022-08-04 16:42:45,477 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:42:45,477 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:42:47,203 [dl_trainer.py:731] WARNING [ 46][ 9200/  196][rank:0] loss: 0.151, average forward (0.010547) and backward (0.020462) time: 0.036816, iotime: 0.005542 
2022-08-04 16:42:47,495 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048321, Speed: 662.239792 images/s
2022-08-04 16:42:47,798 [dl_trainer.py:634] INFO train iter: 9212, num_batches_per_epoch: 196
2022-08-04 16:42:47,799 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 85.825893, lr: 1.000000, avg loss: 0.412767
2022-08-04 16:42:50,434 [dl_trainer.py:822] INFO Epoch 47, lr: 1.000000, val loss: 0.818640, val top-1 acc: 76.867013, top-5 acc: 98.302716
2022-08-04 16:42:51,733 [dl_trainer.py:731] WARNING [ 47][ 9240/  196][rank:0] loss: 0.361, average forward (0.011286) and backward (0.018677) time: 0.097753, iotime: 0.001566 
2022-08-04 16:42:52,064 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114238, Speed: 280.118045 images/s
2022-08-04 16:42:53,655 [dl_trainer.py:731] WARNING [ 47][ 9280/  196][rank:0] loss: 0.524, average forward (0.009769) and backward (0.018531) time: 0.030052, iotime: 0.001496 
2022-08-04 16:42:53,923 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046467, Speed: 688.664123 images/s
2022-08-04 16:42:55,466 [dl_trainer.py:731] WARNING [ 47][ 9320/  196][rank:0] loss: 0.507, average forward (0.009267) and backward (0.018912) time: 0.029830, iotime: 0.001412 
2022-08-04 16:42:55,788 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046618, Speed: 686.435976 images/s
2022-08-04 16:42:57,304 [dl_trainer.py:731] WARNING [ 47][ 9360/  196][rank:0] loss: 1.027, average forward (0.008630) and backward (0.018302) time: 0.028495, iotime: 0.001333 
2022-08-04 16:42:57,324 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:42:57,324 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:42:59,389 [dl_trainer.py:731] WARNING [ 47][ 9400/  196][rank:0] loss: 0.499, average forward (0.009717) and backward (0.020392) time: 0.036037, iotime: 0.005677 
2022-08-04 16:42:59,446 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048751, Speed: 656.395532 images/s
2022-08-04 16:42:59,796 [dl_trainer.py:634] INFO train iter: 9408, num_batches_per_epoch: 196
2022-08-04 16:42:59,797 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 84.773597, lr: 1.000000, avg loss: 0.434403
2022-08-04 16:43:02,444 [dl_trainer.py:822] INFO Epoch 48, lr: 1.000000, val loss: 0.581925, val top-1 acc: 81.349840, top-5 acc: 98.702077
2022-08-04 16:43:03,967 [dl_trainer.py:731] WARNING [ 48][ 9440/  196][rank:0] loss: 0.437, average forward (0.008769) and backward (0.021304) time: 0.098345, iotime: 0.001342 
2022-08-04 16:43:04,031 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114603, Speed: 279.225076 images/s
2022-08-04 16:43:05,775 [dl_trainer.py:731] WARNING [ 48][ 9480/  196][rank:0] loss: 0.588, average forward (0.010147) and backward (0.019583) time: 0.031432, iotime: 0.001454 
2022-08-04 16:43:05,838 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045187, Speed: 708.162542 images/s
2022-08-04 16:43:07,694 [dl_trainer.py:731] WARNING [ 48][ 9520/  196][rank:0] loss: 0.460, average forward (0.011185) and backward (0.019789) time: 0.032833, iotime: 0.001594 
2022-08-04 16:43:07,771 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048300, Speed: 662.522391 images/s
2022-08-04 16:43:09,400 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:43:09,400 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:43:09,783 [dl_trainer.py:731] WARNING [ 48][ 9560/  196][rank:0] loss: 0.383, average forward (0.010205) and backward (0.019918) time: 0.036206, iotime: 0.005816 
2022-08-04 16:43:11,491 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049591, Speed: 645.278779 images/s
2022-08-04 16:43:11,677 [dl_trainer.py:731] WARNING [ 48][ 9600/  196][rank:0] loss: 0.414, average forward (0.009556) and backward (0.018026) time: 0.029268, iotime: 0.001454 
2022-08-04 16:43:11,877 [dl_trainer.py:634] INFO train iter: 9604, num_batches_per_epoch: 196
2022-08-04 16:43:11,877 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 85.841837, lr: 1.000000, avg loss: 0.417121
2022-08-04 16:43:14,563 [dl_trainer.py:822] INFO Epoch 49, lr: 1.000000, val loss: 0.572352, val top-1 acc: 81.569489, top-5 acc: 99.201278
2022-08-04 16:43:16,034 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113551, Speed: 281.811956 images/s
2022-08-04 16:43:16,202 [dl_trainer.py:731] WARNING [ 49][ 9640/  196][rank:0] loss: 0.410, average forward (0.011113) and backward (0.019958) time: 0.100193, iotime: 0.001626 
2022-08-04 16:43:17,830 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044873, Speed: 713.127885 images/s
2022-08-04 16:43:17,984 [dl_trainer.py:731] WARNING [ 49][ 9680/  196][rank:0] loss: 0.696, average forward (0.011027) and backward (0.018963) time: 0.031789, iotime: 0.001544 
2022-08-04 16:43:19,688 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046435, Speed: 689.135284 images/s
2022-08-04 16:43:19,847 [dl_trainer.py:731] WARNING [ 49][ 9720/  196][rank:0] loss: 0.752, average forward (0.009624) and backward (0.019145) time: 0.030508, iotime: 0.001491 
2022-08-04 16:43:21,204 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:43:21,204 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:43:21,850 [dl_trainer.py:731] WARNING [ 49][ 9760/  196][rank:0] loss: 0.238, average forward (0.011050) and backward (0.016048) time: 0.033948, iotime: 0.006578 
2022-08-04 16:43:23,287 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047966, Speed: 667.139618 images/s
2022-08-04 16:43:23,671 [dl_trainer.py:731] WARNING [ 49][ 9800/  196][rank:0] loss: 0.165, average forward (0.009574) and backward (0.018187) time: 0.029432, iotime: 0.001439 
2022-08-04 16:43:23,694 [dl_trainer.py:634] INFO train iter: 9800, num_batches_per_epoch: 196
2022-08-04 16:43:23,694 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 85.028699, lr: 1.000000, avg loss: 0.422657
2022-08-04 16:43:26,395 [dl_trainer.py:822] INFO Epoch 50, lr: 1.000000, val loss: 0.608557, val top-1 acc: 80.720847, top-5 acc: 98.602236
2022-08-04 16:43:27,871 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114579, Speed: 279.283541 images/s
2022-08-04 16:43:28,264 [dl_trainer.py:731] WARNING [ 50][ 9840/  196][rank:0] loss: 0.510, average forward (0.010325) and backward (0.021034) time: 0.101243, iotime: 0.001571 
2022-08-04 16:43:29,740 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046710, Speed: 685.071725 images/s
2022-08-04 16:43:30,146 [dl_trainer.py:731] WARNING [ 50][ 9880/  196][rank:0] loss: 0.381, average forward (0.008647) and backward (0.021630) time: 0.031846, iotime: 0.001357 
2022-08-04 16:43:31,646 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047637, Speed: 671.751407 images/s
2022-08-04 16:43:32,064 [dl_trainer.py:731] WARNING [ 50][ 9920/  196][rank:0] loss: 0.571, average forward (0.010717) and backward (0.021373) time: 0.033941, iotime: 0.001584 
2022-08-04 16:43:33,246 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:43:33,246 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:43:34,065 [dl_trainer.py:731] WARNING [ 50][ 9960/  196][rank:0] loss: 0.624, average forward (0.011226) and backward (0.019335) time: 0.036822, iotime: 0.005998 
2022-08-04 16:43:35,313 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048861, Speed: 654.920913 images/s
2022-08-04 16:43:35,792 [dl_trainer.py:634] INFO train iter: 9996, num_batches_per_epoch: 196
2022-08-04 16:43:35,792 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 85.873724, lr: 1.000000, avg loss: 0.416476
2022-08-04 16:43:38,453 [dl_trainer.py:822] INFO Epoch 51, lr: 1.000000, val loss: 0.642726, val top-1 acc: 79.323083, top-5 acc: 98.861821
2022-08-04 16:43:38,637 [dl_trainer.py:731] WARNING [ 51][10000/  196][rank:0] loss: 0.431, average forward (0.010071) and backward (0.020964) time: 0.099362, iotime: 0.001493 
2022-08-04 16:43:39,760 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.111154, Speed: 287.887836 images/s
2022-08-04 16:43:40,350 [dl_trainer.py:731] WARNING [ 51][10040/  196][rank:0] loss: 0.310, average forward (0.009893) and backward (0.017257) time: 0.028845, iotime: 0.001445 
2022-08-04 16:43:41,611 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046260, Speed: 691.736365 images/s
2022-08-04 16:43:42,243 [dl_trainer.py:731] WARNING [ 51][10080/  196][rank:0] loss: 0.164, average forward (0.009041) and backward (0.020168) time: 0.030863, iotime: 0.001421 
2022-08-04 16:43:43,420 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045224, Speed: 707.587041 images/s
2022-08-04 16:43:44,074 [dl_trainer.py:731] WARNING [ 51][10120/  196][rank:0] loss: 0.472, average forward (0.010469) and backward (0.018841) time: 0.031176, iotime: 0.001585 
2022-08-04 16:43:45,064 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:43:45,064 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:43:46,124 [dl_trainer.py:731] WARNING [ 51][10160/  196][rank:0] loss: 0.199, average forward (0.009501) and backward (0.018708) time: 0.034111, iotime: 0.005646 
2022-08-04 16:43:47,053 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048417, Speed: 660.918297 images/s
2022-08-04 16:43:47,486 [dl_trainer.py:634] INFO train iter: 10192, num_batches_per_epoch: 196
2022-08-04 16:43:47,486 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 86.256378, lr: 1.000000, avg loss: 0.405836
2022-08-04 16:43:50,187 [dl_trainer.py:822] INFO Epoch 52, lr: 1.000000, val loss: 0.726035, val top-1 acc: 78.943690, top-5 acc: 97.563898
2022-08-04 16:43:50,589 [dl_trainer.py:731] WARNING [ 52][10200/  196][rank:0] loss: 0.423, average forward (0.009384) and backward (0.018435) time: 0.098709, iotime: 0.001355 
2022-08-04 16:43:51,469 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.110387, Speed: 289.890107 images/s
2022-08-04 16:43:52,344 [dl_trainer.py:731] WARNING [ 52][10240/  196][rank:0] loss: 0.576, average forward (0.010662) and backward (0.018399) time: 0.030878, iotime: 0.001558 
2022-08-04 16:43:53,245 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044382, Speed: 721.018253 images/s
2022-08-04 16:43:54,080 [dl_trainer.py:731] WARNING [ 52][10280/  196][rank:0] loss: 0.373, average forward (0.010244) and backward (0.017581) time: 0.029578, iotime: 0.001510 
2022-08-04 16:43:55,026 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044504, Speed: 719.038148 images/s
2022-08-04 16:43:55,839 [dl_trainer.py:731] WARNING [ 52][10320/  196][rank:0] loss: 0.377, average forward (0.009868) and backward (0.017809) time: 0.029396, iotime: 0.001480 
2022-08-04 16:43:56,549 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:43:56,549 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:43:57,887 [dl_trainer.py:731] WARNING [ 52][10360/  196][rank:0] loss: 0.443, average forward (0.011307) and backward (0.020627) time: 0.037872, iotime: 0.005665 
2022-08-04 16:43:58,670 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048573, Speed: 658.802630 images/s
2022-08-04 16:43:59,266 [dl_trainer.py:634] INFO train iter: 10388, num_batches_per_epoch: 196
2022-08-04 16:43:59,267 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 85.953444, lr: 1.000000, avg loss: 0.411702
2022-08-04 16:44:01,932 [dl_trainer.py:822] INFO Epoch 53, lr: 1.000000, val loss: 0.669367, val top-1 acc: 78.264776, top-5 acc: 98.672125
2022-08-04 16:44:02,443 [dl_trainer.py:731] WARNING [ 53][10400/  196][rank:0] loss: 0.431, average forward (0.009684) and backward (0.019838) time: 0.097870, iotime: 0.001408 
2022-08-04 16:44:03,189 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112949, Speed: 283.312941 images/s
2022-08-04 16:44:04,239 [dl_trainer.py:731] WARNING [ 53][10440/  196][rank:0] loss: 0.409, average forward (0.008821) and backward (0.019069) time: 0.029471, iotime: 0.001349 
2022-08-04 16:44:04,937 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043699, Speed: 732.276172 images/s
2022-08-04 16:44:06,030 [dl_trainer.py:731] WARNING [ 53][10480/  196][rank:0] loss: 0.274, average forward (0.011085) and backward (0.019753) time: 0.032688, iotime: 0.001596 
2022-08-04 16:44:06,794 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046428, Speed: 689.232956 images/s
2022-08-04 16:44:07,918 [dl_trainer.py:731] WARNING [ 53][10520/  196][rank:0] loss: 0.351, average forward (0.010803) and backward (0.020187) time: 0.032810, iotime: 0.001565 
2022-08-04 16:44:08,394 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:44:08,394 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:44:09,949 [dl_trainer.py:731] WARNING [ 53][10560/  196][rank:0] loss: 0.699, average forward (0.010464) and backward (0.020352) time: 0.036834, iotime: 0.005771 
2022-08-04 16:44:10,463 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048893, Speed: 654.493606 images/s
2022-08-04 16:44:11,078 [dl_trainer.py:634] INFO train iter: 10584, num_batches_per_epoch: 196
2022-08-04 16:44:11,079 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 86.224490, lr: 1.000000, avg loss: 0.391995
2022-08-04 16:44:13,757 [dl_trainer.py:822] INFO Epoch 54, lr: 1.000000, val loss: 0.559066, val top-1 acc: 81.998802, top-5 acc: 99.231230
2022-08-04 16:44:14,557 [dl_trainer.py:731] WARNING [ 54][10600/  196][rank:0] loss: 0.372, average forward (0.009375) and backward (0.020351) time: 0.099657, iotime: 0.001485 
2022-08-04 16:44:15,077 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115360, Speed: 277.391369 images/s
2022-08-04 16:44:16,417 [dl_trainer.py:731] WARNING [ 54][10640/  196][rank:0] loss: 0.375, average forward (0.010061) and backward (0.021414) time: 0.033237, iotime: 0.001512 
2022-08-04 16:44:16,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046375, Speed: 690.034308 images/s
2022-08-04 16:44:18,259 [dl_trainer.py:731] WARNING [ 54][10680/  196][rank:0] loss: 0.457, average forward (0.010532) and backward (0.019120) time: 0.031483, iotime: 0.001565 
2022-08-04 16:44:18,781 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046202, Speed: 692.613241 images/s
2022-08-04 16:44:20,163 [dl_trainer.py:731] WARNING [ 54][10720/  196][rank:0] loss: 0.331, average forward (0.011598) and backward (0.021771) time: 0.035294, iotime: 0.001652 
2022-08-04 16:44:20,420 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:44:20,421 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:44:22,230 [dl_trainer.py:731] WARNING [ 54][10760/  196][rank:0] loss: 0.433, average forward (0.011116) and backward (0.020572) time: 0.037773, iotime: 0.005796 
2022-08-04 16:44:22,484 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049349, Speed: 648.443815 images/s
2022-08-04 16:44:23,138 [dl_trainer.py:634] INFO train iter: 10780, num_batches_per_epoch: 196
2022-08-04 16:44:23,138 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 85.985332, lr: 1.000000, avg loss: 0.408886
2022-08-04 16:44:25,817 [dl_trainer.py:822] INFO Epoch 55, lr: 1.000000, val loss: 0.559728, val top-1 acc: 81.789137, top-5 acc: 99.101438
2022-08-04 16:44:26,731 [dl_trainer.py:731] WARNING [ 55][10800/  196][rank:0] loss: 0.517, average forward (0.009519) and backward (0.020833) time: 0.099121, iotime: 0.001489 
2022-08-04 16:44:27,005 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113031, Speed: 283.109264 images/s
2022-08-04 16:44:28,566 [dl_trainer.py:731] WARNING [ 55][10840/  196][rank:0] loss: 0.533, average forward (0.010465) and backward (0.020167) time: 0.032335, iotime: 0.001453 
2022-08-04 16:44:28,866 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046494, Speed: 688.261275 images/s
2022-08-04 16:44:30,398 [dl_trainer.py:731] WARNING [ 55][10880/  196][rank:0] loss: 0.299, average forward (0.011068) and backward (0.019997) time: 0.032915, iotime: 0.001593 
2022-08-04 16:44:30,717 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046279, Speed: 691.464099 images/s
2022-08-04 16:44:32,354 [dl_trainer.py:731] WARNING [ 55][10920/  196][rank:0] loss: 0.199, average forward (0.009050) and backward (0.021566) time: 0.032240, iotime: 0.001391 
2022-08-04 16:44:32,370 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:44:32,370 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:44:34,376 [dl_trainer.py:731] WARNING [ 55][10960/  196][rank:0] loss: 0.295, average forward (0.009462) and backward (0.020104) time: 0.035193, iotime: 0.005396 
2022-08-04 16:44:34,441 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049627, Speed: 644.814677 images/s
2022-08-04 16:44:35,125 [dl_trainer.py:634] INFO train iter: 10976, num_batches_per_epoch: 196
2022-08-04 16:44:35,125 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 86.304209, lr: 1.000000, avg loss: 0.405306
2022-08-04 16:44:37,865 [dl_trainer.py:822] INFO Epoch 56, lr: 1.000000, val loss: 0.550283, val top-1 acc: 81.429712, top-5 acc: 99.071486
2022-08-04 16:44:38,971 [dl_trainer.py:731] WARNING [ 56][11000/  196][rank:0] loss: 0.522, average forward (0.008824) and backward (0.019167) time: 0.098611, iotime: 0.001356 
2022-08-04 16:44:39,040 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114981, Speed: 278.307005 images/s
2022-08-04 16:44:40,835 [dl_trainer.py:731] WARNING [ 56][11040/  196][rank:0] loss: 0.241, average forward (0.008975) and backward (0.019335) time: 0.029911, iotime: 0.001372 
2022-08-04 16:44:40,892 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046280, Speed: 691.437828 images/s
2022-08-04 16:44:42,661 [dl_trainer.py:731] WARNING [ 56][11080/  196][rank:0] loss: 0.407, average forward (0.009752) and backward (0.020169) time: 0.031596, iotime: 0.001433 
2022-08-04 16:44:42,720 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045694, Speed: 700.306751 images/s
2022-08-04 16:44:44,339 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:44:44,339 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:44:44,770 [dl_trainer.py:731] WARNING [ 56][11120/  196][rank:0] loss: 0.367, average forward (0.009432) and backward (0.019446) time: 0.034724, iotime: 0.005602 
2022-08-04 16:44:46,393 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048959, Speed: 653.607515 images/s
2022-08-04 16:44:46,564 [dl_trainer.py:731] WARNING [ 56][11160/  196][rank:0] loss: 0.355, average forward (0.010052) and backward (0.019785) time: 0.031581, iotime: 0.001487 
2022-08-04 16:44:47,123 [dl_trainer.py:634] INFO train iter: 11172, num_batches_per_epoch: 196
2022-08-04 16:44:47,123 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 86.463648, lr: 1.000000, avg loss: 0.392146
2022-08-04 16:44:49,793 [dl_trainer.py:822] INFO Epoch 57, lr: 1.000000, val loss: 0.698414, val top-1 acc: 77.795527, top-5 acc: 98.722045
2022-08-04 16:44:50,921 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113191, Speed: 282.707313 images/s
2022-08-04 16:44:51,093 [dl_trainer.py:731] WARNING [ 57][11200/  196][rank:0] loss: 0.342, average forward (0.009842) and backward (0.020647) time: 0.098998, iotime: 0.001451 
2022-08-04 16:44:52,818 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047411, Speed: 674.945362 images/s
2022-08-04 16:44:53,002 [dl_trainer.py:731] WARNING [ 57][11240/  196][rank:0] loss: 0.543, average forward (0.010448) and backward (0.021011) time: 0.033197, iotime: 0.001488 
2022-08-04 16:44:54,665 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046168, Speed: 693.122126 images/s
2022-08-04 16:44:54,857 [dl_trainer.py:731] WARNING [ 57][11280/  196][rank:0] loss: 0.488, average forward (0.009997) and backward (0.020697) time: 0.032461, iotime: 0.001525 
2022-08-04 16:44:56,301 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:44:56,301 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:44:56,919 [dl_trainer.py:731] WARNING [ 57][11320/  196][rank:0] loss: 0.433, average forward (0.010580) and backward (0.021197) time: 0.037612, iotime: 0.005568 
2022-08-04 16:44:58,387 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049596, Speed: 645.217235 images/s
2022-08-04 16:44:58,767 [dl_trainer.py:731] WARNING [ 57][11360/  196][rank:0] loss: 0.297, average forward (0.011297) and backward (0.019807) time: 0.033010, iotime: 0.001645 
2022-08-04 16:44:59,152 [dl_trainer.py:634] INFO train iter: 11368, num_batches_per_epoch: 196
2022-08-04 16:44:59,153 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 85.060587, lr: 1.000000, avg loss: 0.437092
2022-08-04 16:45:01,883 [dl_trainer.py:822] INFO Epoch 58, lr: 1.000000, val loss: 0.610754, val top-1 acc: 81.399760, top-5 acc: 98.562300
2022-08-04 16:45:02,981 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114838, Speed: 278.653759 images/s
2022-08-04 16:45:03,389 [dl_trainer.py:731] WARNING [ 58][11400/  196][rank:0] loss: 0.166, average forward (0.010512) and backward (0.019475) time: 0.100683, iotime: 0.001585 
2022-08-04 16:45:04,854 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046806, Speed: 683.667524 images/s
2022-08-04 16:45:05,279 [dl_trainer.py:731] WARNING [ 58][11440/  196][rank:0] loss: 0.445, average forward (0.010727) and backward (0.018800) time: 0.031329, iotime: 0.001539 
2022-08-04 16:45:06,777 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048059, Speed: 665.848743 images/s
2022-08-04 16:45:07,184 [dl_trainer.py:731] WARNING [ 58][11480/  196][rank:0] loss: 0.341, average forward (0.009530) and backward (0.018794) time: 0.030005, iotime: 0.001449 
2022-08-04 16:45:08,401 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:45:08,402 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:45:09,295 [dl_trainer.py:731] WARNING [ 58][11520/  196][rank:0] loss: 0.280, average forward (0.010229) and backward (0.020023) time: 0.036199, iotime: 0.005692 
2022-08-04 16:45:10,531 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050039, Speed: 639.496328 images/s
2022-08-04 16:45:11,162 [dl_trainer.py:731] WARNING [ 58][11560/  196][rank:0] loss: 0.348, average forward (0.008891) and backward (0.019760) time: 0.030223, iotime: 0.001344 
2022-08-04 16:45:11,382 [dl_trainer.py:634] INFO train iter: 11564, num_batches_per_epoch: 196
2022-08-04 16:45:11,382 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 85.060587, lr: 1.000000, avg loss: 0.419526
2022-08-04 16:45:14,065 [dl_trainer.py:822] INFO Epoch 59, lr: 1.000000, val loss: 0.573949, val top-1 acc: 81.389776, top-5 acc: 98.961661
2022-08-04 16:45:15,113 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114524, Speed: 279.417820 images/s
2022-08-04 16:45:15,784 [dl_trainer.py:731] WARNING [ 59][11600/  196][rank:0] loss: 0.321, average forward (0.009667) and backward (0.018387) time: 0.096878, iotime: 0.001432 
2022-08-04 16:45:17,114 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050029, Speed: 639.627140 images/s
2022-08-04 16:45:17,752 [dl_trainer.py:731] WARNING [ 59][11640/  196][rank:0] loss: 0.367, average forward (0.009812) and backward (0.018928) time: 0.030486, iotime: 0.001499 
2022-08-04 16:45:19,037 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048052, Speed: 665.940173 images/s
2022-08-04 16:45:19,679 [dl_trainer.py:731] WARNING [ 59][11680/  196][rank:0] loss: 0.391, average forward (0.009433) and backward (0.019517) time: 0.030597, iotime: 0.001416 
2022-08-04 16:45:20,597 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:45:20,597 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:45:21,722 [dl_trainer.py:731] WARNING [ 59][11720/  196][rank:0] loss: 0.570, average forward (0.010098) and backward (0.019288) time: 0.035045, iotime: 0.005402 
2022-08-04 16:45:22,743 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049389, Speed: 647.919680 images/s
2022-08-04 16:45:23,573 [dl_trainer.py:731] WARNING [ 59][11760/  196][rank:0] loss: 0.247, average forward (0.008881) and backward (0.019235) time: 0.029746, iotime: 0.001395 
2022-08-04 16:45:23,587 [dl_trainer.py:634] INFO train iter: 11760, num_batches_per_epoch: 196
2022-08-04 16:45:23,587 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 86.256378, lr: 1.000000, avg loss: 0.401253
2022-08-04 16:45:26,271 [dl_trainer.py:822] INFO Epoch 60, lr: 1.000000, val loss: 0.510644, val top-1 acc: 83.216853, top-5 acc: 99.241214
2022-08-04 16:45:27,257 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112836, Speed: 283.596212 images/s
2022-08-04 16:45:28,194 [dl_trainer.py:731] WARNING [ 60][11800/  196][rank:0] loss: 0.565, average forward (0.009451) and backward (0.019872) time: 0.099064, iotime: 0.001444 
2022-08-04 16:45:29,181 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048099, Speed: 665.298629 images/s
2022-08-04 16:45:30,053 [dl_trainer.py:731] WARNING [ 60][11840/  196][rank:0] loss: 0.500, average forward (0.010770) and backward (0.019183) time: 0.031818, iotime: 0.001605 
2022-08-04 16:45:30,997 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045374, Speed: 705.247756 images/s
2022-08-04 16:45:31,831 [dl_trainer.py:731] WARNING [ 60][11880/  196][rank:0] loss: 0.323, average forward (0.008919) and backward (0.018676) time: 0.029130, iotime: 0.001312 
2022-08-04 16:45:32,545 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:45:32,546 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:45:33,831 [dl_trainer.py:731] WARNING [ 60][11920/  196][rank:0] loss: 0.396, average forward (0.009218) and backward (0.019455) time: 0.034299, iotime: 0.005403 
2022-08-04 16:45:34,538 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047200, Speed: 677.971040 images/s
2022-08-04 16:45:35,435 [dl_trainer.py:634] INFO train iter: 11956, num_batches_per_epoch: 196
2022-08-04 16:45:35,436 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 86.192602, lr: 1.000000, avg loss: 0.407752
2022-08-04 16:45:38,119 [dl_trainer.py:822] INFO Epoch 61, lr: 1.000000, val loss: 0.553571, val top-1 acc: 81.349840, top-5 acc: 99.171326
2022-08-04 16:45:38,276 [dl_trainer.py:731] WARNING [ 61][11960/  196][rank:0] loss: 0.443, average forward (0.009648) and backward (0.017510) time: 0.095984, iotime: 0.001432 
2022-08-04 16:45:39,076 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113439, Speed: 282.089282 images/s
2022-08-04 16:45:40,220 [dl_trainer.py:731] WARNING [ 61][12000/  196][rank:0] loss: 0.383, average forward (0.009690) and backward (0.018761) time: 0.030259, iotime: 0.001561 
2022-08-04 16:45:40,934 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046433, Speed: 689.167219 images/s
2022-08-04 16:45:41,960 [dl_trainer.py:731] WARNING [ 61][12040/  196][rank:0] loss: 0.400, average forward (0.009689) and backward (0.018217) time: 0.029636, iotime: 0.001479 
2022-08-04 16:45:42,774 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045983, Speed: 695.910912 images/s
2022-08-04 16:45:43,907 [dl_trainer.py:731] WARNING [ 61][12080/  196][rank:0] loss: 0.550, average forward (0.008556) and backward (0.021115) time: 0.031252, iotime: 0.001345 
2022-08-04 16:45:44,408 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:45:44,408 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:45:45,985 [dl_trainer.py:731] WARNING [ 61][12120/  196][rank:0] loss: 0.385, average forward (0.010917) and backward (0.020909) time: 0.037959, iotime: 0.005846 
2022-08-04 16:45:46,489 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049518, Speed: 646.228115 images/s
2022-08-04 16:45:47,460 [dl_trainer.py:634] INFO train iter: 12152, num_batches_per_epoch: 196
2022-08-04 16:45:47,461 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 86.336097, lr: 1.000000, avg loss: 0.401563
2022-08-04 16:45:50,147 [dl_trainer.py:822] INFO Epoch 62, lr: 1.000000, val loss: 0.530683, val top-1 acc: 83.456470, top-5 acc: 98.921725
2022-08-04 16:45:50,569 [dl_trainer.py:731] WARNING [ 62][12160/  196][rank:0] loss: 0.268, average forward (0.009708) and backward (0.019959) time: 0.100391, iotime: 0.001472 
2022-08-04 16:45:51,134 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116106, Speed: 275.609630 images/s
2022-08-04 16:45:52,489 [dl_trainer.py:731] WARNING [ 62][12200/  196][rank:0] loss: 0.341, average forward (0.010108) and backward (0.018250) time: 0.030098, iotime: 0.001495 
2022-08-04 16:45:52,996 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046527, Speed: 687.767342 images/s
2022-08-04 16:45:54,264 [dl_trainer.py:731] WARNING [ 62][12240/  196][rank:0] loss: 0.254, average forward (0.010459) and backward (0.016409) time: 0.028601, iotime: 0.001493 
2022-08-04 16:45:54,808 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045294, Speed: 706.495269 images/s
2022-08-04 16:45:56,110 [dl_trainer.py:731] WARNING [ 62][12280/  196][rank:0] loss: 0.685, average forward (0.009879) and backward (0.016196) time: 0.027717, iotime: 0.001412 
2022-08-04 16:45:56,355 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:45:56,355 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:45:58,213 [dl_trainer.py:731] WARNING [ 62][12320/  196][rank:0] loss: 0.242, average forward (0.010381) and backward (0.020491) time: 0.036845, iotime: 0.005714 
2022-08-04 16:45:58,517 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049437, Speed: 647.291861 images/s
2022-08-04 16:45:59,566 [dl_trainer.py:634] INFO train iter: 12348, num_batches_per_epoch: 196
2022-08-04 16:45:59,566 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 86.973852, lr: 1.000000, avg loss: 0.377444
2022-08-04 16:46:02,221 [dl_trainer.py:822] INFO Epoch 63, lr: 1.000000, val loss: 0.566528, val top-1 acc: 80.760783, top-5 acc: 99.071486
2022-08-04 16:46:02,789 [dl_trainer.py:731] WARNING [ 63][12360/  196][rank:0] loss: 0.414, average forward (0.010791) and backward (0.021777) time: 0.100842, iotime: 0.001556 
2022-08-04 16:46:03,065 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113674, Speed: 281.507304 images/s
2022-08-04 16:46:04,622 [dl_trainer.py:731] WARNING [ 63][12400/  196][rank:0] loss: 0.649, average forward (0.010425) and backward (0.019610) time: 0.031824, iotime: 0.001538 
2022-08-04 16:46:04,904 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045970, Speed: 696.110596 images/s
2022-08-04 16:46:06,509 [dl_trainer.py:731] WARNING [ 63][12440/  196][rank:0] loss: 0.620, average forward (0.010355) and backward (0.019340) time: 0.031477, iotime: 0.001532 
2022-08-04 16:46:06,819 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047874, Speed: 668.428046 images/s
2022-08-04 16:46:08,364 [dl_trainer.py:731] WARNING [ 63][12480/  196][rank:0] loss: 0.391, average forward (0.009312) and backward (0.020572) time: 0.031530, iotime: 0.001418 
2022-08-04 16:46:08,381 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:46:08,381 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:46:10,440 [dl_trainer.py:731] WARNING [ 63][12520/  196][rank:0] loss: 0.385, average forward (0.010096) and backward (0.020846) time: 0.036900, iotime: 0.005690 
2022-08-04 16:46:10,494 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048987, Speed: 653.239354 images/s
2022-08-04 16:46:11,564 [dl_trainer.py:634] INFO train iter: 12544, num_batches_per_epoch: 196
2022-08-04 16:46:11,565 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 86.543367, lr: 1.000000, avg loss: 0.388639
2022-08-04 16:46:14,274 [dl_trainer.py:822] INFO Epoch 64, lr: 1.000000, val loss: 0.592911, val top-1 acc: 80.431310, top-5 acc: 99.221246
2022-08-04 16:46:15,061 [dl_trainer.py:731] WARNING [ 64][12560/  196][rank:0] loss: 0.284, average forward (0.009496) and backward (0.021473) time: 0.101519, iotime: 0.001452 
2022-08-04 16:46:15,120 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115629, Speed: 276.746257 images/s
2022-08-04 16:46:16,979 [dl_trainer.py:731] WARNING [ 64][12600/  196][rank:0] loss: 0.433, average forward (0.010001) and backward (0.022058) time: 0.033828, iotime: 0.001510 
2022-08-04 16:46:17,041 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048002, Speed: 666.638246 images/s
2022-08-04 16:46:18,883 [dl_trainer.py:731] WARNING [ 64][12640/  196][rank:0] loss: 0.114, average forward (0.010418) and backward (0.020880) time: 0.032987, iotime: 0.001442 
2022-08-04 16:46:18,936 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047386, Speed: 675.302103 images/s
2022-08-04 16:46:20,575 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:46:20,575 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:46:20,951 [dl_trainer.py:731] WARNING [ 64][12680/  196][rank:0] loss: 0.579, average forward (0.010270) and backward (0.021394) time: 0.037368, iotime: 0.005447 
2022-08-04 16:46:22,594 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048754, Speed: 656.360009 images/s
2022-08-04 16:46:22,770 [dl_trainer.py:731] WARNING [ 64][12720/  196][rank:0] loss: 0.401, average forward (0.010457) and backward (0.020364) time: 0.032542, iotime: 0.001465 
2022-08-04 16:46:23,700 [dl_trainer.py:634] INFO train iter: 12740, num_batches_per_epoch: 196
2022-08-04 16:46:23,700 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 86.304209, lr: 1.000000, avg loss: 0.397453
2022-08-04 16:46:26,363 [dl_trainer.py:822] INFO Epoch 65, lr: 1.000000, val loss: 0.707501, val top-1 acc: 78.494409, top-5 acc: 97.533946
2022-08-04 16:46:27,113 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112947, Speed: 283.318906 images/s
2022-08-04 16:46:27,291 [dl_trainer.py:731] WARNING [ 65][12760/  196][rank:0] loss: 0.609, average forward (0.009506) and backward (0.021274) time: 0.099058, iotime: 0.001406 
2022-08-04 16:46:29,032 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047967, Speed: 667.131715 images/s
2022-08-04 16:46:29,224 [dl_trainer.py:731] WARNING [ 65][12800/  196][rank:0] loss: 0.346, average forward (0.010948) and backward (0.021032) time: 0.033820, iotime: 0.001573 
2022-08-04 16:46:30,956 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048106, Speed: 665.195012 images/s
2022-08-04 16:46:31,140 [dl_trainer.py:731] WARNING [ 65][12840/  196][rank:0] loss: 0.509, average forward (0.009385) and backward (0.021625) time: 0.032679, iotime: 0.001434 
2022-08-04 16:46:32,585 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:46:32,585 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:46:33,206 [dl_trainer.py:731] WARNING [ 65][12880/  196][rank:0] loss: 0.239, average forward (0.010040) and backward (0.020200) time: 0.036770, iotime: 0.006272 
2022-08-04 16:46:34,665 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049423, Speed: 647.475260 images/s
2022-08-04 16:46:35,093 [dl_trainer.py:731] WARNING [ 65][12920/  196][rank:0] loss: 0.418, average forward (0.008983) and backward (0.021225) time: 0.031821, iotime: 0.001377 
2022-08-04 16:46:35,865 [dl_trainer.py:634] INFO train iter: 12936, num_batches_per_epoch: 196
2022-08-04 16:46:35,866 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 87.165179, lr: 1.000000, avg loss: 0.379880
2022-08-04 16:46:38,480 [dl_trainer.py:822] INFO Epoch 66, lr: 1.000000, val loss: 0.641658, val top-1 acc: 80.720847, top-5 acc: 98.911741
2022-08-04 16:46:39,252 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114681, Speed: 279.034469 images/s
2022-08-04 16:46:39,677 [dl_trainer.py:731] WARNING [ 66][12960/  196][rank:0] loss: 0.410, average forward (0.010466) and backward (0.021941) time: 0.101208, iotime: 0.001583 
2022-08-04 16:46:41,135 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047065, Speed: 679.904451 images/s
2022-08-04 16:46:41,553 [dl_trainer.py:731] WARNING [ 66][13000/  196][rank:0] loss: 0.366, average forward (0.009917) and backward (0.021532) time: 0.033253, iotime: 0.001529 
2022-08-04 16:46:43,044 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047717, Speed: 670.620631 images/s
2022-08-04 16:46:43,441 [dl_trainer.py:731] WARNING [ 66][13040/  196][rank:0] loss: 0.282, average forward (0.008975) and backward (0.021417) time: 0.032013, iotime: 0.001384 
2022-08-04 16:46:44,651 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:46:44,651 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:46:45,529 [dl_trainer.py:731] WARNING [ 66][13080/  196][rank:0] loss: 0.506, average forward (0.011279) and backward (0.020919) time: 0.038136, iotime: 0.005667 
2022-08-04 16:46:46,789 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049904, Speed: 641.231369 images/s
2022-08-04 16:46:47,432 [dl_trainer.py:731] WARNING [ 66][13120/  196][rank:0] loss: 0.184, average forward (0.009139) and backward (0.022172) time: 0.032987, iotime: 0.001449 
2022-08-04 16:46:48,016 [dl_trainer.py:634] INFO train iter: 13132, num_batches_per_epoch: 196
2022-08-04 16:46:48,016 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 86.575255, lr: 1.000000, avg loss: 0.408651
2022-08-04 16:46:50,673 [dl_trainer.py:822] INFO Epoch 67, lr: 1.000000, val loss: 0.467893, val top-1 acc: 84.175319, top-5 acc: 99.331070
2022-08-04 16:46:51,324 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113377, Speed: 282.243307 images/s
2022-08-04 16:46:51,983 [dl_trainer.py:731] WARNING [ 67][13160/  196][rank:0] loss: 0.371, average forward (0.010784) and backward (0.021221) time: 0.100387, iotime: 0.001632 
2022-08-04 16:46:53,201 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046902, Speed: 682.269329 images/s
2022-08-04 16:46:53,870 [dl_trainer.py:731] WARNING [ 67][13200/  196][rank:0] loss: 0.094, average forward (0.010521) and backward (0.021869) time: 0.034159, iotime: 0.001520 
2022-08-04 16:46:55,128 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048167, Speed: 664.358687 images/s
2022-08-04 16:46:55,790 [dl_trainer.py:731] WARNING [ 67][13240/  196][rank:0] loss: 0.467, average forward (0.010118) and backward (0.022498) time: 0.034412, iotime: 0.001552 
2022-08-04 16:46:56,763 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:46:56,764 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:46:57,841 [dl_trainer.py:731] WARNING [ 67][13280/  196][rank:0] loss: 0.332, average forward (0.010730) and backward (0.022014) time: 0.038611, iotime: 0.005606 
2022-08-04 16:46:58,811 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049081, Speed: 651.989604 images/s
2022-08-04 16:46:59,730 [dl_trainer.py:731] WARNING [ 67][13320/  196][rank:0] loss: 0.166, average forward (0.010827) and backward (0.021533) time: 0.034204, iotime: 0.001584 
2022-08-04 16:47:00,123 [dl_trainer.py:634] INFO train iter: 13328, num_batches_per_epoch: 196
2022-08-04 16:47:00,124 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 85.857781, lr: 1.000000, avg loss: 0.401699
2022-08-04 16:47:02,759 [dl_trainer.py:822] INFO Epoch 68, lr: 1.000000, val loss: 0.471596, val top-1 acc: 84.015575, top-5 acc: 99.390974
2022-08-04 16:47:03,415 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115086, Speed: 278.052139 images/s
2022-08-04 16:47:04,294 [dl_trainer.py:731] WARNING [ 68][13360/  196][rank:0] loss: 0.300, average forward (0.010767) and backward (0.019712) time: 0.098753, iotime: 0.001592 
2022-08-04 16:47:05,322 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047671, Speed: 671.267449 images/s
2022-08-04 16:47:06,191 [dl_trainer.py:731] WARNING [ 68][13400/  196][rank:0] loss: 0.443, average forward (0.009599) and backward (0.021804) time: 0.033058, iotime: 0.001417 
2022-08-04 16:47:07,211 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047212, Speed: 677.796966 images/s
2022-08-04 16:47:08,077 [dl_trainer.py:731] WARNING [ 68][13440/  196][rank:0] loss: 0.344, average forward (0.011198) and backward (0.020537) time: 0.033692, iotime: 0.001677 
2022-08-04 16:47:08,827 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:47:08,828 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:47:10,160 [dl_trainer.py:731] WARNING [ 68][13480/  196][rank:0] loss: 0.311, average forward (0.010588) and backward (0.021532) time: 0.038125, iotime: 0.005741 
2022-08-04 16:47:10,913 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049342, Speed: 648.528370 images/s
2022-08-04 16:47:12,040 [dl_trainer.py:731] WARNING [ 68][13520/  196][rank:0] loss: 0.442, average forward (0.009872) and backward (0.019333) time: 0.030886, iotime: 0.001431 
2022-08-04 16:47:12,258 [dl_trainer.py:634] INFO train iter: 13524, num_batches_per_epoch: 196
2022-08-04 16:47:12,258 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 85.634566, lr: 1.000000, avg loss: 0.411250
2022-08-04 16:47:14,939 [dl_trainer.py:822] INFO Epoch 69, lr: 1.000000, val loss: 0.712999, val top-1 acc: 78.883786, top-5 acc: 98.482428
2022-08-04 16:47:15,519 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115141, Speed: 277.918997 images/s
2022-08-04 16:47:16,640 [dl_trainer.py:731] WARNING [ 69][13560/  196][rank:0] loss: 0.198, average forward (0.010881) and backward (0.021175) time: 0.100966, iotime: 0.001553 
2022-08-04 16:47:17,404 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047097, Speed: 679.445652 images/s
2022-08-04 16:47:18,537 [dl_trainer.py:731] WARNING [ 69][13600/  196][rank:0] loss: 0.458, average forward (0.009919) and backward (0.020584) time: 0.032178, iotime: 0.001430 
2022-08-04 16:47:19,296 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047296, Speed: 676.589396 images/s
2022-08-04 16:47:20,412 [dl_trainer.py:731] WARNING [ 69][13640/  196][rank:0] loss: 0.600, average forward (0.009355) and backward (0.020163) time: 0.031168, iotime: 0.001423 
2022-08-04 16:47:20,900 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:47:20,900 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:47:22,518 [dl_trainer.py:731] WARNING [ 69][13680/  196][rank:0] loss: 0.521, average forward (0.009617) and backward (0.019628) time: 0.034880, iotime: 0.005389 
2022-08-04 16:47:23,051 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050054, Speed: 639.310883 images/s
2022-08-04 16:47:24,410 [dl_trainer.py:731] WARNING [ 69][13720/  196][rank:0] loss: 0.530, average forward (0.010384) and backward (0.020441) time: 0.032602, iotime: 0.001518 
2022-08-04 16:47:24,428 [dl_trainer.py:634] INFO train iter: 13720, num_batches_per_epoch: 196
2022-08-04 16:47:24,428 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 85.921556, lr: 1.000000, avg loss: 0.398349
2022-08-04 16:47:27,075 [dl_trainer.py:822] INFO Epoch 70, lr: 1.000000, val loss: 0.510749, val top-1 acc: 83.815895, top-5 acc: 98.951677
2022-08-04 16:47:27,653 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115039, Speed: 278.167638 images/s
2022-08-04 16:47:29,020 [dl_trainer.py:731] WARNING [ 70][13760/  196][rank:0] loss: 0.385, average forward (0.010950) and backward (0.021812) time: 0.101972, iotime: 0.001601 
2022-08-04 16:47:29,563 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047722, Speed: 670.547759 images/s
2022-08-04 16:47:30,938 [dl_trainer.py:731] WARNING [ 70][13800/  196][rank:0] loss: 0.322, average forward (0.010093) and backward (0.021342) time: 0.033141, iotime: 0.001461 
2022-08-04 16:47:31,467 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047580, Speed: 672.558011 images/s
2022-08-04 16:47:32,837 [dl_trainer.py:731] WARNING [ 70][13840/  196][rank:0] loss: 0.388, average forward (0.011160) and backward (0.022613) time: 0.035586, iotime: 0.001546 
2022-08-04 16:47:33,094 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:47:33,094 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:47:34,942 [dl_trainer.py:731] WARNING [ 70][13880/  196][rank:0] loss: 0.488, average forward (0.009991) and backward (0.020729) time: 0.036608, iotime: 0.005642 
2022-08-04 16:47:35,229 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050149, Speed: 638.096782 images/s
2022-08-04 16:47:36,609 [dl_trainer.py:634] INFO train iter: 13916, num_batches_per_epoch: 196
2022-08-04 16:47:36,609 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 86.862245, lr: 1.000000, avg loss: 0.381054
2022-08-04 16:47:39,279 [dl_trainer.py:822] INFO Epoch 71, lr: 1.000000, val loss: 0.618900, val top-1 acc: 80.241613, top-5 acc: 99.311102
2022-08-04 16:47:39,446 [dl_trainer.py:731] WARNING [ 71][13920/  196][rank:0] loss: 0.428, average forward (0.010221) and backward (0.019970) time: 0.098737, iotime: 0.001464 
2022-08-04 16:47:39,748 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112951, Speed: 283.308934 images/s
2022-08-04 16:47:41,376 [dl_trainer.py:731] WARNING [ 71][13960/  196][rank:0] loss: 0.336, average forward (0.011067) and backward (0.020583) time: 0.033536, iotime: 0.001618 
2022-08-04 16:47:41,657 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047721, Speed: 670.564929 images/s
2022-08-04 16:47:43,231 [dl_trainer.py:731] WARNING [ 71][14000/  196][rank:0] loss: 0.692, average forward (0.009488) and backward (0.020125) time: 0.031321, iotime: 0.001467 
2022-08-04 16:47:43,494 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045902, Speed: 697.136537 images/s
2022-08-04 16:47:45,030 [dl_trainer.py:731] WARNING [ 71][14040/  196][rank:0] loss: 0.540, average forward (0.009396) and backward (0.020343) time: 0.031388, iotime: 0.001406 
2022-08-04 16:47:45,045 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:47:45,045 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:47:47,131 [dl_trainer.py:731] WARNING [ 71][14080/  196][rank:0] loss: 0.332, average forward (0.011977) and backward (0.021446) time: 0.039677, iotime: 0.005971 
2022-08-04 16:47:47,179 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049117, Speed: 651.511164 images/s
2022-08-04 16:47:48,600 [dl_trainer.py:634] INFO train iter: 14112, num_batches_per_epoch: 196
2022-08-04 16:47:48,600 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 87.165179, lr: 1.000000, avg loss: 0.380877
2022-08-04 16:47:51,252 [dl_trainer.py:822] INFO Epoch 72, lr: 1.000000, val loss: 0.509763, val top-1 acc: 83.636182, top-5 acc: 99.091454
2022-08-04 16:47:51,630 [dl_trainer.py:731] WARNING [ 72][14120/  196][rank:0] loss: 0.288, average forward (0.009122) and backward (0.018563) time: 0.096184, iotime: 0.001403 
2022-08-04 16:47:51,677 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112435, Speed: 284.608143 images/s
2022-08-04 16:47:53,535 [dl_trainer.py:731] WARNING [ 72][14160/  196][rank:0] loss: 0.434, average forward (0.009248) and backward (0.020027) time: 0.030997, iotime: 0.001485 
2022-08-04 16:47:53,589 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047787, Speed: 669.631689 images/s
2022-08-04 16:47:55,395 [dl_trainer.py:731] WARNING [ 72][14200/  196][rank:0] loss: 0.479, average forward (0.010466) and backward (0.021996) time: 0.034282, iotime: 0.001569 
2022-08-04 16:47:55,452 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046569, Speed: 687.152725 images/s
2022-08-04 16:47:57,102 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:47:57,103 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:47:57,493 [dl_trainer.py:731] WARNING [ 72][14240/  196][rank:0] loss: 0.668, average forward (0.012084) and backward (0.022423) time: 0.040832, iotime: 0.006050 
2022-08-04 16:47:59,178 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049654, Speed: 644.453629 images/s
2022-08-04 16:47:59,352 [dl_trainer.py:731] WARNING [ 72][14280/  196][rank:0] loss: 0.362, average forward (0.010588) and backward (0.019802) time: 0.032156, iotime: 0.001509 
2022-08-04 16:48:00,669 [dl_trainer.py:634] INFO train iter: 14308, num_batches_per_epoch: 196
2022-08-04 16:48:00,669 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 86.049107, lr: 1.000000, avg loss: 0.403228
2022-08-04 16:48:03,359 [dl_trainer.py:822] INFO Epoch 73, lr: 1.000000, val loss: 0.629168, val top-1 acc: 80.411342, top-5 acc: 99.141374
2022-08-04 16:48:03,753 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114365, Speed: 279.805638 images/s
2022-08-04 16:48:03,937 [dl_trainer.py:731] WARNING [ 73][14320/  196][rank:0] loss: 0.349, average forward (0.009952) and backward (0.020395) time: 0.099375, iotime: 0.001466 
2022-08-04 16:48:05,680 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048174, Speed: 664.253143 images/s
2022-08-04 16:48:05,866 [dl_trainer.py:731] WARNING [ 73][14360/  196][rank:0] loss: 0.096, average forward (0.011019) and backward (0.021169) time: 0.034065, iotime: 0.001595 
2022-08-04 16:48:07,600 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047981, Speed: 666.935300 images/s
2022-08-04 16:48:07,780 [dl_trainer.py:731] WARNING [ 73][14400/  196][rank:0] loss: 0.513, average forward (0.010595) and backward (0.019095) time: 0.031564, iotime: 0.001609 
2022-08-04 16:48:09,252 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:48:09,253 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:48:09,926 [dl_trainer.py:731] WARNING [ 73][14440/  196][rank:0] loss: 0.412, average forward (0.010427) and backward (0.021241) time: 0.038001, iotime: 0.006065 
2022-08-04 16:48:11,366 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050190, Speed: 637.572192 images/s
2022-08-04 16:48:11,765 [dl_trainer.py:731] WARNING [ 73][14480/  196][rank:0] loss: 0.678, average forward (0.008932) and backward (0.020057) time: 0.030578, iotime: 0.001362 
2022-08-04 16:48:12,913 [dl_trainer.py:634] INFO train iter: 14504, num_batches_per_epoch: 196
2022-08-04 16:48:12,914 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 86.160714, lr: 1.000000, avg loss: 0.401848
2022-08-04 16:48:15,585 [dl_trainer.py:822] INFO Epoch 74, lr: 1.000000, val loss: 0.494728, val top-1 acc: 83.845847, top-5 acc: 99.131390
2022-08-04 16:48:15,942 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114406, Speed: 279.704411 images/s
2022-08-04 16:48:16,358 [dl_trainer.py:731] WARNING [ 74][14520/  196][rank:0] loss: 0.075, average forward (0.009627) and backward (0.021194) time: 0.100401, iotime: 0.001442 
2022-08-04 16:48:17,860 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047928, Speed: 667.670086 images/s
2022-08-04 16:48:18,262 [dl_trainer.py:731] WARNING [ 74][14560/  196][rank:0] loss: 0.262, average forward (0.011362) and backward (0.021923) time: 0.035162, iotime: 0.001613 
2022-08-04 16:48:19,745 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047096, Speed: 679.470245 images/s
2022-08-04 16:48:20,182 [dl_trainer.py:731] WARNING [ 74][14600/  196][rank:0] loss: 0.834, average forward (0.010561) and backward (0.022433) time: 0.034766, iotime: 0.001512 
2022-08-04 16:48:21,321 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:48:21,321 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:48:22,157 [dl_trainer.py:731] WARNING [ 74][14640/  196][rank:0] loss: 0.539, average forward (0.010150) and backward (0.019315) time: 0.035417, iotime: 0.005701 
2022-08-04 16:48:23,425 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049044, Speed: 652.477418 images/s
2022-08-04 16:48:24,056 [dl_trainer.py:731] WARNING [ 74][14680/  196][rank:0] loss: 0.478, average forward (0.010485) and backward (0.020617) time: 0.032977, iotime: 0.001608 
2022-08-04 16:48:25,014 [dl_trainer.py:634] INFO train iter: 14700, num_batches_per_epoch: 196
2022-08-04 16:48:25,015 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 86.383929, lr: 1.000000, avg loss: 0.401138
2022-08-04 16:48:27,700 [dl_trainer.py:822] INFO Epoch 75, lr: 1.000000, val loss: 0.458328, val top-1 acc: 84.894169, top-5 acc: 99.241214
2022-08-04 16:48:27,991 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114147, Speed: 280.339630 images/s
2022-08-04 16:48:28,640 [dl_trainer.py:731] WARNING [ 75][14720/  196][rank:0] loss: 0.386, average forward (0.010967) and backward (0.020507) time: 0.100562, iotime: 0.001614 
2022-08-04 16:48:29,893 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047550, Speed: 672.981738 images/s
2022-08-04 16:48:30,538 [dl_trainer.py:731] WARNING [ 75][14760/  196][rank:0] loss: 0.626, average forward (0.010262) and backward (0.019877) time: 0.031883, iotime: 0.001489 
2022-08-04 16:48:31,767 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046835, Speed: 683.249021 images/s
2022-08-04 16:48:32,428 [dl_trainer.py:731] WARNING [ 75][14800/  196][rank:0] loss: 0.349, average forward (0.009723) and backward (0.019812) time: 0.031216, iotime: 0.001435 
2022-08-04 16:48:33,419 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:48:33,419 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:48:34,451 [dl_trainer.py:731] WARNING [ 75][14840/  196][rank:0] loss: 0.323, average forward (0.009784) and backward (0.019956) time: 0.035550, iotime: 0.005560 
2022-08-04 16:48:35,440 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048958, Speed: 653.616682 images/s
2022-08-04 16:48:36,293 [dl_trainer.py:731] WARNING [ 75][14880/  196][rank:0] loss: 0.191, average forward (0.009768) and backward (0.018928) time: 0.030351, iotime: 0.001414 
2022-08-04 16:48:37,063 [dl_trainer.py:634] INFO train iter: 14896, num_batches_per_epoch: 196
2022-08-04 16:48:37,063 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 87.037628, lr: 1.000000, avg loss: 0.368121
2022-08-04 16:48:39,701 [dl_trainer.py:822] INFO Epoch 76, lr: 1.000000, val loss: 0.635527, val top-1 acc: 80.411342, top-5 acc: 98.682109
2022-08-04 16:48:39,968 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113172, Speed: 282.754348 images/s
2022-08-04 16:48:40,842 [dl_trainer.py:731] WARNING [ 76][14920/  196][rank:0] loss: 0.418, average forward (0.009894) and backward (0.021360) time: 0.099850, iotime: 0.001469 
2022-08-04 16:48:41,846 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046957, Speed: 681.477596 images/s
2022-08-04 16:48:42,692 [dl_trainer.py:731] WARNING [ 76][14960/  196][rank:0] loss: 0.195, average forward (0.009531) and backward (0.020755) time: 0.031937, iotime: 0.001404 
2022-08-04 16:48:43,727 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047017, Speed: 680.609689 images/s
2022-08-04 16:48:44,612 [dl_trainer.py:731] WARNING [ 76][15000/  196][rank:0] loss: 0.432, average forward (0.010152) and backward (0.020931) time: 0.032881, iotime: 0.001536 
2022-08-04 16:48:45,330 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:48:45,331 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:48:46,637 [dl_trainer.py:731] WARNING [ 76][15040/  196][rank:0] loss: 0.301, average forward (0.010268) and backward (0.022247) time: 0.038262, iotime: 0.005488 
2022-08-04 16:48:47,360 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048423, Speed: 660.838419 images/s
2022-08-04 16:48:48,496 [dl_trainer.py:731] WARNING [ 76][15080/  196][rank:0] loss: 0.355, average forward (0.009754) and backward (0.019385) time: 0.030834, iotime: 0.001449 
2022-08-04 16:48:49,111 [dl_trainer.py:634] INFO train iter: 15092, num_batches_per_epoch: 196
2022-08-04 16:48:49,112 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 86.639031, lr: 1.000000, avg loss: 0.391445
2022-08-04 16:48:51,781 [dl_trainer.py:822] INFO Epoch 77, lr: 1.000000, val loss: 0.753456, val top-1 acc: 78.214856, top-5 acc: 98.592252
2022-08-04 16:48:51,965 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115113, Speed: 277.988791 images/s
2022-08-04 16:48:53,014 [dl_trainer.py:731] WARNING [ 77][15120/  196][rank:0] loss: 0.270, average forward (0.010175) and backward (0.017974) time: 0.096684, iotime: 0.001472 
2022-08-04 16:48:53,790 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045612, Speed: 701.571574 images/s
2022-08-04 16:48:54,892 [dl_trainer.py:731] WARNING [ 77][15160/  196][rank:0] loss: 0.901, average forward (0.009044) and backward (0.020861) time: 0.031518, iotime: 0.001376 
2022-08-04 16:48:55,644 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046349, Speed: 690.411888 images/s
2022-08-04 16:48:56,678 [dl_trainer.py:731] WARNING [ 77][15200/  196][rank:0] loss: 0.308, average forward (0.010862) and backward (0.018330) time: 0.031013, iotime: 0.001562 
2022-08-04 16:48:57,162 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:48:57,163 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:48:58,733 [dl_trainer.py:731] WARNING [ 77][15240/  196][rank:0] loss: 0.300, average forward (0.009189) and backward (0.020188) time: 0.035027, iotime: 0.005430 
2022-08-04 16:48:59,265 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048259, Speed: 663.095248 images/s
2022-08-04 16:49:00,600 [dl_trainer.py:731] WARNING [ 77][15280/  196][rank:0] loss: 0.536, average forward (0.009924) and backward (0.021169) time: 0.032853, iotime: 0.001509 
2022-08-04 16:49:01,011 [dl_trainer.py:634] INFO train iter: 15288, num_batches_per_epoch: 196
2022-08-04 16:49:01,011 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 86.734694, lr: 1.000000, avg loss: 0.386612
2022-08-04 16:49:03,668 [dl_trainer.py:822] INFO Epoch 78, lr: 1.000000, val loss: 0.519462, val top-1 acc: 83.107029, top-5 acc: 99.201278
2022-08-04 16:49:03,869 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115078, Speed: 278.071150 images/s
2022-08-04 16:49:05,262 [dl_trainer.py:731] WARNING [ 78][15320/  196][rank:0] loss: 0.280, average forward (0.010139) and backward (0.022014) time: 0.102233, iotime: 0.001484 
2022-08-04 16:49:05,820 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048765, Speed: 656.210141 images/s
2022-08-04 16:49:07,194 [dl_trainer.py:731] WARNING [ 78][15360/  196][rank:0] loss: 0.183, average forward (0.011918) and backward (0.023302) time: 0.037177, iotime: 0.001673 
2022-08-04 16:49:07,754 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048327, Speed: 662.155908 images/s
2022-08-04 16:49:09,165 [dl_trainer.py:731] WARNING [ 78][15400/  196][rank:0] loss: 0.332, average forward (0.010770) and backward (0.023438) time: 0.036070, iotime: 0.001591 
2022-08-04 16:49:09,425 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:49:09,425 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:49:11,206 [dl_trainer.py:731] WARNING [ 78][15440/  196][rank:0] loss: 0.384, average forward (0.009139) and backward (0.021425) time: 0.036631, iotime: 0.005832 
2022-08-04 16:49:11,504 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049987, Speed: 640.168908 images/s
2022-08-04 16:49:13,062 [dl_trainer.py:731] WARNING [ 78][15480/  196][rank:0] loss: 0.214, average forward (0.008928) and backward (0.019325) time: 0.029753, iotime: 0.001282 
2022-08-04 16:49:13,265 [dl_trainer.py:634] INFO train iter: 15484, num_batches_per_epoch: 196
2022-08-04 16:49:13,265 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 87.053571, lr: 1.000000, avg loss: 0.373024
2022-08-04 16:49:15,987 [dl_trainer.py:822] INFO Epoch 79, lr: 1.000000, val loss: 0.850049, val top-1 acc: 76.537540, top-5 acc: 97.424121
2022-08-04 16:49:16,080 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114389, Speed: 279.748164 images/s
2022-08-04 16:49:17,688 [dl_trainer.py:731] WARNING [ 79][15520/  196][rank:0] loss: 0.255, average forward (0.010548) and backward (0.021384) time: 0.101840, iotime: 0.001507 
2022-08-04 16:49:17,982 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047529, Speed: 673.267502 images/s
2022-08-04 16:49:19,552 [dl_trainer.py:731] WARNING [ 79][15560/  196][rank:0] loss: 0.289, average forward (0.010512) and backward (0.020693) time: 0.032989, iotime: 0.001523 
2022-08-04 16:49:19,845 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046557, Speed: 687.326910 images/s
2022-08-04 16:49:21,410 [dl_trainer.py:731] WARNING [ 79][15600/  196][rank:0] loss: 0.488, average forward (0.011086) and backward (0.021030) time: 0.033953, iotime: 0.001580 
2022-08-04 16:49:21,414 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:49:21,414 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:49:23,393 [dl_trainer.py:731] WARNING [ 79][15640/  196][rank:0] loss: 0.108, average forward (0.009306) and backward (0.021388) time: 0.036481, iotime: 0.005556 
2022-08-04 16:49:23,455 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048118, Speed: 665.036585 images/s
2022-08-04 16:49:25,318 [dl_trainer.py:731] WARNING [ 79][15680/  196][rank:0] loss: 0.749, average forward (0.010431) and backward (0.020885) time: 0.033090, iotime: 0.001523 
2022-08-04 16:49:25,330 [dl_trainer.py:634] INFO train iter: 15680, num_batches_per_epoch: 196
2022-08-04 16:49:25,331 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 86.176658, lr: 1.000000, avg loss: 0.396056
2022-08-04 16:49:28,038 [dl_trainer.py:822] INFO Epoch 80, lr: 1.000000, val loss: 0.476146, val top-1 acc: 84.484824, top-5 acc: 99.321086
2022-08-04 16:49:28,133 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116940, Speed: 273.644219 images/s
2022-08-04 16:49:30,028 [dl_trainer.py:731] WARNING [ 80][15720/  196][rank:0] loss: 0.326, average forward (0.009605) and backward (0.022794) time: 0.103113, iotime: 0.001510 
2022-08-04 16:49:30,085 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048803, Speed: 655.702899 images/s
2022-08-04 16:49:31,945 [dl_trainer.py:731] WARNING [ 80][15760/  196][rank:0] loss: 0.603, average forward (0.010302) and backward (0.020359) time: 0.032449, iotime: 0.001540 
2022-08-04 16:49:32,014 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048207, Speed: 663.810374 images/s
2022-08-04 16:49:33,681 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:49:33,681 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:49:34,078 [dl_trainer.py:731] WARNING [ 80][15800/  196][rank:0] loss: 0.155, average forward (0.009287) and backward (0.022281) time: 0.037571, iotime: 0.005759 
2022-08-04 16:49:35,725 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049465, Speed: 646.922297 images/s
2022-08-04 16:49:35,886 [dl_trainer.py:731] WARNING [ 80][15840/  196][rank:0] loss: 0.308, average forward (0.009368) and backward (0.020376) time: 0.031381, iotime: 0.001398 
2022-08-04 16:49:37,544 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045465, Speed: 703.839894 images/s
2022-08-04 16:49:37,545 [dl_trainer.py:634] INFO train iter: 15876, num_batches_per_epoch: 196
2022-08-04 16:49:37,545 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 86.511480, lr: 1.000000, avg loss: 0.395730
2022-08-04 16:49:40,228 [dl_trainer.py:822] INFO Epoch 81, lr: 1.000000, val loss: 0.569054, val top-1 acc: 81.649361, top-5 acc: 98.991613
2022-08-04 16:49:40,411 [dl_trainer.py:731] WARNING [ 81][15880/  196][rank:0] loss: 0.318, average forward (0.010066) and backward (0.020063) time: 0.099010, iotime: 0.001493 
2022-08-04 16:49:42,091 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113670, Speed: 281.516707 images/s
2022-08-04 16:49:42,256 [dl_trainer.py:731] WARNING [ 81][15920/  196][rank:0] loss: 0.258, average forward (0.010143) and backward (0.021577) time: 0.033480, iotime: 0.001516 
2022-08-04 16:49:43,992 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047493, Speed: 673.784029 images/s
2022-08-04 16:49:44,166 [dl_trainer.py:731] WARNING [ 81][15960/  196][rank:0] loss: 0.196, average forward (0.009061) and backward (0.021321) time: 0.032016, iotime: 0.001411 
2022-08-04 16:49:45,654 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:49:45,654 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:49:46,279 [dl_trainer.py:731] WARNING [ 81][16000/  196][rank:0] loss: 0.270, average forward (0.010396) and backward (0.022561) time: 0.038948, iotime: 0.005730 
2022-08-04 16:49:47,739 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049947, Speed: 640.683713 images/s
2022-08-04 16:49:48,136 [dl_trainer.py:731] WARNING [ 81][16040/  196][rank:0] loss: 0.716, average forward (0.009526) and backward (0.018711) time: 0.029985, iotime: 0.001493 
2022-08-04 16:49:49,587 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046195, Speed: 692.712170 images/s
2022-08-04 16:49:49,637 [dl_trainer.py:634] INFO train iter: 16072, num_batches_per_epoch: 196
2022-08-04 16:49:49,637 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 90.927934, lr: 0.100000, avg loss: 0.262064
2022-08-04 16:49:52,358 [dl_trainer.py:822] INFO Epoch 82, lr: 0.100000, val loss: 0.307821, val top-1 acc: 89.736422, top-5 acc: 99.710463
2022-08-04 16:49:52,790 [dl_trainer.py:731] WARNING [ 82][16080/  196][rank:0] loss: 0.315, average forward (0.009148) and backward (0.021485) time: 0.101898, iotime: 0.001377 
2022-08-04 16:49:54,297 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.117719, Speed: 271.832644 images/s
2022-08-04 16:49:54,700 [dl_trainer.py:731] WARNING [ 82][16120/  196][rank:0] loss: 0.332, average forward (0.010107) and backward (0.021991) time: 0.033822, iotime: 0.001474 
2022-08-04 16:49:56,176 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046973, Speed: 681.244289 images/s
2022-08-04 16:49:56,595 [dl_trainer.py:731] WARNING [ 82][16160/  196][rank:0] loss: 0.395, average forward (0.009258) and backward (0.018919) time: 0.029918, iotime: 0.001491 
2022-08-04 16:49:57,771 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:49:57,771 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:49:58,630 [dl_trainer.py:731] WARNING [ 82][16200/  196][rank:0] loss: 0.191, average forward (0.009247) and backward (0.020208) time: 0.035099, iotime: 0.005398 
2022-08-04 16:49:59,870 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049242, Speed: 649.852374 images/s
2022-08-04 16:50:00,541 [dl_trainer.py:731] WARNING [ 82][16240/  196][rank:0] loss: 0.164, average forward (0.009129) and backward (0.020176) time: 0.030995, iotime: 0.001416 
2022-08-04 16:50:01,783 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047804, Speed: 669.393067 images/s
2022-08-04 16:50:01,880 [dl_trainer.py:634] INFO train iter: 16268, num_batches_per_epoch: 196
2022-08-04 16:50:01,881 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 92.299107, lr: 0.100000, avg loss: 0.221206
2022-08-04 16:50:04,612 [dl_trainer.py:822] INFO Epoch 83, lr: 0.100000, val loss: 0.294782, val top-1 acc: 90.275559, top-5 acc: 99.740415
2022-08-04 16:50:05,165 [dl_trainer.py:731] WARNING [ 83][16280/  196][rank:0] loss: 0.623, average forward (0.011120) and backward (0.020338) time: 0.101744, iotime: 0.001650 
2022-08-04 16:50:06,432 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116188, Speed: 275.414600 images/s
2022-08-04 16:50:07,103 [dl_trainer.py:731] WARNING [ 83][16320/  196][rank:0] loss: 0.161, average forward (0.009986) and backward (0.022505) time: 0.034345, iotime: 0.001592 
2022-08-04 16:50:08,331 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047473, Speed: 674.071660 images/s
2022-08-04 16:50:09,001 [dl_trainer.py:731] WARNING [ 83][16360/  196][rank:0] loss: 0.205, average forward (0.009874) and backward (0.021138) time: 0.032698, iotime: 0.001446 
2022-08-04 16:50:10,001 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:50:10,001 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:50:11,071 [dl_trainer.py:731] WARNING [ 83][16400/  196][rank:0] loss: 0.098, average forward (0.012206) and backward (0.021007) time: 0.039512, iotime: 0.006018 
2022-08-04 16:50:12,048 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049537, Speed: 645.976187 images/s
2022-08-04 16:50:12,888 [dl_trainer.py:731] WARNING [ 83][16440/  196][rank:0] loss: 0.194, average forward (0.010202) and backward (0.018066) time: 0.030029, iotime: 0.001498 
2022-08-04 16:50:13,904 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046373, Speed: 690.061005 images/s
2022-08-04 16:50:14,060 [dl_trainer.py:634] INFO train iter: 16464, num_batches_per_epoch: 196
2022-08-04 16:50:14,060 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 92.873087, lr: 0.100000, avg loss: 0.209685
2022-08-04 16:50:16,738 [dl_trainer.py:822] INFO Epoch 84, lr: 0.100000, val loss: 0.294598, val top-1 acc: 90.385383, top-5 acc: 99.750399
2022-08-04 16:50:17,524 [dl_trainer.py:731] WARNING [ 84][16480/  196][rank:0] loss: 0.380, average forward (0.010829) and backward (0.019834) time: 0.100380, iotime: 0.001579 
2022-08-04 16:50:18,549 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116126, Speed: 275.562126 images/s
2022-08-04 16:50:19,404 [dl_trainer.py:731] WARNING [ 84][16520/  196][rank:0] loss: 0.210, average forward (0.010249) and backward (0.020242) time: 0.032213, iotime: 0.001467 
2022-08-04 16:50:20,399 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046233, Speed: 692.143650 images/s
2022-08-04 16:50:21,295 [dl_trainer.py:731] WARNING [ 84][16560/  196][rank:0] loss: 0.243, average forward (0.008999) and backward (0.021198) time: 0.031763, iotime: 0.001347 
2022-08-04 16:50:22,025 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:50:22,025 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:50:23,356 [dl_trainer.py:731] WARNING [ 84][16600/  196][rank:0] loss: 0.280, average forward (0.009092) and backward (0.020697) time: 0.035222, iotime: 0.005191 
2022-08-04 16:50:24,109 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049456, Speed: 647.035650 images/s
2022-08-04 16:50:25,253 [dl_trainer.py:731] WARNING [ 84][16640/  196][rank:0] loss: 0.090, average forward (0.009522) and backward (0.021359) time: 0.032556, iotime: 0.001428 
2022-08-04 16:50:26,005 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047377, Speed: 675.432600 images/s
2022-08-04 16:50:26,201 [dl_trainer.py:634] INFO train iter: 16660, num_batches_per_epoch: 196
2022-08-04 16:50:26,201 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 94.005102, lr: 0.100000, avg loss: 0.177554
2022-08-04 16:50:28,945 [dl_trainer.py:822] INFO Epoch 85, lr: 0.100000, val loss: 0.301014, val top-1 acc: 90.425319, top-5 acc: 99.730431
2022-08-04 16:50:29,911 [dl_trainer.py:731] WARNING [ 85][16680/  196][rank:0] loss: 0.208, average forward (0.010395) and backward (0.022015) time: 0.102858, iotime: 0.001509 
2022-08-04 16:50:30,675 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116748, Speed: 274.094717 images/s
2022-08-04 16:50:31,825 [dl_trainer.py:731] WARNING [ 85][16720/  196][rank:0] loss: 0.326, average forward (0.010768) and backward (0.022552) time: 0.035182, iotime: 0.001606 
2022-08-04 16:50:32,600 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048122, Speed: 664.974614 images/s
2022-08-04 16:50:33,736 [dl_trainer.py:731] WARNING [ 85][16760/  196][rank:0] loss: 0.195, average forward (0.010387) and backward (0.021789) time: 0.033973, iotime: 0.001548 
2022-08-04 16:50:34,247 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:50:34,247 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:50:35,825 [dl_trainer.py:731] WARNING [ 85][16800/  196][rank:0] loss: 0.159, average forward (0.010567) and backward (0.021444) time: 0.038016, iotime: 0.005743 
2022-08-04 16:50:36,343 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049887, Speed: 641.451854 images/s
2022-08-04 16:50:37,695 [dl_trainer.py:731] WARNING [ 85][16840/  196][rank:0] loss: 0.179, average forward (0.010318) and backward (0.020937) time: 0.033014, iotime: 0.001512 
2022-08-04 16:50:38,249 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047631, Speed: 671.837655 images/s
2022-08-04 16:50:38,484 [dl_trainer.py:634] INFO train iter: 16856, num_batches_per_epoch: 196
2022-08-04 16:50:38,485 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 94.339923, lr: 0.100000, avg loss: 0.173328
2022-08-04 16:50:41,162 [dl_trainer.py:822] INFO Epoch 86, lr: 0.100000, val loss: 0.293833, val top-1 acc: 90.615016, top-5 acc: 99.770367
2022-08-04 16:50:42,286 [dl_trainer.py:731] WARNING [ 86][16880/  196][rank:0] loss: 0.152, average forward (0.010348) and backward (0.020790) time: 0.100684, iotime: 0.001500 
2022-08-04 16:50:42,810 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114003, Speed: 280.695242 images/s
2022-08-04 16:50:44,121 [dl_trainer.py:731] WARNING [ 86][16920/  196][rank:0] loss: 0.113, average forward (0.008551) and backward (0.022269) time: 0.032373, iotime: 0.001342 
2022-08-04 16:50:44,641 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045778, Speed: 699.024453 images/s
2022-08-04 16:50:46,016 [dl_trainer.py:731] WARNING [ 86][16960/  196][rank:0] loss: 0.047, average forward (0.009657) and backward (0.020281) time: 0.031621, iotime: 0.001451 
2022-08-04 16:50:46,289 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:50:46,289 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:50:48,077 [dl_trainer.py:731] WARNING [ 86][17000/  196][rank:0] loss: 0.063, average forward (0.008728) and backward (0.020000) time: 0.034948, iotime: 0.005991 
2022-08-04 16:50:48,398 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050067, Speed: 639.141169 images/s
2022-08-04 16:50:49,982 [dl_trainer.py:731] WARNING [ 86][17040/  196][rank:0] loss: 0.194, average forward (0.009410) and backward (0.020264) time: 0.031325, iotime: 0.001423 
2022-08-04 16:50:50,276 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046942, Speed: 681.694529 images/s
2022-08-04 16:50:50,577 [dl_trainer.py:634] INFO train iter: 17052, num_batches_per_epoch: 196
2022-08-04 16:50:50,577 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 94.244260, lr: 0.100000, avg loss: 0.175669
2022-08-04 16:50:53,244 [dl_trainer.py:822] INFO Epoch 87, lr: 0.100000, val loss: 0.295117, val top-1 acc: 90.605032, top-5 acc: 99.720447
2022-08-04 16:50:54,608 [dl_trainer.py:731] WARNING [ 87][17080/  196][rank:0] loss: 0.240, average forward (0.010008) and backward (0.021748) time: 0.100251, iotime: 0.001504 
2022-08-04 16:50:54,889 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115334, Speed: 277.455134 images/s
2022-08-04 16:50:56,489 [dl_trainer.py:731] WARNING [ 87][17120/  196][rank:0] loss: 0.242, average forward (0.011045) and backward (0.019826) time: 0.032768, iotime: 0.001637 
2022-08-04 16:50:56,744 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046352, Speed: 690.364834 images/s
2022-08-04 16:50:58,308 [dl_trainer.py:731] WARNING [ 87][17160/  196][rank:0] loss: 0.229, average forward (0.008879) and backward (0.020623) time: 0.031159, iotime: 0.001431 
2022-08-04 16:50:58,322 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:50:58,323 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:51:00,360 [dl_trainer.py:731] WARNING [ 87][17200/  196][rank:0] loss: 0.050, average forward (0.009112) and backward (0.018784) time: 0.033785, iotime: 0.005652 
2022-08-04 16:51:00,413 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048899, Speed: 654.415954 images/s
2022-08-04 16:51:02,234 [dl_trainer.py:731] WARNING [ 87][17240/  196][rank:0] loss: 0.530, average forward (0.009595) and backward (0.020810) time: 0.032067, iotime: 0.001424 
2022-08-04 16:51:02,298 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047124, Speed: 679.054115 images/s
2022-08-04 16:51:02,629 [dl_trainer.py:634] INFO train iter: 17248, num_batches_per_epoch: 196
2022-08-04 16:51:02,630 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 94.563138, lr: 0.100000, avg loss: 0.165250
2022-08-04 16:51:05,307 [dl_trainer.py:822] INFO Epoch 88, lr: 0.100000, val loss: 0.289742, val top-1 acc: 90.664936, top-5 acc: 99.790335
2022-08-04 16:51:06,843 [dl_trainer.py:731] WARNING [ 88][17280/  196][rank:0] loss: 0.158, average forward (0.009941) and backward (0.019715) time: 0.099827, iotime: 0.001498 
2022-08-04 16:51:06,911 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115301, Speed: 277.534723 images/s
2022-08-04 16:51:08,700 [dl_trainer.py:731] WARNING [ 88][17320/  196][rank:0] loss: 0.165, average forward (0.009380) and backward (0.021016) time: 0.032111, iotime: 0.001478 
2022-08-04 16:51:08,761 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046243, Speed: 691.993593 images/s
2022-08-04 16:51:10,378 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:51:10,379 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:51:10,798 [dl_trainer.py:731] WARNING [ 88][17360/  196][rank:0] loss: 0.163, average forward (0.011338) and backward (0.019785) time: 0.037592, iotime: 0.006189 
2022-08-04 16:51:12,467 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049391, Speed: 647.896619 images/s
2022-08-04 16:51:12,619 [dl_trainer.py:731] WARNING [ 88][17400/  196][rank:0] loss: 0.132, average forward (0.009659) and backward (0.017935) time: 0.029262, iotime: 0.001435 
2022-08-04 16:51:14,353 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047145, Speed: 678.756553 images/s
2022-08-04 16:51:14,537 [dl_trainer.py:731] WARNING [ 88][17440/  196][rank:0] loss: 0.221, average forward (0.010155) and backward (0.021968) time: 0.033875, iotime: 0.001493 
2022-08-04 16:51:14,734 [dl_trainer.py:634] INFO train iter: 17444, num_batches_per_epoch: 196
2022-08-04 16:51:14,734 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 94.515306, lr: 0.100000, avg loss: 0.158985
2022-08-04 16:51:17,464 [dl_trainer.py:822] INFO Epoch 89, lr: 0.100000, val loss: 0.278355, val top-1 acc: 91.204073, top-5 acc: 99.780351
2022-08-04 16:51:18,988 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115872, Speed: 276.167255 images/s
2022-08-04 16:51:19,167 [dl_trainer.py:731] WARNING [ 89][17480/  196][rank:0] loss: 0.187, average forward (0.009914) and backward (0.020709) time: 0.100678, iotime: 0.001483 
2022-08-04 16:51:20,859 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046749, Speed: 684.500137 images/s
2022-08-04 16:51:21,046 [dl_trainer.py:731] WARNING [ 89][17520/  196][rank:0] loss: 0.031, average forward (0.008830) and backward (0.020576) time: 0.031090, iotime: 0.001456 
2022-08-04 16:51:22,494 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:51:22,495 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:51:23,096 [dl_trainer.py:731] WARNING [ 89][17560/  196][rank:0] loss: 0.110, average forward (0.008550) and backward (0.021261) time: 0.035413, iotime: 0.005384 
2022-08-04 16:51:24,608 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049970, Speed: 640.380557 images/s
2022-08-04 16:51:25,045 [dl_trainer.py:731] WARNING [ 89][17600/  196][rank:0] loss: 0.124, average forward (0.008694) and backward (0.021112) time: 0.031383, iotime: 0.001338 
2022-08-04 16:51:26,508 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047482, Speed: 673.940165 images/s
2022-08-04 16:51:26,910 [dl_trainer.py:731] WARNING [ 89][17640/  196][rank:0] loss: 0.105, average forward (0.010561) and backward (0.021316) time: 0.033672, iotime: 0.001529 
2022-08-04 16:51:26,918 [dl_trainer.py:634] INFO train iter: 17640, num_batches_per_epoch: 196
2022-08-04 16:51:26,918 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 95.041454, lr: 0.100000, avg loss: 0.140948
2022-08-04 16:51:29,579 [dl_trainer.py:822] INFO Epoch 90, lr: 0.100000, val loss: 0.306350, val top-1 acc: 90.465256, top-5 acc: 99.720447
2022-08-04 16:51:31,042 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113346, Speed: 282.322416 images/s
2022-08-04 16:51:31,447 [dl_trainer.py:731] WARNING [ 90][17680/  196][rank:0] loss: 0.135, average forward (0.010010) and backward (0.020917) time: 0.099757, iotime: 0.001546 
2022-08-04 16:51:32,934 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047295, Speed: 676.601504 images/s
2022-08-04 16:51:33,380 [dl_trainer.py:731] WARNING [ 90][17720/  196][rank:0] loss: 0.047, average forward (0.009298) and backward (0.021305) time: 0.032266, iotime: 0.001423 
2022-08-04 16:51:34,599 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:51:34,599 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:51:35,461 [dl_trainer.py:731] WARNING [ 90][17760/  196][rank:0] loss: 0.083, average forward (0.011056) and backward (0.019855) time: 0.036667, iotime: 0.005495 
2022-08-04 16:51:36,658 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049628, Speed: 644.795099 images/s
2022-08-04 16:51:37,292 [dl_trainer.py:731] WARNING [ 90][17800/  196][rank:0] loss: 0.118, average forward (0.009929) and backward (0.019307) time: 0.030985, iotime: 0.001499 
2022-08-04 16:51:38,505 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046174, Speed: 693.029074 images/s
2022-08-04 16:51:38,971 [dl_trainer.py:634] INFO train iter: 17836, num_batches_per_epoch: 196
2022-08-04 16:51:38,971 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 94.834184, lr: 0.100000, avg loss: 0.151214
2022-08-04 16:51:41,648 [dl_trainer.py:822] INFO Epoch 91, lr: 0.100000, val loss: 0.297082, val top-1 acc: 90.585064, top-5 acc: 99.750399
2022-08-04 16:51:41,839 [dl_trainer.py:731] WARNING [ 91][17840/  196][rank:0] loss: 0.190, average forward (0.009902) and backward (0.021598) time: 0.100202, iotime: 0.001467 
2022-08-04 16:51:43,056 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113756, Speed: 281.304283 images/s
2022-08-04 16:51:43,727 [dl_trainer.py:731] WARNING [ 91][17880/  196][rank:0] loss: 0.097, average forward (0.010370) and backward (0.021338) time: 0.033462, iotime: 0.001509 
2022-08-04 16:51:45,008 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048790, Speed: 655.866471 images/s
2022-08-04 16:51:45,659 [dl_trainer.py:731] WARNING [ 91][17920/  196][rank:0] loss: 0.143, average forward (0.010367) and backward (0.020977) time: 0.033193, iotime: 0.001587 
2022-08-04 16:51:46,632 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:51:46,632 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:51:47,746 [dl_trainer.py:731] WARNING [ 91][17960/  196][rank:0] loss: 0.187, average forward (0.011448) and backward (0.020643) time: 0.038192, iotime: 0.005850 
2022-08-04 16:51:48,783 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050316, Speed: 635.980924 images/s
2022-08-04 16:51:49,628 [dl_trainer.py:731] WARNING [ 91][18000/  196][rank:0] loss: 0.274, average forward (0.008173) and backward (0.019132) time: 0.028860, iotime: 0.001340 
2022-08-04 16:51:50,615 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045794, Speed: 698.781072 images/s
2022-08-04 16:51:51,140 [dl_trainer.py:634] INFO train iter: 18032, num_batches_per_epoch: 196
2022-08-04 16:51:51,140 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 95.041454, lr: 0.100000, avg loss: 0.138299
2022-08-04 16:51:53,839 [dl_trainer.py:822] INFO Epoch 92, lr: 0.100000, val loss: 0.289374, val top-1 acc: 91.094249, top-5 acc: 99.750399
2022-08-04 16:51:54,214 [dl_trainer.py:731] WARNING [ 92][18040/  196][rank:0] loss: 0.020, average forward (0.009502) and backward (0.021187) time: 0.100389, iotime: 0.001461 
2022-08-04 16:51:55,179 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114079, Speed: 280.507796 images/s
2022-08-04 16:51:56,020 [dl_trainer.py:731] WARNING [ 92][18080/  196][rank:0] loss: 0.182, average forward (0.009398) and backward (0.019715) time: 0.030786, iotime: 0.001440 
2022-08-04 16:51:57,040 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046535, Speed: 687.651852 images/s
2022-08-04 16:51:57,912 [dl_trainer.py:731] WARNING [ 92][18120/  196][rank:0] loss: 0.159, average forward (0.009760) and backward (0.021240) time: 0.032681, iotime: 0.001433 
2022-08-04 16:51:58,625 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:51:58,625 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:52:00,006 [dl_trainer.py:731] WARNING [ 92][18160/  196][rank:0] loss: 0.090, average forward (0.009805) and backward (0.019509) time: 0.035072, iotime: 0.005505 
2022-08-04 16:52:00,777 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049804, Speed: 642.520877 images/s
2022-08-04 16:52:01,945 [dl_trainer.py:731] WARNING [ 92][18200/  196][rank:0] loss: 0.158, average forward (0.011881) and backward (0.022549) time: 0.036354, iotime: 0.001635 
2022-08-04 16:52:02,735 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048954, Speed: 653.669731 images/s
2022-08-04 16:52:03,317 [dl_trainer.py:634] INFO train iter: 18228, num_batches_per_epoch: 196
2022-08-04 16:52:03,317 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 95.376276, lr: 0.100000, avg loss: 0.139387
2022-08-04 16:52:05,879 [dl_trainer.py:822] INFO Epoch 93, lr: 0.100000, val loss: 0.293222, val top-1 acc: 90.884585, top-5 acc: 99.780351
2022-08-04 16:52:06,424 [dl_trainer.py:731] WARNING [ 93][18240/  196][rank:0] loss: 0.086, average forward (0.010809) and backward (0.021052) time: 0.097810, iotime: 0.001562 
2022-08-04 16:52:07,215 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.111969, Speed: 285.794408 images/s
2022-08-04 16:52:08,339 [dl_trainer.py:731] WARNING [ 93][18280/  196][rank:0] loss: 0.152, average forward (0.008547) and backward (0.021010) time: 0.031066, iotime: 0.001299 
2022-08-04 16:52:09,086 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046777, Speed: 684.089766 images/s
2022-08-04 16:52:10,225 [dl_trainer.py:731] WARNING [ 93][18320/  196][rank:0] loss: 0.095, average forward (0.010144) and backward (0.021831) time: 0.033715, iotime: 0.001502 
2022-08-04 16:52:10,730 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:52:10,730 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:52:12,286 [dl_trainer.py:731] WARNING [ 93][18360/  196][rank:0] loss: 0.035, average forward (0.009880) and backward (0.019611) time: 0.035300, iotime: 0.005568 
2022-08-04 16:52:12,845 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050106, Speed: 638.643385 images/s
2022-08-04 16:52:14,239 [dl_trainer.py:731] WARNING [ 93][18400/  196][rank:0] loss: 0.098, average forward (0.008386) and backward (0.018825) time: 0.028765, iotime: 0.001325 
2022-08-04 16:52:14,783 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048435, Speed: 660.682234 images/s
2022-08-04 16:52:15,401 [dl_trainer.py:634] INFO train iter: 18424, num_batches_per_epoch: 196
2022-08-04 16:52:15,401 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 95.280612, lr: 0.100000, avg loss: 0.131888
2022-08-04 16:52:18,060 [dl_trainer.py:822] INFO Epoch 94, lr: 0.100000, val loss: 0.295746, val top-1 acc: 90.814696, top-5 acc: 99.780351
2022-08-04 16:52:18,843 [dl_trainer.py:731] WARNING [ 94][18440/  196][rank:0] loss: 0.061, average forward (0.009074) and backward (0.019498) time: 0.098309, iotime: 0.001393 
2022-08-04 16:52:19,385 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115036, Speed: 278.174830 images/s
2022-08-04 16:52:20,753 [dl_trainer.py:731] WARNING [ 94][18480/  196][rank:0] loss: 0.068, average forward (0.008971) and backward (0.019655) time: 0.030242, iotime: 0.001375 
2022-08-04 16:52:21,294 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047721, Speed: 670.563086 images/s
2022-08-04 16:52:22,632 [dl_trainer.py:731] WARNING [ 94][18520/  196][rank:0] loss: 0.285, average forward (0.010136) and backward (0.020021) time: 0.031910, iotime: 0.001500 
2022-08-04 16:52:22,887 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:52:22,887 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:52:24,700 [dl_trainer.py:731] WARNING [ 94][18560/  196][rank:0] loss: 0.041, average forward (0.010891) and backward (0.019097) time: 0.036176, iotime: 0.005888 
2022-08-04 16:52:25,005 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049465, Speed: 646.928034 images/s
2022-08-04 16:52:26,592 [dl_trainer.py:731] WARNING [ 94][18600/  196][rank:0] loss: 0.147, average forward (0.011885) and backward (0.021679) time: 0.035529, iotime: 0.001692 
2022-08-04 16:52:26,882 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046887, Speed: 682.484857 images/s
2022-08-04 16:52:27,519 [dl_trainer.py:634] INFO train iter: 18620, num_batches_per_epoch: 196
2022-08-04 16:52:27,520 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 95.137117, lr: 0.100000, avg loss: 0.133293
2022-08-04 16:52:30,244 [dl_trainer.py:822] INFO Epoch 95, lr: 0.100000, val loss: 0.295574, val top-1 acc: 91.084265, top-5 acc: 99.730431
2022-08-04 16:52:31,193 [dl_trainer.py:731] WARNING [ 95][18640/  196][rank:0] loss: 0.006, average forward (0.010500) and backward (0.020216) time: 0.100712, iotime: 0.001555 
2022-08-04 16:52:31,491 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115218, Speed: 277.734637 images/s
2022-08-04 16:52:33,070 [dl_trainer.py:731] WARNING [ 95][18680/  196][rank:0] loss: 0.070, average forward (0.008547) and backward (0.019348) time: 0.029473, iotime: 0.001357 
2022-08-04 16:52:33,363 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046794, Speed: 683.855016 images/s
2022-08-04 16:52:34,937 [dl_trainer.py:731] WARNING [ 95][18720/  196][rank:0] loss: 0.058, average forward (0.009021) and backward (0.020315) time: 0.031014, iotime: 0.001456 
2022-08-04 16:52:34,949 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:52:34,949 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:52:36,989 [dl_trainer.py:731] WARNING [ 95][18760/  196][rank:0] loss: 0.028, average forward (0.009850) and backward (0.019941) time: 0.035674, iotime: 0.005640 
2022-08-04 16:52:37,041 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049029, Speed: 652.673078 images/s
2022-08-04 16:52:38,907 [dl_trainer.py:731] WARNING [ 95][18800/  196][rank:0] loss: 0.029, average forward (0.010458) and backward (0.020282) time: 0.032581, iotime: 0.001574 
2022-08-04 16:52:38,970 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048202, Speed: 663.869556 images/s
2022-08-04 16:52:39,700 [dl_trainer.py:634] INFO train iter: 18816, num_batches_per_epoch: 196
2022-08-04 16:52:39,700 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 96.014031, lr: 0.100000, avg loss: 0.124816
2022-08-04 16:52:42,344 [dl_trainer.py:822] INFO Epoch 96, lr: 0.100000, val loss: 0.298750, val top-1 acc: 90.994409, top-5 acc: 99.830272
2022-08-04 16:52:43,521 [dl_trainer.py:731] WARNING [ 96][18840/  196][rank:0] loss: 0.110, average forward (0.010017) and backward (0.021807) time: 0.100201, iotime: 0.001483 
2022-08-04 16:52:43,585 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115371, Speed: 277.367006 images/s
2022-08-04 16:52:45,449 [dl_trainer.py:731] WARNING [ 96][18880/  196][rank:0] loss: 0.034, average forward (0.009743) and backward (0.020436) time: 0.031908, iotime: 0.001472 
2022-08-04 16:52:45,517 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048282, Speed: 662.768902 images/s
2022-08-04 16:52:47,168 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:52:47,168 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:52:47,562 [dl_trainer.py:731] WARNING [ 96][18920/  196][rank:0] loss: 0.213, average forward (0.009684) and backward (0.020702) time: 0.036569, iotime: 0.005929 
2022-08-04 16:52:49,282 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050187, Speed: 637.615969 images/s
2022-08-04 16:52:49,466 [dl_trainer.py:731] WARNING [ 96][18960/  196][rank:0] loss: 0.112, average forward (0.011030) and backward (0.020730) time: 0.033635, iotime: 0.001603 
2022-08-04 16:52:51,184 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047555, Speed: 672.904304 images/s
2022-08-04 16:52:51,348 [dl_trainer.py:731] WARNING [ 96][19000/  196][rank:0] loss: 0.234, average forward (0.009597) and backward (0.020860) time: 0.032199, iotime: 0.001493 
2022-08-04 16:52:51,954 [dl_trainer.py:634] INFO train iter: 19012, num_batches_per_epoch: 196
2022-08-04 16:52:51,954 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 95.998087, lr: 0.100000, avg loss: 0.118347
2022-08-04 16:52:54,701 [dl_trainer.py:822] INFO Epoch 97, lr: 0.100000, val loss: 0.321332, val top-1 acc: 90.425319, top-5 acc: 99.800319
2022-08-04 16:52:55,852 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116667, Speed: 274.284378 images/s
2022-08-04 16:52:56,028 [dl_trainer.py:731] WARNING [ 97][19040/  196][rank:0] loss: 0.102, average forward (0.009281) and backward (0.020990) time: 0.100736, iotime: 0.001485 
2022-08-04 16:52:57,741 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047204, Speed: 677.914990 images/s
2022-08-04 16:52:57,922 [dl_trainer.py:731] WARNING [ 97][19080/  196][rank:0] loss: 0.036, average forward (0.010974) and backward (0.022146) time: 0.035023, iotime: 0.001642 
2022-08-04 16:52:59,390 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:52:59,390 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:53:00,032 [dl_trainer.py:731] WARNING [ 97][19120/  196][rank:0] loss: 0.236, average forward (0.012108) and backward (0.021616) time: 0.040180, iotime: 0.006174 
2022-08-04 16:53:01,457 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049525, Speed: 646.140509 images/s
2022-08-04 16:53:01,873 [dl_trainer.py:731] WARNING [ 97][19160/  196][rank:0] loss: 0.095, average forward (0.009913) and backward (0.018782) time: 0.030506, iotime: 0.001548 
2022-08-04 16:53:03,362 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047607, Speed: 672.175632 images/s
2022-08-04 16:53:03,785 [dl_trainer.py:731] WARNING [ 97][19200/  196][rank:0] loss: 0.152, average forward (0.009838) and backward (0.019616) time: 0.031191, iotime: 0.001493 
2022-08-04 16:53:04,184 [dl_trainer.py:634] INFO train iter: 19208, num_batches_per_epoch: 196
2022-08-04 16:53:04,185 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 95.487883, lr: 0.100000, avg loss: 0.124898
2022-08-04 16:53:06,803 [dl_trainer.py:822] INFO Epoch 98, lr: 0.100000, val loss: 0.304799, val top-1 acc: 90.754792, top-5 acc: 99.810304
2022-08-04 16:53:07,985 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115548, Speed: 276.941335 images/s
2022-08-04 16:53:08,414 [dl_trainer.py:731] WARNING [ 98][19240/  196][rank:0] loss: 0.286, average forward (0.008729) and backward (0.019284) time: 0.096513, iotime: 0.001390 
2022-08-04 16:53:09,837 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046300, Speed: 691.142574 images/s
2022-08-04 16:53:10,233 [dl_trainer.py:731] WARNING [ 98][19280/  196][rank:0] loss: 0.083, average forward (0.008976) and backward (0.018685) time: 0.029252, iotime: 0.001354 
2022-08-04 16:53:11,395 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:53:11,395 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:53:12,201 [dl_trainer.py:731] WARNING [ 98][19320/  196][rank:0] loss: 0.028, average forward (0.009102) and backward (0.018400) time: 0.033375, iotime: 0.005634 
2022-08-04 16:53:13,398 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047462, Speed: 674.218226 images/s
2022-08-04 16:53:14,034 [dl_trainer.py:731] WARNING [ 98][19360/  196][rank:0] loss: 0.070, average forward (0.008382) and backward (0.022040) time: 0.031884, iotime: 0.001250 
2022-08-04 16:53:15,249 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046267, Speed: 691.633973 images/s
2022-08-04 16:53:15,892 [dl_trainer.py:731] WARNING [ 98][19400/  196][rank:0] loss: 0.184, average forward (0.009789) and backward (0.020460) time: 0.031968, iotime: 0.001471 
2022-08-04 16:53:16,076 [dl_trainer.py:634] INFO train iter: 19404, num_batches_per_epoch: 196
2022-08-04 16:53:16,076 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 95.583546, lr: 0.100000, avg loss: 0.120916
2022-08-04 16:53:18,754 [dl_trainer.py:822] INFO Epoch 99, lr: 0.100000, val loss: 0.326670, val top-1 acc: 90.515176, top-5 acc: 99.710463
2022-08-04 16:53:19,815 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114146, Speed: 280.341855 images/s
2022-08-04 16:53:20,426 [dl_trainer.py:731] WARNING [ 99][19440/  196][rank:0] loss: 0.043, average forward (0.009761) and backward (0.019825) time: 0.098350, iotime: 0.001478 
2022-08-04 16:53:21,603 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044675, Speed: 716.287453 images/s
2022-08-04 16:53:22,260 [dl_trainer.py:731] WARNING [ 99][19480/  196][rank:0] loss: 0.142, average forward (0.009666) and backward (0.020756) time: 0.032061, iotime: 0.001407 
2022-08-04 16:53:23,266 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:53:23,267 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:53:24,338 [dl_trainer.py:731] WARNING [ 99][19520/  196][rank:0] loss: 0.193, average forward (0.009101) and backward (0.021909) time: 0.037097, iotime: 0.005824 
2022-08-04 16:53:25,304 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049332, Speed: 648.662517 images/s
2022-08-04 16:53:26,183 [dl_trainer.py:731] WARNING [ 99][19560/  196][rank:0] loss: 0.116, average forward (0.010138) and backward (0.018745) time: 0.030653, iotime: 0.001516 
2022-08-04 16:53:27,199 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047343, Speed: 675.913309 images/s
2022-08-04 16:53:28,070 [dl_trainer.py:731] WARNING [ 99][19600/  196][rank:0] loss: 0.130, average forward (0.010556) and backward (0.020561) time: 0.033110, iotime: 0.001718 
2022-08-04 16:53:28,077 [dl_trainer.py:634] INFO train iter: 19600, num_batches_per_epoch: 196
2022-08-04 16:53:28,078 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 95.455995, lr: 0.100000, avg loss: 0.125472
2022-08-04 16:53:30,769 [dl_trainer.py:822] INFO Epoch 100, lr: 0.100000, val loss: 0.308893, val top-1 acc: 90.694888, top-5 acc: 99.760383
2022-08-04 16:53:31,812 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115313, Speed: 277.505688 images/s
2022-08-04 16:53:32,682 [dl_trainer.py:731] WARNING [100][19640/  196][rank:0] loss: 0.241, average forward (0.010714) and backward (0.018333) time: 0.100311, iotime: 0.001609 
2022-08-04 16:53:33,709 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047378, Speed: 675.415436 images/s
2022-08-04 16:53:34,570 [dl_trainer.py:731] WARNING [100][19680/  196][rank:0] loss: 0.125, average forward (0.009551) and backward (0.017442) time: 0.028726, iotime: 0.001498 
2022-08-04 16:53:35,300 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:53:35,301 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:53:36,631 [dl_trainer.py:731] WARNING [100][19720/  196][rank:0] loss: 0.061, average forward (0.008917) and backward (0.019262) time: 0.033753, iotime: 0.005345 
2022-08-04 16:53:37,379 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048914, Speed: 654.214868 images/s
2022-08-04 16:53:38,491 [dl_trainer.py:731] WARNING [100][19760/  196][rank:0] loss: 0.033, average forward (0.010683) and backward (0.020120) time: 0.032655, iotime: 0.001593 
2022-08-04 16:53:39,270 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047267, Speed: 677.004133 images/s
2022-08-04 16:53:40,219 [dl_trainer.py:634] INFO train iter: 19796, num_batches_per_epoch: 196
2022-08-04 16:53:40,219 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 96.396684, lr: 0.100000, avg loss: 0.110273
2022-08-04 16:53:42,924 [dl_trainer.py:822] INFO Epoch 101, lr: 0.100000, val loss: 0.310030, val top-1 acc: 90.864617, top-5 acc: 99.640575
2022-08-04 16:53:43,093 [dl_trainer.py:731] WARNING [101][19800/  196][rank:0] loss: 0.135, average forward (0.010216) and backward (0.020192) time: 0.099899, iotime: 0.001529 
2022-08-04 16:53:43,856 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114637, Speed: 279.142932 images/s
2022-08-04 16:53:44,972 [dl_trainer.py:731] WARNING [101][19840/  196][rank:0] loss: 0.168, average forward (0.010006) and backward (0.019187) time: 0.030920, iotime: 0.001466 
2022-08-04 16:53:45,770 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047832, Speed: 669.001856 images/s
2022-08-04 16:53:46,933 [dl_trainer.py:731] WARNING [101][19880/  196][rank:0] loss: 0.086, average forward (0.011454) and backward (0.021518) time: 0.034828, iotime: 0.001586 
2022-08-04 16:53:47,431 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:53:47,431 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:53:48,957 [dl_trainer.py:731] WARNING [101][19920/  196][rank:0] loss: 0.068, average forward (0.009258) and backward (0.018360) time: 0.033632, iotime: 0.005788 
2022-08-04 16:53:49,520 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049975, Speed: 640.325564 images/s
2022-08-04 16:53:50,855 [dl_trainer.py:731] WARNING [101][19960/  196][rank:0] loss: 0.214, average forward (0.010799) and backward (0.020649) time: 0.033323, iotime: 0.001603 
2022-08-04 16:53:51,395 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046864, Speed: 682.831811 images/s
2022-08-04 16:53:52,427 [dl_trainer.py:634] INFO train iter: 19992, num_batches_per_epoch: 196
2022-08-04 16:53:52,427 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 96.029974, lr: 0.100000, avg loss: 0.111482
2022-08-04 16:53:55,088 [dl_trainer.py:822] INFO Epoch 102, lr: 0.100000, val loss: 0.317445, val top-1 acc: 90.794728, top-5 acc: 99.760383
2022-08-04 16:53:55,450 [dl_trainer.py:731] WARNING [102][20000/  196][rank:0] loss: 0.146, average forward (0.009462) and backward (0.020367) time: 0.098879, iotime: 0.001447 
2022-08-04 16:53:55,973 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114436, Speed: 279.632734 images/s
2022-08-04 16:53:57,324 [dl_trainer.py:731] WARNING [102][20040/  196][rank:0] loss: 0.231, average forward (0.010001) and backward (0.022055) time: 0.033793, iotime: 0.001484 
2022-08-04 16:53:57,802 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045708, Speed: 700.100179 images/s
2022-08-04 16:53:59,097 [dl_trainer.py:731] WARNING [102][20080/  196][rank:0] loss: 0.136, average forward (0.010213) and backward (0.019749) time: 0.031711, iotime: 0.001504 
2022-08-04 16:53:59,353 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:53:59,353 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:54:01,182 [dl_trainer.py:731] WARNING [102][20120/  196][rank:0] loss: 0.176, average forward (0.009661) and backward (0.019400) time: 0.034859, iotime: 0.005559 
2022-08-04 16:54:01,478 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048991, Speed: 653.184801 images/s
2022-08-04 16:54:03,113 [dl_trainer.py:731] WARNING [102][20160/  196][rank:0] loss: 0.081, average forward (0.009147) and backward (0.020930) time: 0.031735, iotime: 0.001437 
2022-08-04 16:54:03,399 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048021, Speed: 666.373878 images/s
2022-08-04 16:54:04,437 [dl_trainer.py:634] INFO train iter: 20188, num_batches_per_epoch: 196
2022-08-04 16:54:04,437 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 95.232781, lr: 0.100000, avg loss: 0.126740
2022-08-04 16:54:07,084 [dl_trainer.py:822] INFO Epoch 103, lr: 0.100000, val loss: 0.357101, val top-1 acc: 89.956070, top-5 acc: 99.630591
2022-08-04 16:54:07,651 [dl_trainer.py:731] WARNING [103][20200/  196][rank:0] loss: 0.104, average forward (0.009992) and backward (0.020385) time: 0.098374, iotime: 0.001502 
2022-08-04 16:54:07,939 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113481, Speed: 281.985775 images/s
2022-08-04 16:54:09,542 [dl_trainer.py:731] WARNING [103][20240/  196][rank:0] loss: 0.093, average forward (0.009352) and backward (0.021389) time: 0.032402, iotime: 0.001434 
2022-08-04 16:54:09,828 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047214, Speed: 677.762226 images/s
2022-08-04 16:54:11,430 [dl_trainer.py:731] WARNING [103][20280/  196][rank:0] loss: 0.031, average forward (0.009699) and backward (0.020302) time: 0.031710, iotime: 0.001453 
2022-08-04 16:54:11,440 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:54:11,440 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:54:13,505 [dl_trainer.py:731] WARNING [103][20320/  196][rank:0] loss: 0.128, average forward (0.010434) and backward (0.021377) time: 0.037686, iotime: 0.005609 
2022-08-04 16:54:13,577 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049970, Speed: 640.385527 images/s
2022-08-04 16:54:15,464 [dl_trainer.py:731] WARNING [103][20360/  196][rank:0] loss: 0.011, average forward (0.010581) and backward (0.021808) time: 0.034184, iotime: 0.001536 
2022-08-04 16:54:15,525 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048676, Speed: 657.410797 images/s
2022-08-04 16:54:16,627 [dl_trainer.py:634] INFO train iter: 20384, num_batches_per_epoch: 196
2022-08-04 16:54:16,627 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 95.998087, lr: 0.100000, avg loss: 0.115175
2022-08-04 16:54:19,305 [dl_trainer.py:822] INFO Epoch 104, lr: 0.100000, val loss: 0.315782, val top-1 acc: 90.944489, top-5 acc: 99.730431
2022-08-04 16:54:20,105 [dl_trainer.py:731] WARNING [104][20400/  196][rank:0] loss: 0.045, average forward (0.011331) and backward (0.021581) time: 0.103491, iotime: 0.001668 
2022-08-04 16:54:20,167 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116051, Speed: 275.740427 images/s
2022-08-04 16:54:22,014 [dl_trainer.py:731] WARNING [104][20440/  196][rank:0] loss: 0.028, average forward (0.009895) and backward (0.021615) time: 0.033270, iotime: 0.001515 
2022-08-04 16:54:22,070 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047545, Speed: 673.042229 images/s
2022-08-04 16:54:23,718 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:54:23,719 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:54:24,127 [dl_trainer.py:731] WARNING [104][20480/  196][rank:0] loss: 0.084, average forward (0.012595) and backward (0.022727) time: 0.041598, iotime: 0.005983 
2022-08-04 16:54:25,867 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050594, Speed: 632.486791 images/s
2022-08-04 16:54:26,031 [dl_trainer.py:731] WARNING [104][20520/  196][rank:0] loss: 0.102, average forward (0.011063) and backward (0.020159) time: 0.033018, iotime: 0.001549 
2022-08-04 16:54:27,749 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047046, Speed: 680.182596 images/s
2022-08-04 16:54:27,901 [dl_trainer.py:731] WARNING [104][20560/  196][rank:0] loss: 0.069, average forward (0.010126) and backward (0.017126) time: 0.028927, iotime: 0.001424 
2022-08-04 16:54:28,849 [dl_trainer.py:634] INFO train iter: 20580, num_batches_per_epoch: 196
2022-08-04 16:54:28,849 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 95.790816, lr: 0.100000, avg loss: 0.117178
2022-08-04 16:54:31,487 [dl_trainer.py:822] INFO Epoch 105, lr: 0.100000, val loss: 0.316216, val top-1 acc: 90.954473, top-5 acc: 99.710463
2022-08-04 16:54:32,276 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113165, Speed: 282.772443 images/s
2022-08-04 16:54:32,448 [dl_trainer.py:731] WARNING [105][20600/  196][rank:0] loss: 0.005, average forward (0.009681) and backward (0.019984) time: 0.097389, iotime: 0.001463 
2022-08-04 16:54:34,136 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046494, Speed: 688.266657 images/s
2022-08-04 16:54:34,316 [dl_trainer.py:731] WARNING [105][20640/  196][rank:0] loss: 0.124, average forward (0.010231) and backward (0.021264) time: 0.033209, iotime: 0.001460 
2022-08-04 16:54:35,746 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:54:35,746 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:54:36,375 [dl_trainer.py:731] WARNING [105][20680/  196][rank:0] loss: 0.158, average forward (0.011565) and backward (0.020662) time: 0.038282, iotime: 0.005797 
2022-08-04 16:54:37,832 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049249, Speed: 649.758540 images/s
2022-08-04 16:54:38,228 [dl_trainer.py:731] WARNING [105][20720/  196][rank:0] loss: 0.055, average forward (0.009850) and backward (0.019444) time: 0.030988, iotime: 0.001453 
2022-08-04 16:54:39,688 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046399, Speed: 689.670696 images/s
2022-08-04 16:54:40,102 [dl_trainer.py:731] WARNING [105][20760/  196][rank:0] loss: 0.100, average forward (0.009038) and backward (0.019917) time: 0.030567, iotime: 0.001389 
2022-08-04 16:54:40,875 [dl_trainer.py:634] INFO train iter: 20776, num_batches_per_epoch: 196
2022-08-04 16:54:40,875 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 95.982143, lr: 0.100000, avg loss: 0.115243
2022-08-04 16:54:43,538 [dl_trainer.py:822] INFO Epoch 106, lr: 0.100000, val loss: 0.354213, val top-1 acc: 89.796326, top-5 acc: 99.670527
2022-08-04 16:54:44,298 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115244, Speed: 277.672309 images/s
2022-08-04 16:54:44,708 [dl_trainer.py:731] WARNING [106][20800/  196][rank:0] loss: 0.032, average forward (0.009618) and backward (0.021479) time: 0.100572, iotime: 0.001454 
2022-08-04 16:54:46,189 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047277, Speed: 676.867310 images/s
2022-08-04 16:54:46,624 [dl_trainer.py:731] WARNING [106][20840/  196][rank:0] loss: 0.131, average forward (0.010246) and backward (0.020297) time: 0.032286, iotime: 0.001497 
2022-08-04 16:54:47,852 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:54:47,852 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:54:48,731 [dl_trainer.py:731] WARNING [106][20880/  196][rank:0] loss: 0.310, average forward (0.009432) and backward (0.021145) time: 0.036189, iotime: 0.005358 
2022-08-04 16:54:49,958 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050233, Speed: 637.036451 images/s
2022-08-04 16:54:50,632 [dl_trainer.py:731] WARNING [106][20920/  196][rank:0] loss: 0.067, average forward (0.009748) and backward (0.021233) time: 0.032726, iotime: 0.001489 
2022-08-04 16:54:51,836 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046946, Speed: 681.634203 images/s
2022-08-04 16:54:52,497 [dl_trainer.py:731] WARNING [106][20960/  196][rank:0] loss: 0.066, average forward (0.010340) and backward (0.020635) time: 0.032758, iotime: 0.001518 
2022-08-04 16:54:53,072 [dl_trainer.py:634] INFO train iter: 20972, num_batches_per_epoch: 196
2022-08-04 16:54:53,073 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 95.679209, lr: 0.100000, avg loss: 0.117605
2022-08-04 16:54:55,743 [dl_trainer.py:822] INFO Epoch 107, lr: 0.100000, val loss: 0.321451, val top-1 acc: 90.684904, top-5 acc: 99.770367
2022-08-04 16:54:56,370 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113316, Speed: 282.396860 images/s
2022-08-04 16:54:56,978 [dl_trainer.py:731] WARNING [107][21000/  196][rank:0] loss: 0.095, average forward (0.009895) and backward (0.019890) time: 0.098239, iotime: 0.001391 
2022-08-04 16:54:58,168 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044952, Speed: 711.870452 images/s
2022-08-04 16:54:58,802 [dl_trainer.py:731] WARNING [107][21040/  196][rank:0] loss: 0.024, average forward (0.009400) and backward (0.019214) time: 0.030201, iotime: 0.001354 
2022-08-04 16:54:59,712 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:54:59,712 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:55:00,810 [dl_trainer.py:731] WARNING [107][21080/  196][rank:0] loss: 0.167, average forward (0.008900) and backward (0.018764) time: 0.033617, iotime: 0.005708 
2022-08-04 16:55:01,807 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048504, Speed: 659.743990 images/s
2022-08-04 16:55:02,671 [dl_trainer.py:731] WARNING [107][21120/  196][rank:0] loss: 0.065, average forward (0.009210) and backward (0.021635) time: 0.032559, iotime: 0.001478 
2022-08-04 16:55:03,672 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046616, Speed: 686.456338 images/s
2022-08-04 16:55:04,557 [dl_trainer.py:731] WARNING [107][21160/  196][rank:0] loss: 0.131, average forward (0.009153) and backward (0.021561) time: 0.032330, iotime: 0.001383 
2022-08-04 16:55:04,951 [dl_trainer.py:634] INFO train iter: 21168, num_batches_per_epoch: 196
2022-08-04 16:55:04,951 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 95.806760, lr: 0.100000, avg loss: 0.117040
2022-08-04 16:55:07,648 [dl_trainer.py:822] INFO Epoch 108, lr: 0.100000, val loss: 0.357389, val top-1 acc: 89.926118, top-5 acc: 99.680511
2022-08-04 16:55:08,368 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.117374, Speed: 272.631722 images/s
2022-08-04 16:55:09,217 [dl_trainer.py:731] WARNING [108][21200/  196][rank:0] loss: 0.130, average forward (0.010455) and backward (0.019261) time: 0.102035, iotime: 0.001484 
2022-08-04 16:55:10,223 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046357, Speed: 690.290361 images/s
2022-08-04 16:55:11,131 [dl_trainer.py:731] WARNING [108][21240/  196][rank:0] loss: 0.141, average forward (0.009661) and backward (0.021863) time: 0.033166, iotime: 0.001407 
2022-08-04 16:55:11,871 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:55:11,871 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:55:13,281 [dl_trainer.py:731] WARNING [108][21280/  196][rank:0] loss: 0.142, average forward (0.009794) and backward (0.021762) time: 0.038043, iotime: 0.006220 
2022-08-04 16:55:14,062 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051172, Speed: 625.340481 images/s
2022-08-04 16:55:15,167 [dl_trainer.py:731] WARNING [108][21320/  196][rank:0] loss: 0.109, average forward (0.009611) and backward (0.021244) time: 0.032522, iotime: 0.001428 
2022-08-04 16:55:15,930 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046689, Speed: 685.380363 images/s
2022-08-04 16:55:17,091 [dl_trainer.py:731] WARNING [108][21360/  196][rank:0] loss: 0.049, average forward (0.011216) and backward (0.021929) time: 0.035008, iotime: 0.001594 
2022-08-04 16:55:17,301 [dl_trainer.py:634] INFO train iter: 21364, num_batches_per_epoch: 196
2022-08-04 16:55:17,302 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 95.870536, lr: 0.100000, avg loss: 0.112637
2022-08-04 16:55:19,975 [dl_trainer.py:822] INFO Epoch 109, lr: 0.100000, val loss: 0.353511, val top-1 acc: 90.425319, top-5 acc: 99.680511
2022-08-04 16:55:20,567 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115915, Speed: 276.064144 images/s
2022-08-04 16:55:21,692 [dl_trainer.py:731] WARNING [109][21400/  196][rank:0] loss: 0.325, average forward (0.011174) and backward (0.022260) time: 0.102240, iotime: 0.001632 
2022-08-04 16:55:22,462 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047375, Speed: 675.458264 images/s
2022-08-04 16:55:23,619 [dl_trainer.py:731] WARNING [109][21440/  196][rank:0] loss: 0.168, average forward (0.010889) and backward (0.023135) time: 0.035862, iotime: 0.001564 
2022-08-04 16:55:24,108 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:55:24,108 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:55:25,644 [dl_trainer.py:731] WARNING [109][21480/  196][rank:0] loss: 0.051, average forward (0.009765) and backward (0.018814) time: 0.034760, iotime: 0.005916 
2022-08-04 16:55:26,203 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049857, Speed: 641.829597 images/s
2022-08-04 16:55:27,555 [dl_trainer.py:731] WARNING [109][21520/  196][rank:0] loss: 0.127, average forward (0.010770) and backward (0.021401) time: 0.034063, iotime: 0.001622 
2022-08-04 16:55:28,094 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047261, Speed: 677.086526 images/s
2022-08-04 16:55:29,517 [dl_trainer.py:731] WARNING [109][21560/  196][rank:0] loss: 0.065, average forward (0.010851) and backward (0.021026) time: 0.033751, iotime: 0.001610 
2022-08-04 16:55:29,539 [dl_trainer.py:634] INFO train iter: 21560, num_batches_per_epoch: 196
2022-08-04 16:55:29,539 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 95.934311, lr: 0.100000, avg loss: 0.117685
2022-08-04 16:55:32,200 [dl_trainer.py:822] INFO Epoch 110, lr: 0.100000, val loss: 0.322049, val top-1 acc: 90.754792, top-5 acc: 99.700479
2022-08-04 16:55:32,743 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116214, Speed: 275.353040 images/s
2022-08-04 16:55:34,179 [dl_trainer.py:731] WARNING [110][21600/  196][rank:0] loss: 0.113, average forward (0.011085) and backward (0.023331) time: 0.103344, iotime: 0.001627 
2022-08-04 16:55:34,731 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049683, Speed: 644.087911 images/s
2022-08-04 16:55:36,149 [dl_trainer.py:731] WARNING [110][21640/  196][rank:0] loss: 0.093, average forward (0.011050) and backward (0.023371) time: 0.036275, iotime: 0.001586 
2022-08-04 16:55:36,408 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:55:36,409 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:55:38,208 [dl_trainer.py:731] WARNING [110][21680/  196][rank:0] loss: 0.109, average forward (0.010342) and backward (0.021014) time: 0.037224, iotime: 0.005608 
2022-08-04 16:55:38,490 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050104, Speed: 638.676530 images/s
2022-08-04 16:55:40,071 [dl_trainer.py:731] WARNING [110][21720/  196][rank:0] loss: 0.062, average forward (0.010305) and backward (0.019889) time: 0.032034, iotime: 0.001591 
2022-08-04 16:55:40,359 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046718, Speed: 684.959236 images/s
2022-08-04 16:55:41,795 [dl_trainer.py:634] INFO train iter: 21756, num_batches_per_epoch: 196
2022-08-04 16:55:41,795 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 95.455995, lr: 0.100000, avg loss: 0.120591
2022-08-04 16:55:44,474 [dl_trainer.py:822] INFO Epoch 111, lr: 0.100000, val loss: 0.344351, val top-1 acc: 90.445288, top-5 acc: 99.690495
2022-08-04 16:55:44,649 [dl_trainer.py:731] WARNING [111][21760/  196][rank:0] loss: 0.169, average forward (0.010490) and backward (0.020814) time: 0.100141, iotime: 0.001545 
2022-08-04 16:55:44,916 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113922, Speed: 280.894886 images/s
2022-08-04 16:55:46,435 [dl_trainer.py:731] WARNING [111][21800/  196][rank:0] loss: 0.045, average forward (0.009206) and backward (0.020020) time: 0.030879, iotime: 0.001411 
2022-08-04 16:55:46,734 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045424, Speed: 704.477066 images/s
2022-08-04 16:55:48,334 [dl_trainer.py:731] WARNING [111][21840/  196][rank:0] loss: 0.199, average forward (0.008440) and backward (0.020766) time: 0.030740, iotime: 0.001316 
2022-08-04 16:55:48,356 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:55:48,356 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:55:50,433 [dl_trainer.py:731] WARNING [111][21880/  196][rank:0] loss: 0.056, average forward (0.013121) and backward (0.022402) time: 0.041721, iotime: 0.005863 
2022-08-04 16:55:50,487 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050028, Speed: 639.639851 images/s
2022-08-04 16:55:52,356 [dl_trainer.py:731] WARNING [111][21920/  196][rank:0] loss: 0.063, average forward (0.010060) and backward (0.021224) time: 0.033172, iotime: 0.001623 
2022-08-04 16:55:52,416 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048199, Speed: 663.917665 images/s
2022-08-04 16:55:53,806 [dl_trainer.py:634] INFO train iter: 21952, num_batches_per_epoch: 196
2022-08-04 16:55:53,807 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 95.695153, lr: 0.100000, avg loss: 0.122335
2022-08-04 16:55:56,535 [dl_trainer.py:822] INFO Epoch 112, lr: 0.100000, val loss: 0.325577, val top-1 acc: 90.365415, top-5 acc: 99.680511
2022-08-04 16:55:56,919 [dl_trainer.py:731] WARNING [112][21960/  196][rank:0] loss: 0.335, average forward (0.009999) and backward (0.018525) time: 0.098990, iotime: 0.001482 
2022-08-04 16:55:56,970 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113860, Speed: 281.047873 images/s
2022-08-04 16:55:58,818 [dl_trainer.py:731] WARNING [112][22000/  196][rank:0] loss: 0.245, average forward (0.009716) and backward (0.021278) time: 0.032718, iotime: 0.001488 
2022-08-04 16:55:58,888 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047924, Speed: 667.719079 images/s
2022-08-04 16:56:00,499 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:56:00,500 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:56:00,884 [dl_trainer.py:731] WARNING [112][22040/  196][rank:0] loss: 0.125, average forward (0.008815) and backward (0.022958) time: 0.037698, iotime: 0.005700 
2022-08-04 16:56:02,575 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049139, Speed: 651.215370 images/s
2022-08-04 16:56:02,765 [dl_trainer.py:731] WARNING [112][22080/  196][rank:0] loss: 0.016, average forward (0.010787) and backward (0.019773) time: 0.032476, iotime: 0.001631 
2022-08-04 16:56:04,529 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048827, Speed: 655.373680 images/s
2022-08-04 16:56:04,706 [dl_trainer.py:731] WARNING [112][22120/  196][rank:0] loss: 0.213, average forward (0.009581) and backward (0.021432) time: 0.032700, iotime: 0.001443 
2022-08-04 16:56:06,053 [dl_trainer.py:634] INFO train iter: 22148, num_batches_per_epoch: 196
2022-08-04 16:56:06,054 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 95.870536, lr: 0.100000, avg loss: 0.114706
2022-08-04 16:56:08,754 [dl_trainer.py:822] INFO Epoch 113, lr: 0.100000, val loss: 0.345630, val top-1 acc: 89.976038, top-5 acc: 99.650559
2022-08-04 16:56:09,121 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114811, Speed: 278.718048 images/s
2022-08-04 16:56:09,296 [dl_trainer.py:731] WARNING [113][22160/  196][rank:0] loss: 0.068, average forward (0.008879) and backward (0.020218) time: 0.098333, iotime: 0.001383 
2022-08-04 16:56:10,938 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045398, Speed: 704.879136 images/s
2022-08-04 16:56:11,105 [dl_trainer.py:731] WARNING [113][22200/  196][rank:0] loss: 0.130, average forward (0.009784) and backward (0.021056) time: 0.032548, iotime: 0.001454 
2022-08-04 16:56:12,550 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:56:12,550 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:56:13,139 [dl_trainer.py:731] WARNING [113][22240/  196][rank:0] loss: 0.182, average forward (0.009635) and backward (0.019461) time: 0.035579, iotime: 0.006239 
2022-08-04 16:56:14,438 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046651, Speed: 685.941608 images/s
2022-08-04 16:56:14,834 [dl_trainer.py:731] WARNING [113][22280/  196][rank:0] loss: 0.202, average forward (0.008845) and backward (0.019071) time: 0.029512, iotime: 0.001367 
2022-08-04 16:56:16,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047328, Speed: 676.129183 images/s
2022-08-04 16:56:16,730 [dl_trainer.py:731] WARNING [113][22320/  196][rank:0] loss: 0.107, average forward (0.009624) and backward (0.019127) time: 0.030473, iotime: 0.001473 
2022-08-04 16:56:17,866 [dl_trainer.py:634] INFO train iter: 22344, num_batches_per_epoch: 196
2022-08-04 16:56:17,866 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 95.886480, lr: 0.100000, avg loss: 0.121881
2022-08-04 16:56:20,510 [dl_trainer.py:822] INFO Epoch 114, lr: 0.100000, val loss: 0.354847, val top-1 acc: 90.115815, top-5 acc: 99.700479
2022-08-04 16:56:20,942 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115245, Speed: 277.669795 images/s
2022-08-04 16:56:21,348 [dl_trainer.py:731] WARNING [114][22360/  196][rank:0] loss: 0.128, average forward (0.009309) and backward (0.018727) time: 0.098045, iotime: 0.001413 
2022-08-04 16:56:22,825 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047063, Speed: 679.941392 images/s
2022-08-04 16:56:23,228 [dl_trainer.py:731] WARNING [114][22400/  196][rank:0] loss: 0.083, average forward (0.010510) and backward (0.020517) time: 0.032828, iotime: 0.001547 
2022-08-04 16:56:24,331 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:56:24,331 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:56:25,173 [dl_trainer.py:731] WARNING [114][22440/  196][rank:0] loss: 0.084, average forward (0.010174) and backward (0.017926) time: 0.033984, iotime: 0.005639 
2022-08-04 16:56:26,424 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047977, Speed: 666.992019 images/s
2022-08-04 16:56:27,074 [dl_trainer.py:731] WARNING [114][22480/  196][rank:0] loss: 0.199, average forward (0.011383) and backward (0.022563) time: 0.035902, iotime: 0.001677 
2022-08-04 16:56:28,295 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046741, Speed: 684.628102 images/s
2022-08-04 16:56:28,907 [dl_trainer.py:731] WARNING [114][22520/  196][rank:0] loss: 0.038, average forward (0.010582) and backward (0.019671) time: 0.032030, iotime: 0.001516 
2022-08-04 16:56:29,882 [dl_trainer.py:634] INFO train iter: 22540, num_batches_per_epoch: 196
2022-08-04 16:56:29,883 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 96.285077, lr: 0.100000, avg loss: 0.112309
2022-08-04 16:56:32,595 [dl_trainer.py:822] INFO Epoch 115, lr: 0.100000, val loss: 0.349651, val top-1 acc: 90.125799, top-5 acc: 99.740415
2022-08-04 16:56:32,874 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114466, Speed: 279.559171 images/s
2022-08-04 16:56:33,489 [dl_trainer.py:731] WARNING [115][22560/  196][rank:0] loss: 0.165, average forward (0.010277) and backward (0.020882) time: 0.100872, iotime: 0.001558 
2022-08-04 16:56:34,737 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046565, Speed: 687.212624 images/s
2022-08-04 16:56:35,333 [dl_trainer.py:731] WARNING [115][22600/  196][rank:0] loss: 0.050, average forward (0.011336) and backward (0.021059) time: 0.034266, iotime: 0.001605 
2022-08-04 16:56:36,304 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:56:36,304 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:56:37,372 [dl_trainer.py:731] WARNING [115][22640/  196][rank:0] loss: 0.104, average forward (0.008911) and backward (0.019374) time: 0.034193, iotime: 0.005673 
2022-08-04 16:56:38,353 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048191, Speed: 664.023080 images/s
2022-08-04 16:56:39,238 [dl_trainer.py:731] WARNING [115][22680/  196][rank:0] loss: 0.036, average forward (0.010290) and backward (0.019786) time: 0.031845, iotime: 0.001522 
2022-08-04 16:56:40,229 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046884, Speed: 682.528847 images/s
2022-08-04 16:56:41,106 [dl_trainer.py:731] WARNING [115][22720/  196][rank:0] loss: 0.245, average forward (0.010975) and backward (0.020365) time: 0.033179, iotime: 0.001582 
2022-08-04 16:56:41,839 [dl_trainer.py:634] INFO train iter: 22736, num_batches_per_epoch: 196
2022-08-04 16:56:41,839 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 96.364796, lr: 0.100000, avg loss: 0.114012
2022-08-04 16:56:44,556 [dl_trainer.py:822] INFO Epoch 116, lr: 0.100000, val loss: 0.365020, val top-1 acc: 89.616613, top-5 acc: 99.690495
2022-08-04 16:56:44,858 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115707, Speed: 276.560813 images/s
2022-08-04 16:56:45,715 [dl_trainer.py:731] WARNING [116][22760/  196][rank:0] loss: 0.030, average forward (0.009671) and backward (0.020292) time: 0.101390, iotime: 0.001488 
2022-08-04 16:56:46,715 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046438, Speed: 689.083628 images/s
2022-08-04 16:56:47,632 [dl_trainer.py:731] WARNING [116][22800/  196][rank:0] loss: 0.176, average forward (0.010087) and backward (0.023216) time: 0.035023, iotime: 0.001472 
2022-08-04 16:56:48,365 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:56:48,365 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:56:49,657 [dl_trainer.py:731] WARNING [116][22840/  196][rank:0] loss: 0.075, average forward (0.009845) and backward (0.019265) time: 0.035277, iotime: 0.005908 
2022-08-04 16:56:50,422 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049411, Speed: 647.634305 images/s
2022-08-04 16:56:51,555 [dl_trainer.py:731] WARNING [116][22880/  196][rank:0] loss: 0.039, average forward (0.009617) and backward (0.021293) time: 0.032710, iotime: 0.001554 
2022-08-04 16:56:52,293 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046751, Speed: 684.481374 images/s
2022-08-04 16:56:53,418 [dl_trainer.py:731] WARNING [116][22920/  196][rank:0] loss: 0.111, average forward (0.010862) and backward (0.021241) time: 0.033963, iotime: 0.001599 
2022-08-04 16:56:54,030 [dl_trainer.py:634] INFO train iter: 22932, num_batches_per_epoch: 196
2022-08-04 16:56:54,031 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 95.679209, lr: 0.100000, avg loss: 0.121928
2022-08-04 16:56:56,765 [dl_trainer.py:822] INFO Epoch 117, lr: 0.100000, val loss: 0.361493, val top-1 acc: 89.866214, top-5 acc: 99.600639
2022-08-04 16:56:56,970 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116906, Speed: 273.723646 images/s
2022-08-04 16:56:58,096 [dl_trainer.py:731] WARNING [117][22960/  196][rank:0] loss: 0.115, average forward (0.011177) and backward (0.022309) time: 0.103859, iotime: 0.001667 
2022-08-04 16:56:58,877 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047677, Speed: 671.182438 images/s
2022-08-04 16:57:00,011 [dl_trainer.py:731] WARNING [117][23000/  196][rank:0] loss: 0.171, average forward (0.010131) and backward (0.020782) time: 0.032732, iotime: 0.001560 
2022-08-04 16:57:00,496 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:57:00,496 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:57:02,088 [dl_trainer.py:731] WARNING [117][23040/  196][rank:0] loss: 0.039, average forward (0.010366) and backward (0.020731) time: 0.037234, iotime: 0.005864 
2022-08-04 16:57:02,626 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049966, Speed: 640.430098 images/s
2022-08-04 16:57:04,033 [dl_trainer.py:731] WARNING [117][23080/  196][rank:0] loss: 0.043, average forward (0.010372) and backward (0.020558) time: 0.032863, iotime: 0.001656 
2022-08-04 16:57:04,562 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048385, Speed: 661.364433 images/s
2022-08-04 16:57:05,906 [dl_trainer.py:731] WARNING [117][23120/  196][rank:0] loss: 0.054, average forward (0.011254) and backward (0.020534) time: 0.033720, iotime: 0.001629 
2022-08-04 16:57:06,314 [dl_trainer.py:634] INFO train iter: 23128, num_batches_per_epoch: 196
2022-08-04 16:57:06,314 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 96.476403, lr: 0.100000, avg loss: 0.108716
2022-08-04 16:57:08,967 [dl_trainer.py:822] INFO Epoch 118, lr: 0.100000, val loss: 0.370531, val top-1 acc: 89.686502, top-5 acc: 99.710463
2022-08-04 16:57:09,127 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114132, Speed: 280.377300 images/s
2022-08-04 16:57:10,505 [dl_trainer.py:731] WARNING [118][23160/  196][rank:0] loss: 0.176, average forward (0.012251) and backward (0.022284) time: 0.103559, iotime: 0.001770 
2022-08-04 16:57:11,051 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048077, Speed: 665.593995 images/s
2022-08-04 16:57:12,417 [dl_trainer.py:731] WARNING [118][23200/  196][rank:0] loss: 0.257, average forward (0.010988) and backward (0.021933) time: 0.034845, iotime: 0.001585 
2022-08-04 16:57:12,687 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:57:12,688 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:57:14,517 [dl_trainer.py:731] WARNING [118][23240/  196][rank:0] loss: 0.135, average forward (0.010562) and backward (0.021451) time: 0.038014, iotime: 0.005748 
2022-08-04 16:57:14,805 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050036, Speed: 639.533543 images/s
2022-08-04 16:57:16,349 [dl_trainer.py:731] WARNING [118][23280/  196][rank:0] loss: 0.127, average forward (0.008133) and backward (0.019500) time: 0.029087, iotime: 0.001241 
2022-08-04 16:57:16,662 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046424, Speed: 689.302246 images/s
2022-08-04 16:57:18,203 [dl_trainer.py:731] WARNING [118][23320/  196][rank:0] loss: 0.040, average forward (0.009047) and backward (0.020299) time: 0.031014, iotime: 0.001432 
2022-08-04 16:57:18,395 [dl_trainer.py:634] INFO train iter: 23324, num_batches_per_epoch: 196
2022-08-04 16:57:18,396 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 95.535714, lr: 0.100000, avg loss: 0.124699
2022-08-04 16:57:21,041 [dl_trainer.py:822] INFO Epoch 119, lr: 0.100000, val loss: 0.346018, val top-1 acc: 90.295527, top-5 acc: 99.690495
2022-08-04 16:57:21,135 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.111812, Speed: 286.195748 images/s
2022-08-04 16:57:22,713 [dl_trainer.py:731] WARNING [119][23360/  196][rank:0] loss: 0.042, average forward (0.009535) and backward (0.020907) time: 0.098361, iotime: 0.001468 
2022-08-04 16:57:22,996 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046520, Speed: 687.871677 images/s
2022-08-04 16:57:24,548 [dl_trainer.py:731] WARNING [119][23400/  196][rank:0] loss: 0.136, average forward (0.009196) and backward (0.019812) time: 0.030718, iotime: 0.001478 
2022-08-04 16:57:24,563 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:57:24,563 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:57:26,597 [dl_trainer.py:731] WARNING [119][23440/  196][rank:0] loss: 0.156, average forward (0.011739) and backward (0.019572) time: 0.037470, iotime: 0.005875 
2022-08-04 16:57:26,663 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048868, Speed: 654.827782 images/s
2022-08-04 16:57:28,434 [dl_trainer.py:731] WARNING [119][23480/  196][rank:0] loss: 0.057, average forward (0.009526) and backward (0.018751) time: 0.029937, iotime: 0.001420 
2022-08-04 16:57:28,497 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045845, Speed: 697.997672 images/s
2022-08-04 16:57:30,334 [dl_trainer.py:731] WARNING [119][23520/  196][rank:0] loss: 0.081, average forward (0.009875) and backward (0.018459) time: 0.030067, iotime: 0.001472 
2022-08-04 16:57:30,346 [dl_trainer.py:634] INFO train iter: 23520, num_batches_per_epoch: 196
2022-08-04 16:57:30,347 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 96.157526, lr: 0.100000, avg loss: 0.114118
2022-08-04 16:57:33,037 [dl_trainer.py:822] INFO Epoch 120, lr: 0.100000, val loss: 0.359681, val top-1 acc: 89.986022, top-5 acc: 99.760383
2022-08-04 16:57:33,158 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116522, Speed: 274.625304 images/s
2022-08-04 16:57:34,932 [dl_trainer.py:731] WARNING [120][23560/  196][rank:0] loss: 0.116, average forward (0.009053) and backward (0.019654) time: 0.099448, iotime: 0.001354 
2022-08-04 16:57:34,997 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045970, Speed: 696.104008 images/s
2022-08-04 16:57:36,591 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:57:36,591 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:57:36,999 [dl_trainer.py:731] WARNING [120][23600/  196][rank:0] loss: 0.072, average forward (0.009320) and backward (0.019879) time: 0.034887, iotime: 0.005448 
2022-08-04 16:57:38,703 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049392, Speed: 647.876228 images/s
2022-08-04 16:57:38,857 [dl_trainer.py:731] WARNING [120][23640/  196][rank:0] loss: 0.157, average forward (0.010385) and backward (0.019849) time: 0.032120, iotime: 0.001626 
2022-08-04 16:57:40,571 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046688, Speed: 685.402413 images/s
2022-08-04 16:57:40,728 [dl_trainer.py:731] WARNING [120][23680/  196][rank:0] loss: 0.019, average forward (0.009087) and backward (0.018950) time: 0.029670, iotime: 0.001387 
2022-08-04 16:57:42,397 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045638, Speed: 701.165026 images/s
2022-08-04 16:57:42,398 [dl_trainer.py:634] INFO train iter: 23716, num_batches_per_epoch: 196
2022-08-04 16:57:42,398 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 96.269133, lr: 0.100000, avg loss: 0.109058
2022-08-04 16:57:45,044 [dl_trainer.py:822] INFO Epoch 121, lr: 0.100000, val loss: 0.342197, val top-1 acc: 90.674920, top-5 acc: 99.680511
2022-08-04 16:57:45,212 [dl_trainer.py:731] WARNING [121][23720/  196][rank:0] loss: 0.143, average forward (0.010710) and backward (0.019711) time: 0.098467, iotime: 0.001537 
2022-08-04 16:57:46,948 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113751, Speed: 281.315176 images/s
2022-08-04 16:57:47,130 [dl_trainer.py:731] WARNING [121][23760/  196][rank:0] loss: 0.119, average forward (0.009240) and backward (0.021525) time: 0.032482, iotime: 0.001476 
2022-08-04 16:57:48,552 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:57:48,552 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:57:49,174 [dl_trainer.py:731] WARNING [121][23800/  196][rank:0] loss: 0.078, average forward (0.009000) and backward (0.021110) time: 0.035988, iotime: 0.005635 
2022-08-04 16:57:50,635 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049146, Speed: 651.114867 images/s
2022-08-04 16:57:51,042 [dl_trainer.py:731] WARNING [121][23840/  196][rank:0] loss: 0.074, average forward (0.010410) and backward (0.019821) time: 0.032087, iotime: 0.001593 
2022-08-04 16:57:52,570 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048361, Speed: 661.684205 images/s
2022-08-04 16:57:52,997 [dl_trainer.py:731] WARNING [121][23880/  196][rank:0] loss: 0.042, average forward (0.010000) and backward (0.023043) time: 0.034898, iotime: 0.001607 
2022-08-04 16:57:54,510 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048480, Speed: 660.065708 images/s
2022-08-04 16:57:54,561 [dl_trainer.py:634] INFO train iter: 23912, num_batches_per_epoch: 196
2022-08-04 16:57:54,562 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 95.902423, lr: 0.100000, avg loss: 0.111041
2022-08-04 16:57:57,227 [dl_trainer.py:822] INFO Epoch 122, lr: 0.100000, val loss: 0.347262, val top-1 acc: 90.435304, top-5 acc: 99.680511
2022-08-04 16:57:57,751 [dl_trainer.py:731] WARNING [122][23920/  196][rank:0] loss: 0.264, average forward (0.010997) and backward (0.022423) time: 0.105667, iotime: 0.001591 
2022-08-04 16:57:59,232 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.118031, Speed: 271.114843 images/s
2022-08-04 16:57:59,663 [dl_trainer.py:731] WARNING [122][23960/  196][rank:0] loss: 0.233, average forward (0.010908) and backward (0.019424) time: 0.032241, iotime: 0.001644 
2022-08-04 16:58:00,924 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:58:00,924 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:58:01,766 [dl_trainer.py:731] WARNING [122][24000/  196][rank:0] loss: 0.043, average forward (0.009679) and backward (0.020598) time: 0.036332, iotime: 0.005822 
2022-08-04 16:58:03,040 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050754, Speed: 630.495135 images/s
2022-08-04 16:58:03,683 [dl_trainer.py:731] WARNING [122][24040/  196][rank:0] loss: 0.038, average forward (0.009521) and backward (0.020242) time: 0.031475, iotime: 0.001474 
2022-08-04 16:58:04,883 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046046, Speed: 694.962154 images/s
2022-08-04 16:58:05,505 [dl_trainer.py:731] WARNING [122][24080/  196][rank:0] loss: 0.031, average forward (0.011503) and backward (0.020548) time: 0.033943, iotime: 0.001639 
2022-08-04 16:58:06,745 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046551, Speed: 687.412540 images/s
2022-08-04 16:58:06,835 [dl_trainer.py:634] INFO train iter: 24108, num_batches_per_epoch: 196
2022-08-04 16:58:06,835 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 97.193878, lr: 0.010000, avg loss: 0.084426
2022-08-04 16:58:09,445 [dl_trainer.py:822] INFO Epoch 123, lr: 0.010000, val loss: 0.294263, val top-1 acc: 91.513578, top-5 acc: 99.680511
2022-08-04 16:58:10,020 [dl_trainer.py:731] WARNING [123][24120/  196][rank:0] loss: 0.031, average forward (0.011317) and backward (0.021570) time: 0.100081, iotime: 0.001613 
2022-08-04 16:58:11,263 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112928, Speed: 283.366101 images/s
2022-08-04 16:58:11,891 [dl_trainer.py:731] WARNING [123][24160/  196][rank:0] loss: 0.090, average forward (0.010783) and backward (0.020343) time: 0.032989, iotime: 0.001599 
2022-08-04 16:58:12,857 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:58:12,857 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:58:13,964 [dl_trainer.py:731] WARNING [123][24200/  196][rank:0] loss: 0.015, average forward (0.010827) and backward (0.021411) time: 0.038357, iotime: 0.005838 
2022-08-04 16:58:14,968 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049392, Speed: 647.879647 images/s
2022-08-04 16:58:15,832 [dl_trainer.py:731] WARNING [123][24240/  196][rank:0] loss: 0.041, average forward (0.011086) and backward (0.017896) time: 0.030870, iotime: 0.001621 
2022-08-04 16:58:16,856 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047172, Speed: 678.373438 images/s
2022-08-04 16:58:17,795 [dl_trainer.py:731] WARNING [123][24280/  196][rank:0] loss: 0.020, average forward (0.009589) and backward (0.023359) time: 0.034697, iotime: 0.001504 
2022-08-04 16:58:18,840 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049589, Speed: 645.305615 images/s
2022-08-04 16:58:18,982 [dl_trainer.py:634] INFO train iter: 24304, num_batches_per_epoch: 196
2022-08-04 16:58:18,982 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 97.831633, lr: 0.010000, avg loss: 0.064458
2022-08-04 16:58:21,664 [dl_trainer.py:822] INFO Epoch 124, lr: 0.010000, val loss: 0.289110, val top-1 acc: 91.873003, top-5 acc: 99.720447
2022-08-04 16:58:22,561 [dl_trainer.py:731] WARNING [124][24320/  196][rank:0] loss: 0.060, average forward (0.011224) and backward (0.022340) time: 0.106318, iotime: 0.001611 
2022-08-04 16:58:23,567 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.118166, Speed: 270.805164 images/s
2022-08-04 16:58:24,496 [dl_trainer.py:731] WARNING [124][24360/  196][rank:0] loss: 0.038, average forward (0.009813) and backward (0.022792) time: 0.034311, iotime: 0.001464 
2022-08-04 16:58:25,259 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:58:25,259 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:58:26,616 [dl_trainer.py:731] WARNING [124][24400/  196][rank:0] loss: 0.032, average forward (0.009567) and backward (0.020846) time: 0.036158, iotime: 0.005507 
2022-08-04 16:58:27,411 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051241, Speed: 624.503471 images/s
2022-08-04 16:58:28,589 [dl_trainer.py:731] WARNING [124][24440/  196][rank:0] loss: 0.055, average forward (0.010208) and backward (0.022069) time: 0.034147, iotime: 0.001586 
2022-08-04 16:58:29,345 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048325, Speed: 662.179838 images/s
2022-08-04 16:58:30,498 [dl_trainer.py:731] WARNING [124][24480/  196][rank:0] loss: 0.089, average forward (0.010813) and backward (0.020694) time: 0.033402, iotime: 0.001621 
2022-08-04 16:58:31,283 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048432, Speed: 660.723377 images/s
2022-08-04 16:58:31,478 [dl_trainer.py:634] INFO train iter: 24500, num_batches_per_epoch: 196
2022-08-04 16:58:31,479 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 98.118622, lr: 0.010000, avg loss: 0.062073
2022-08-04 16:58:34,154 [dl_trainer.py:822] INFO Epoch 125, lr: 0.010000, val loss: 0.287089, val top-1 acc: 92.022764, top-5 acc: 99.750399
2022-08-04 16:58:35,127 [dl_trainer.py:731] WARNING [125][24520/  196][rank:0] loss: 0.037, average forward (0.010621) and backward (0.021328) time: 0.100813, iotime: 0.001614 
2022-08-04 16:58:35,924 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115962, Speed: 275.953222 images/s
2022-08-04 16:58:37,102 [dl_trainer.py:731] WARNING [125][24560/  196][rank:0] loss: 0.078, average forward (0.010221) and backward (0.022971) time: 0.034965, iotime: 0.001510 
2022-08-04 16:58:37,598 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:58:37,603 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:58:39,140 [dl_trainer.py:731] WARNING [125][24600/  196][rank:0] loss: 0.100, average forward (0.009873) and backward (0.020532) time: 0.036534, iotime: 0.005881 
2022-08-04 16:58:39,644 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049511, Speed: 646.316700 images/s
2022-08-04 16:58:41,042 [dl_trainer.py:731] WARNING [125][24640/  196][rank:0] loss: 0.023, average forward (0.010030) and backward (0.020606) time: 0.032394, iotime: 0.001509 
2022-08-04 16:58:41,533 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047210, Speed: 677.826233 images/s
2022-08-04 16:58:42,892 [dl_trainer.py:731] WARNING [125][24680/  196][rank:0] loss: 0.135, average forward (0.010841) and backward (0.020242) time: 0.032937, iotime: 0.001589 
2022-08-04 16:58:43,449 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047892, Speed: 668.170572 images/s
2022-08-04 16:58:43,695 [dl_trainer.py:634] INFO train iter: 24696, num_batches_per_epoch: 196
2022-08-04 16:58:43,696 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 98.086735, lr: 0.010000, avg loss: 0.060956
2022-08-04 16:58:46,403 [dl_trainer.py:822] INFO Epoch 126, lr: 0.010000, val loss: 0.286655, val top-1 acc: 91.992812, top-5 acc: 99.720447
2022-08-04 16:58:47,579 [dl_trainer.py:731] WARNING [126][24720/  196][rank:0] loss: 0.056, average forward (0.009350) and backward (0.020561) time: 0.100844, iotime: 0.001391 
2022-08-04 16:58:48,091 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.116047, Speed: 275.750071 images/s
2022-08-04 16:58:49,504 [dl_trainer.py:731] WARNING [126][24760/  196][rank:0] loss: 0.103, average forward (0.009471) and backward (0.020414) time: 0.031573, iotime: 0.001440 
2022-08-04 16:58:49,772 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:58:49,772 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:58:51,545 [dl_trainer.py:731] WARNING [126][24800/  196][rank:0] loss: 0.023, average forward (0.008669) and backward (0.019996) time: 0.034645, iotime: 0.005744 
2022-08-04 16:58:51,823 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049741, Speed: 643.337669 images/s
2022-08-04 16:58:53,358 [dl_trainer.py:731] WARNING [126][24840/  196][rank:0] loss: 0.042, average forward (0.010219) and backward (0.021159) time: 0.033056, iotime: 0.001429 
2022-08-04 16:58:53,670 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046154, Speed: 693.329256 images/s
2022-08-04 16:58:55,258 [dl_trainer.py:731] WARNING [126][24880/  196][rank:0] loss: 0.014, average forward (0.010064) and backward (0.020632) time: 0.032404, iotime: 0.001447 
2022-08-04 16:58:55,550 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046995, Speed: 680.919155 images/s
2022-08-04 16:58:55,832 [dl_trainer.py:634] INFO train iter: 24892, num_batches_per_epoch: 196
2022-08-04 16:58:55,832 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 98.389668, lr: 0.010000, avg loss: 0.054227
2022-08-04 16:58:58,523 [dl_trainer.py:822] INFO Epoch 127, lr: 0.010000, val loss: 0.287507, val top-1 acc: 92.092652, top-5 acc: 99.710463
2022-08-04 16:58:59,870 [dl_trainer.py:731] WARNING [127][24920/  196][rank:0] loss: 0.023, average forward (0.009891) and backward (0.021348) time: 0.100318, iotime: 0.001486 
2022-08-04 16:59:00,165 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.115357, Speed: 277.399066 images/s
2022-08-04 16:59:01,723 [dl_trainer.py:731] WARNING [127][24960/  196][rank:0] loss: 0.034, average forward (0.010374) and backward (0.019433) time: 0.031668, iotime: 0.001618 
2022-08-04 16:59:01,740 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:59:01,740 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:59:03,725 [dl_trainer.py:731] WARNING [127][25000/  196][rank:0] loss: 0.010, average forward (0.009838) and backward (0.018932) time: 0.034727, iotime: 0.005685 
2022-08-04 16:59:03,791 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048343, Speed: 661.938967 images/s
2022-08-04 16:59:05,568 [dl_trainer.py:731] WARNING [127][25040/  196][rank:0] loss: 0.042, average forward (0.010004) and backward (0.017610) time: 0.029306, iotime: 0.001437 
2022-08-04 16:59:05,635 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046076, Speed: 694.502399 images/s
2022-08-04 16:59:07,431 [dl_trainer.py:731] WARNING [127][25080/  196][rank:0] loss: 0.094, average forward (0.008466) and backward (0.021760) time: 0.031776, iotime: 0.001338 
2022-08-04 16:59:07,491 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046382, Speed: 689.919741 images/s
2022-08-04 16:59:07,829 [dl_trainer.py:634] INFO train iter: 25088, num_batches_per_epoch: 196
2022-08-04 16:59:07,830 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 98.134566, lr: 0.010000, avg loss: 0.055044
2022-08-04 16:59:10,568 [dl_trainer.py:822] INFO Epoch 128, lr: 0.010000, val loss: 0.286890, val top-1 acc: 92.082668, top-5 acc: 99.730431
2022-08-04 16:59:12,119 [dl_trainer.py:731] WARNING [128][25120/  196][rank:0] loss: 0.023, average forward (0.009085) and backward (0.021013) time: 0.101258, iotime: 0.001409 
2022-08-04 16:59:12,178 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.117166, Speed: 273.117079 images/s
2022-08-04 16:59:13,748 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:59:13,748 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:59:14,147 [dl_trainer.py:731] WARNING [128][25160/  196][rank:0] loss: 0.047, average forward (0.009408) and backward (0.020576) time: 0.036028, iotime: 0.005804 
2022-08-04 16:59:15,859 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049070, Speed: 652.136129 images/s
2022-08-04 16:59:16,037 [dl_trainer.py:731] WARNING [128][25200/  196][rank:0] loss: 0.070, average forward (0.009712) and backward (0.021473) time: 0.032932, iotime: 0.001496 
2022-08-04 16:59:17,787 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048171, Speed: 664.295554 images/s
2022-08-04 16:59:17,973 [dl_trainer.py:731] WARNING [128][25240/  196][rank:0] loss: 0.021, average forward (0.010525) and backward (0.020871) time: 0.033187, iotime: 0.001520 
2022-08-04 16:59:19,727 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048487, Speed: 659.968095 images/s
2022-08-04 16:59:19,904 [dl_trainer.py:731] WARNING [128][25280/  196][rank:0] loss: 0.018, average forward (0.009284) and backward (0.022098) time: 0.033005, iotime: 0.001382 
2022-08-04 16:59:20,098 [dl_trainer.py:634] INFO train iter: 25284, num_batches_per_epoch: 196
2022-08-04 16:59:20,098 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 98.357781, lr: 0.010000, avg loss: 0.048130
2022-08-04 16:59:22,769 [dl_trainer.py:822] INFO Epoch 129, lr: 0.010000, val loss: 0.285843, val top-1 acc: 92.162540, top-5 acc: 99.700479
2022-08-04 16:59:24,290 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114088, Speed: 280.484905 images/s
2022-08-04 16:59:24,448 [dl_trainer.py:731] WARNING [129][25320/  196][rank:0] loss: 0.011, average forward (0.011267) and backward (0.020643) time: 0.100741, iotime: 0.001608 
2022-08-04 16:59:25,925 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:59:25,926 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:59:26,553 [dl_trainer.py:731] WARNING [129][25360/  196][rank:0] loss: 0.096, average forward (0.010984) and backward (0.021530) time: 0.039017, iotime: 0.006238 
2022-08-04 16:59:28,017 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049670, Speed: 644.246373 images/s
2022-08-04 16:59:28,419 [dl_trainer.py:731] WARNING [129][25400/  196][rank:0] loss: 0.078, average forward (0.009601) and backward (0.018804) time: 0.030032, iotime: 0.001395 
2022-08-04 16:59:29,871 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046341, Speed: 690.534701 images/s
2022-08-04 16:59:30,308 [dl_trainer.py:731] WARNING [129][25440/  196][rank:0] loss: 0.081, average forward (0.010648) and backward (0.020150) time: 0.032662, iotime: 0.001586 
2022-08-04 16:59:31,835 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049075, Speed: 652.057130 images/s
2022-08-04 16:59:32,268 [dl_trainer.py:731] WARNING [129][25480/  196][rank:0] loss: 0.079, average forward (0.009613) and backward (0.023069) time: 0.034366, iotime: 0.001432 
2022-08-04 16:59:32,279 [dl_trainer.py:634] INFO train iter: 25480, num_batches_per_epoch: 196
2022-08-04 16:59:32,279 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 98.501276, lr: 0.010000, avg loss: 0.050058
2022-08-04 16:59:34,923 [dl_trainer.py:822] INFO Epoch 130, lr: 0.010000, val loss: 0.289151, val top-1 acc: 92.172524, top-5 acc: 99.730431
2022-08-04 16:59:36,781 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123633, Speed: 258.830789 images/s
2022-08-04 16:59:37,220 [dl_trainer.py:731] WARNING [130][25520/  196][rank:0] loss: 0.010, average forward (0.011160) and backward (0.020633) time: 0.108819, iotime: 0.001627 
2022-08-04 16:59:38,467 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:59:38,467 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:59:39,296 [dl_trainer.py:731] WARNING [130][25560/  196][rank:0] loss: 0.025, average forward (0.011202) and backward (0.021692) time: 0.038866, iotime: 0.005710 
2022-08-04 16:59:40,509 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049686, Speed: 644.038590 images/s
2022-08-04 16:59:41,091 [dl_trainer.py:731] WARNING [130][25600/  196][rank:0] loss: 0.022, average forward (0.009506) and backward (0.019145) time: 0.030322, iotime: 0.001426 
2022-08-04 16:59:42,262 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.043797, Speed: 730.640101 images/s
2022-08-04 16:59:42,916 [dl_trainer.py:731] WARNING [130][25640/  196][rank:0] loss: 0.020, average forward (0.009578) and backward (0.019374) time: 0.030605, iotime: 0.001410 
2022-08-04 16:59:44,145 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047085, Speed: 679.619995 images/s
2022-08-04 16:59:44,603 [dl_trainer.py:634] INFO train iter: 25676, num_batches_per_epoch: 196
2022-08-04 16:59:44,603 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 98.357781, lr: 0.010000, avg loss: 0.051231
2022-08-04 16:59:47,501 [dl_trainer.py:822] INFO Epoch 131, lr: 0.010000, val loss: 0.288825, val top-1 acc: 92.132588, top-5 acc: 99.700479
2022-08-04 16:59:47,703 [dl_trainer.py:731] WARNING [131][25680/  196][rank:0] loss: 0.025, average forward (0.010913) and backward (0.019213) time: 0.104614, iotime: 0.001591 
2022-08-04 16:59:48,987 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121020, Speed: 264.419011 images/s
2022-08-04 16:59:49,636 [dl_trainer.py:731] WARNING [131][25720/  196][rank:0] loss: 0.039, average forward (0.011960) and backward (0.022053) time: 0.036105, iotime: 0.001804 
2022-08-04 16:59:50,609 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 16:59:50,609 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 16:59:51,717 [dl_trainer.py:731] WARNING [131][25760/  196][rank:0] loss: 0.018, average forward (0.010687) and backward (0.021346) time: 0.038036, iotime: 0.005734 
2022-08-04 16:59:52,732 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049921, Speed: 641.009851 images/s
2022-08-04 16:59:53,627 [dl_trainer.py:731] WARNING [131][25800/  196][rank:0] loss: 0.053, average forward (0.012532) and backward (0.021809) time: 0.036376, iotime: 0.001750 
2022-08-04 16:59:54,643 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047748, Speed: 670.182051 images/s
2022-08-04 16:59:55,489 [dl_trainer.py:731] WARNING [131][25840/  196][rank:0] loss: 0.011, average forward (0.009509) and backward (0.019086) time: 0.030399, iotime: 0.001530 
2022-08-04 16:59:56,509 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046643, Speed: 686.059575 images/s
2022-08-04 16:59:57,044 [dl_trainer.py:634] INFO train iter: 25872, num_batches_per_epoch: 196
2022-08-04 16:59:57,044 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 98.565051, lr: 0.010000, avg loss: 0.046270
2022-08-04 16:59:59,769 [dl_trainer.py:822] INFO Epoch 132, lr: 0.010000, val loss: 0.291198, val top-1 acc: 92.232428, top-5 acc: 99.690495
2022-08-04 17:00:00,270 [dl_trainer.py:731] WARNING [132][25880/  196][rank:0] loss: 0.231, average forward (0.010188) and backward (0.021966) time: 0.105783, iotime: 0.001586 
2022-08-04 17:00:01,262 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.118815, Speed: 269.327063 images/s
2022-08-04 17:00:02,163 [dl_trainer.py:731] WARNING [132][25920/  196][rank:0] loss: 0.081, average forward (0.010516) and backward (0.021853) time: 0.034231, iotime: 0.001603 
2022-08-04 17:00:02,918 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:00:02,918 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:00:04,233 [dl_trainer.py:731] WARNING [132][25960/  196][rank:0] loss: 0.022, average forward (0.010624) and backward (0.021010) time: 0.038052, iotime: 0.006144 
2022-08-04 17:00:05,068 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050729, Speed: 630.797580 images/s
2022-08-04 17:00:06,185 [dl_trainer.py:731] WARNING [132][26000/  196][rank:0] loss: 0.008, average forward (0.009653) and backward (0.021846) time: 0.033198, iotime: 0.001441 
2022-08-04 17:00:06,942 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046838, Speed: 683.209546 images/s
2022-08-04 17:00:08,051 [dl_trainer.py:731] WARNING [132][26040/  196][rank:0] loss: 0.128, average forward (0.009120) and backward (0.020254) time: 0.031054, iotime: 0.001443 
2022-08-04 17:00:08,825 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047067, Speed: 679.881548 images/s
2022-08-04 17:00:09,368 [dl_trainer.py:634] INFO train iter: 26068, num_batches_per_epoch: 196
2022-08-04 17:00:09,368 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 98.533163, lr: 0.010000, avg loss: 0.047321
2022-08-04 17:00:12,045 [dl_trainer.py:822] INFO Epoch 133, lr: 0.010000, val loss: 0.289361, val top-1 acc: 92.132588, top-5 acc: 99.710463
2022-08-04 17:00:12,652 [dl_trainer.py:731] WARNING [133][26080/  196][rank:0] loss: 0.099, average forward (0.011242) and backward (0.021263) time: 0.101444, iotime: 0.001585 
2022-08-04 17:00:13,406 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114511, Speed: 279.449133 images/s
2022-08-04 17:00:14,547 [dl_trainer.py:731] WARNING [133][26120/  196][rank:0] loss: 0.017, average forward (0.009745) and backward (0.020550) time: 0.032033, iotime: 0.001487 
2022-08-04 17:00:15,055 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:00:15,055 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:00:16,588 [dl_trainer.py:731] WARNING [133][26160/  196][rank:0] loss: 0.024, average forward (0.009295) and backward (0.021068) time: 0.036249, iotime: 0.005654 
2022-08-04 17:00:17,089 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049085, Speed: 651.928969 images/s
2022-08-04 17:00:18,356 [dl_trainer.py:731] WARNING [133][26200/  196][rank:0] loss: 0.027, average forward (0.008521) and backward (0.020961) time: 0.031040, iotime: 0.001346 
2022-08-04 17:00:18,871 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044536, Speed: 718.520418 images/s
2022-08-04 17:00:20,154 [dl_trainer.py:731] WARNING [133][26240/  196][rank:0] loss: 0.002, average forward (0.008750) and backward (0.019056) time: 0.029386, iotime: 0.001347 
2022-08-04 17:00:20,636 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044115, Speed: 725.384975 images/s
2022-08-04 17:00:21,228 [dl_trainer.py:634] INFO train iter: 26264, num_batches_per_epoch: 196
2022-08-04 17:00:21,228 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 98.596939, lr: 0.010000, avg loss: 0.046319
2022-08-04 17:00:23,930 [dl_trainer.py:822] INFO Epoch 134, lr: 0.010000, val loss: 0.289529, val top-1 acc: 92.142572, top-5 acc: 99.700479
2022-08-04 17:00:24,917 [dl_trainer.py:731] WARNING [134][26280/  196][rank:0] loss: 0.060, average forward (0.009564) and backward (0.020701) time: 0.105726, iotime: 0.001438 
2022-08-04 17:00:25,431 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.119863, Speed: 266.972195 images/s
2022-08-04 17:00:26,821 [dl_trainer.py:731] WARNING [134][26320/  196][rank:0] loss: 0.011, average forward (0.011037) and backward (0.021101) time: 0.034027, iotime: 0.001620 
2022-08-04 17:00:27,078 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:00:27,078 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:00:28,843 [dl_trainer.py:731] WARNING [134][26360/  196][rank:0] loss: 0.011, average forward (0.010886) and backward (0.021530) time: 0.038285, iotime: 0.005621 
2022-08-04 17:00:29,154 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049613, Speed: 644.989361 images/s
2022-08-04 17:00:30,791 [dl_trainer.py:731] WARNING [134][26400/  196][rank:0] loss: 0.054, average forward (0.010024) and backward (0.021358) time: 0.033104, iotime: 0.001466 
2022-08-04 17:00:31,074 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048009, Speed: 666.537356 images/s
2022-08-04 17:00:32,663 [dl_trainer.py:731] WARNING [134][26440/  196][rank:0] loss: 0.069, average forward (0.010017) and backward (0.020593) time: 0.032380, iotime: 0.001513 
2022-08-04 17:00:32,944 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046714, Speed: 685.012635 images/s
2022-08-04 17:00:33,574 [dl_trainer.py:634] INFO train iter: 26460, num_batches_per_epoch: 196
2022-08-04 17:00:33,574 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 98.437500, lr: 0.010000, avg loss: 0.047746
2022-08-04 17:00:36,248 [dl_trainer.py:822] INFO Epoch 135, lr: 0.010000, val loss: 0.296462, val top-1 acc: 92.122604, top-5 acc: 99.700479
2022-08-04 17:00:37,203 [dl_trainer.py:731] WARNING [135][26480/  196][rank:0] loss: 0.104, average forward (0.009823) and backward (0.017960) time: 0.096506, iotime: 0.001508 
2022-08-04 17:00:37,489 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.113623, Speed: 281.634334 images/s
2022-08-04 17:00:39,064 [dl_trainer.py:731] WARNING [135][26520/  196][rank:0] loss: 0.049, average forward (0.008997) and backward (0.018672) time: 0.029286, iotime: 0.001379 
2022-08-04 17:00:39,077 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:00:39,077 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:00:41,069 [dl_trainer.py:731] WARNING [135][26560/  196][rank:0] loss: 0.016, average forward (0.009184) and backward (0.019402) time: 0.034355, iotime: 0.005534 
2022-08-04 17:00:41,114 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048319, Speed: 662.271249 images/s
2022-08-04 17:00:42,893 [dl_trainer.py:731] WARNING [135][26600/  196][rank:0] loss: 0.011, average forward (0.010417) and backward (0.018507) time: 0.030663, iotime: 0.001486 
2022-08-04 17:00:42,958 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046094, Speed: 694.236839 images/s
2022-08-04 17:00:44,794 [dl_trainer.py:731] WARNING [135][26640/  196][rank:0] loss: 0.023, average forward (0.010162) and backward (0.020579) time: 0.032554, iotime: 0.001550 
2022-08-04 17:00:44,842 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047079, Speed: 679.708362 images/s
2022-08-04 17:00:45,552 [dl_trainer.py:634] INFO train iter: 26656, num_batches_per_epoch: 196
2022-08-04 17:00:45,552 [dl_trainer.py:635] INFO Epoch 136, avg train acc: 98.724490, lr: 0.010000, avg loss: 0.041541
2022-08-04 17:00:48,255 [dl_trainer.py:822] INFO Epoch 136, lr: 0.010000, val loss: 0.293885, val top-1 acc: 92.312300, top-5 acc: 99.700479
2022-08-04 17:00:49,483 [dl_trainer.py:731] WARNING [136][26680/  196][rank:0] loss: 0.034, average forward (0.009753) and backward (0.018852) time: 0.101995, iotime: 0.001418 
2022-08-04 17:00:49,553 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.117772, Speed: 271.711371 images/s
2022-08-04 17:00:51,161 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:00:51,161 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:00:51,568 [dl_trainer.py:731] WARNING [136][26720/  196][rank:0] loss: 0.010, average forward (0.011639) and backward (0.020509) time: 0.038218, iotime: 0.005792 
2022-08-04 17:00:53,304 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050006, Speed: 639.924041 images/s
2022-08-04 17:00:53,478 [dl_trainer.py:731] WARNING [136][26760/  196][rank:0] loss: 0.016, average forward (0.010067) and backward (0.021341) time: 0.033212, iotime: 0.001545 
2022-08-04 17:00:55,248 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048570, Speed: 658.845964 images/s
2022-08-04 17:00:55,413 [dl_trainer.py:731] WARNING [136][26800/  196][rank:0] loss: 0.030, average forward (0.008282) and backward (0.019647) time: 0.029460, iotime: 0.001302 
2022-08-04 17:00:57,078 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045742, Speed: 699.578721 images/s
2022-08-04 17:00:57,240 [dl_trainer.py:731] WARNING [136][26840/  196][rank:0] loss: 0.020, average forward (0.009635) and backward (0.017489) time: 0.028742, iotime: 0.001378 
2022-08-04 17:00:57,783 [dl_trainer.py:634] INFO train iter: 26852, num_batches_per_epoch: 196
2022-08-04 17:00:57,783 [dl_trainer.py:635] INFO Epoch 137, avg train acc: 98.309949, lr: 0.010000, avg loss: 0.047809
2022-08-04 17:01:00,478 [dl_trainer.py:822] INFO Epoch 137, lr: 0.010000, val loss: 0.295724, val top-1 acc: 92.132588, top-5 acc: 99.740415
2022-08-04 17:01:01,563 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.112129, Speed: 285.385029 images/s
2022-08-04 17:01:01,732 [dl_trainer.py:731] WARNING [137][26880/  196][rank:0] loss: 0.057, average forward (0.008979) and backward (0.020355) time: 0.098375, iotime: 0.001380 
2022-08-04 17:01:03,109 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:01:03,109 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:01:03,729 [dl_trainer.py:731] WARNING [137][26920/  196][rank:0] loss: 0.041, average forward (0.010078) and backward (0.017870) time: 0.033706, iotime: 0.005509 
2022-08-04 17:01:05,148 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047772, Speed: 669.846878 images/s
2022-08-04 17:01:05,545 [dl_trainer.py:731] WARNING [137][26960/  196][rank:0] loss: 0.154, average forward (0.008974) and backward (0.020331) time: 0.030964, iotime: 0.001418 
2022-08-04 17:01:07,012 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046592, Speed: 686.809275 images/s
2022-08-04 17:01:07,413 [dl_trainer.py:731] WARNING [137][27000/  196][rank:0] loss: 0.146, average forward (0.009331) and backward (0.020713) time: 0.031698, iotime: 0.001420 
2022-08-04 17:01:08,837 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.045622, Speed: 701.413554 images/s
2022-08-04 17:01:09,227 [dl_trainer.py:731] WARNING [137][27040/  196][rank:0] loss: 0.089, average forward (0.009176) and backward (0.018475) time: 0.029292, iotime: 0.001409 
2022-08-04 17:01:09,615 [dl_trainer.py:634] INFO train iter: 27048, num_batches_per_epoch: 196
2022-08-04 17:01:09,615 [dl_trainer.py:635] INFO Epoch 138, avg train acc: 98.517219, lr: 0.010000, avg loss: 0.046563
2022-08-04 17:01:12,344 [dl_trainer.py:822] INFO Epoch 138, lr: 0.010000, val loss: 0.291432, val top-1 acc: 92.342252, top-5 acc: 99.760383
2022-08-04 17:01:13,670 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.120799, Speed: 264.902138 images/s
2022-08-04 17:01:14,093 [dl_trainer.py:731] WARNING [138][27080/  196][rank:0] loss: 0.036, average forward (0.009172) and backward (0.018627) time: 0.104245, iotime: 0.001412 
2022-08-04 17:01:15,301 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:01:15,301 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:01:16,115 [dl_trainer.py:731] WARNING [138][27120/  196][rank:0] loss: 0.025, average forward (0.010367) and backward (0.019465) time: 0.036019, iotime: 0.005929 
2022-08-04 17:01:17,345 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048968, Speed: 653.490829 images/s
2022-08-04 17:01:17,967 [dl_trainer.py:731] WARNING [138][27160/  196][rank:0] loss: 0.022, average forward (0.009720) and backward (0.018714) time: 0.030106, iotime: 0.001434 
2022-08-04 17:01:19,138 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.044809, Speed: 714.138802 images/s
2022-08-04 17:01:19,754 [dl_trainer.py:731] WARNING [138][27200/  196][rank:0] loss: 0.031, average forward (0.010525) and backward (0.019530) time: 0.031825, iotime: 0.001524 
2022-08-04 17:01:21,000 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046514, Speed: 687.968286 images/s
2022-08-04 17:01:21,656 [dl_trainer.py:731] WARNING [138][27240/  196][rank:0] loss: 0.107, average forward (0.010184) and backward (0.021500) time: 0.033422, iotime: 0.001492 
2022-08-04 17:01:21,851 [dl_trainer.py:634] INFO train iter: 27244, num_batches_per_epoch: 196
2022-08-04 17:01:21,852 [dl_trainer.py:635] INFO Epoch 139, avg train acc: 98.947704, lr: 0.010000, avg loss: 0.036729
2022-08-04 17:01:24,524 [dl_trainer.py:822] INFO Epoch 139, lr: 0.010000, val loss: 0.289156, val top-1 acc: 92.272364, top-5 acc: 99.720447
2022-08-04 17:01:25,566 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.114154, Speed: 280.322723 images/s
2022-08-04 17:01:26,200 [dl_trainer.py:731] WARNING [139][27280/  196][rank:0] loss: 0.045, average forward (0.011089) and backward (0.019752) time: 0.099605, iotime: 0.001611 
2022-08-04 17:01:27,171 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:01:27,171 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:01:28,269 [dl_trainer.py:731] WARNING [139][27320/  196][rank:0] loss: 0.011, average forward (0.009467) and backward (0.020909) time: 0.036269, iotime: 0.005654 
2022-08-04 17:01:29,304 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049814, Speed: 642.395325 images/s
2022-08-04 17:01:30,185 [dl_trainer.py:731] WARNING [139][27360/  196][rank:0] loss: 0.004, average forward (0.009825) and backward (0.021039) time: 0.032569, iotime: 0.001468 
2022-08-04 17:01:31,218 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047852, Speed: 668.721700 images/s
2022-08-04 17:01:32,103 [dl_trainer.py:731] WARNING [139][27400/  196][rank:0] loss: 0.051, average forward (0.008872) and backward (0.020983) time: 0.031459, iotime: 0.001360 
2022-08-04 17:01:33,102 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047090, Speed: 679.550402 images/s
2022-08-04 17:01:33,987 [dl_trainer.py:731] WARNING [139][27440/  196][rank:0] loss: 0.011, average forward (0.009519) and backward (0.021559) time: 0.032744, iotime: 0.001421 
2022-08-04 17:01:34,004 [dl_trainer.py:634] INFO train iter: 27440, num_batches_per_epoch: 196
2022-08-04 17:01:34,004 [dl_trainer.py:635] INFO Epoch 140, avg train acc: 98.772321, lr: 0.010000, avg loss: 0.039294
2022-08-04 17:01:36,634 [dl_trainer.py:822] INFO Epoch 140, lr: 0.010000, val loss: 0.300733, val top-1 acc: 92.082668, top-5 acc: 99.690495
2022-08-04 17:01:37,979 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121897, Speed: 262.516445 images/s
2022-08-04 17:01:38,898 [dl_trainer.py:731] WARNING [140][27480/  196][rank:0] loss: 0.028, average forward (0.010609) and backward (0.022896) time: 0.108944, iotime: 0.001592 
2022-08-04 17:01:39,633 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:01:39,633 [distributed_optimizer.py:143] INFO The number of selected gradients: []

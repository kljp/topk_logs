2022-08-04 18:14:42,759 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=128, compressor='topk', data_dir='./data', dataset='cifar10', density=1.0, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 18:15:16,573 [dl_trainer.py:254] INFO num_batches_per_epoch: 49
2022-08-04 18:15:16,874 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 18:15:16,885 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 18:15:16,885 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 18:15:16,885 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 18:15:17,665 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 18:15:21,754 [dl_trainer.py:731] WARNING [  0][   40/   49][rank:0] loss: 1.650, average forward (0.059820) and backward (0.026190) time: 0.089157, iotime: 0.002093 
2022-08-04 18:15:21,822 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.101379, Speed: 1262.593256 images/s
2022-08-04 18:15:22,211 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 1078
2022-08-04 18:15:22,212 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:22,450 [dl_trainer.py:634] INFO train iter: 49, num_batches_per_epoch: 49
2022-08-04 18:15:22,450 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 27.248087, lr: 0.020327, avg loss: 2.081544
2022-08-04 18:15:24,003 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020327, val loss: 1.781284, val top-1 acc: 34.533228, top-5 acc: 87.410997
2022-08-04 18:15:25,441 [dl_trainer.py:731] WARNING [  1][   80/   49][rank:0] loss: 1.749, average forward (0.010231) and backward (0.021259) time: 0.077585, iotime: 0.006860 
2022-08-04 18:15:25,887 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083697, Speed: 1529.319284 images/s
2022-08-04 18:15:26,227 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:26,227 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:26,535 [dl_trainer.py:634] INFO train iter: 98, num_batches_per_epoch: 49
2022-08-04 18:15:26,536 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 44.993622, lr: 0.040245, avg loss: 1.496036
2022-08-04 18:15:28,063 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040245, val loss: 1.791348, val top-1 acc: 42.276503, top-5 acc: 91.969937
2022-08-04 18:15:29,187 [dl_trainer.py:731] WARNING [  2][  120/   49][rank:0] loss: 1.338, average forward (0.009808) and backward (0.025076) time: 0.081258, iotime: 0.006684 
2022-08-04 18:15:30,019 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086062, Speed: 1487.297121 images/s
2022-08-04 18:15:30,351 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:30,351 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:30,692 [dl_trainer.py:634] INFO train iter: 147, num_batches_per_epoch: 49
2022-08-04 18:15:30,693 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 54.878827, lr: 0.060163, avg loss: 1.228473
2022-08-04 18:15:32,197 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060163, val loss: 2.660195, val top-1 acc: 31.823576, top-5 acc: 83.109177
2022-08-04 18:15:32,804 [dl_trainer.py:731] WARNING [  3][  160/   49][rank:0] loss: 1.248, average forward (0.009951) and backward (0.022834) time: 0.078257, iotime: 0.007452 
2022-08-04 18:15:33,955 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081973, Speed: 1561.485286 images/s
2022-08-04 18:15:34,291 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:34,292 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:34,687 [dl_trainer.py:634] INFO train iter: 196, num_batches_per_epoch: 49
2022-08-04 18:15:34,687 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 61.926020, lr: 0.080082, avg loss: 1.065025
2022-08-04 18:15:36,202 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080082, val loss: 1.982809, val top-1 acc: 48.239715, top-5 acc: 93.591772
2022-08-04 18:15:36,437 [dl_trainer.py:731] WARNING [  4][  200/   49][rank:0] loss: 0.982, average forward (0.010161) and backward (0.022941) time: 0.079686, iotime: 0.007243 
2022-08-04 18:15:38,037 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085017, Speed: 1505.589318 images/s
2022-08-04 18:15:38,347 [dl_trainer.py:731] WARNING [  4][  240/   49][rank:0] loss: 0.712, average forward (0.009313) and backward (0.021272) time: 0.032940, iotime: 0.002098 
2022-08-04 18:15:38,362 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:38,362 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:38,820 [dl_trainer.py:634] INFO train iter: 245, num_batches_per_epoch: 49
2022-08-04 18:15:38,821 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 67.729592, lr: 0.100000, avg loss: 0.897077
2022-08-04 18:15:40,362 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 1.587425, val top-1 acc: 54.657832, top-5 acc: 93.829114
2022-08-04 18:15:42,015 [dl_trainer.py:731] WARNING [  5][  280/   49][rank:0] loss: 0.762, average forward (0.009669) and backward (0.020475) time: 0.075957, iotime: 0.006910 
2022-08-04 18:15:42,091 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084433, Speed: 1515.996906 images/s
2022-08-04 18:15:42,393 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:42,393 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:42,875 [dl_trainer.py:634] INFO train iter: 294, num_batches_per_epoch: 49
2022-08-04 18:15:42,876 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 71.859056, lr: 0.100000, avg loss: 0.797792
2022-08-04 18:15:44,420 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 1.315575, val top-1 acc: 60.294699, top-5 acc: 96.073972
2022-08-04 18:15:45,669 [dl_trainer.py:731] WARNING [  6][  320/   49][rank:0] loss: 0.595, average forward (0.009509) and backward (0.022458) time: 0.078432, iotime: 0.007043 
2022-08-04 18:15:46,133 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084184, Speed: 1520.477230 images/s
2022-08-04 18:15:46,451 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:46,451 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:46,997 [dl_trainer.py:634] INFO train iter: 343, num_batches_per_epoch: 49
2022-08-04 18:15:46,998 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 75.940689, lr: 0.100000, avg loss: 0.685511
2022-08-04 18:15:48,603 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 0.921309, val top-1 acc: 69.748813, top-5 acc: 98.219937
2022-08-04 18:15:49,376 [dl_trainer.py:731] WARNING [  7][  360/   49][rank:0] loss: 0.625, average forward (0.009829) and backward (0.022930) time: 0.080204, iotime: 0.006956 
2022-08-04 18:15:50,170 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084089, Speed: 1522.196432 images/s
2022-08-04 18:15:50,483 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:50,483 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:51,058 [dl_trainer.py:634] INFO train iter: 392, num_batches_per_epoch: 49
2022-08-04 18:15:51,058 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 77.359694, lr: 0.100000, avg loss: 0.654269
2022-08-04 18:15:52,607 [dl_trainer.py:822] INFO Epoch 8, lr: 0.100000, val loss: 0.992686, val top-1 acc: 69.125791, top-5 acc: 97.399130
2022-08-04 18:15:53,001 [dl_trainer.py:731] WARNING [  8][  400/   49][rank:0] loss: 0.554, average forward (0.009374) and backward (0.022023) time: 0.078625, iotime: 0.007374 
2022-08-04 18:15:54,244 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084857, Speed: 1508.417454 images/s
2022-08-04 18:15:54,599 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:54,599 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:55,158 [dl_trainer.py:731] WARNING [  8][  440/   49][rank:0] loss: 0.521, average forward (0.009527) and backward (0.023114) time: 0.039582, iotime: 0.006645 
2022-08-04 18:15:55,221 [dl_trainer.py:634] INFO train iter: 441, num_batches_per_epoch: 49
2022-08-04 18:15:55,221 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 79.607781, lr: 0.100000, avg loss: 0.578213
2022-08-04 18:15:56,757 [dl_trainer.py:822] INFO Epoch 9, lr: 0.100000, val loss: 0.870114, val top-1 acc: 71.904668, top-5 acc: 98.299051
2022-08-04 18:15:58,263 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083696, Speed: 1529.344062 images/s
2022-08-04 18:15:58,585 [dl_trainer.py:731] WARNING [  9][  480/   49][rank:0] loss: 0.537, average forward (0.009286) and backward (0.021085) time: 0.071167, iotime: 0.002053 
2022-08-04 18:15:58,598 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:15:58,598 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:15:59,251 [dl_trainer.py:634] INFO train iter: 490, num_batches_per_epoch: 49
2022-08-04 18:15:59,252 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 80.931122, lr: 0.100000, avg loss: 0.542893
2022-08-04 18:16:00,816 [dl_trainer.py:822] INFO Epoch 10, lr: 0.100000, val loss: 0.650612, val top-1 acc: 78.263449, top-5 acc: 98.763845
2022-08-04 18:16:02,260 [dl_trainer.py:731] WARNING [ 10][  520/   49][rank:0] loss: 0.431, average forward (0.009530) and backward (0.020163) time: 0.077671, iotime: 0.007358 
2022-08-04 18:16:02,330 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084718, Speed: 1510.892154 images/s
2022-08-04 18:16:02,654 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:02,655 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:03,398 [dl_trainer.py:634] INFO train iter: 539, num_batches_per_epoch: 49
2022-08-04 18:16:03,398 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 82.174745, lr: 0.100000, avg loss: 0.511567
2022-08-04 18:16:04,975 [dl_trainer.py:822] INFO Epoch 11, lr: 0.100000, val loss: 0.998611, val top-1 acc: 69.996044, top-5 acc: 97.142009
2022-08-04 18:16:05,984 [dl_trainer.py:731] WARNING [ 11][  560/   49][rank:0] loss: 0.464, average forward (0.009857) and backward (0.018816) time: 0.076096, iotime: 0.007629 
2022-08-04 18:16:06,465 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086099, Speed: 1486.667758 images/s
2022-08-04 18:16:06,812 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:06,812 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:07,627 [dl_trainer.py:634] INFO train iter: 588, num_batches_per_epoch: 49
2022-08-04 18:16:07,628 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 81.728316, lr: 0.100000, avg loss: 0.515349
2022-08-04 18:16:09,243 [dl_trainer.py:822] INFO Epoch 12, lr: 0.100000, val loss: 0.622115, val top-1 acc: 79.301820, top-5 acc: 99.070411
2022-08-04 18:16:09,870 [dl_trainer.py:731] WARNING [ 12][  600/   49][rank:0] loss: 0.534, average forward (0.010194) and backward (0.019887) time: 0.079815, iotime: 0.007645 
2022-08-04 18:16:10,695 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088080, Speed: 1453.225917 images/s
2022-08-04 18:16:11,039 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:11,039 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:11,829 [dl_trainer.py:634] INFO train iter: 637, num_batches_per_epoch: 49
2022-08-04 18:16:11,830 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 84.167730, lr: 0.100000, avg loss: 0.462996
2022-08-04 18:16:13,371 [dl_trainer.py:822] INFO Epoch 13, lr: 0.100000, val loss: 0.646061, val top-1 acc: 79.588608, top-5 acc: 98.605617
2022-08-04 18:16:13,508 [dl_trainer.py:731] WARNING [ 13][  640/   49][rank:0] loss: 0.675, average forward (0.009302) and backward (0.022218) time: 0.077518, iotime: 0.007078 
2022-08-04 18:16:14,703 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083481, Speed: 1533.277296 images/s
2022-08-04 18:16:15,035 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:15,035 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:15,628 [dl_trainer.py:731] WARNING [ 13][  680/   49][rank:0] loss: 0.359, average forward (0.009193) and backward (0.020698) time: 0.037416, iotime: 0.007258 
2022-08-04 18:16:15,925 [dl_trainer.py:634] INFO train iter: 686, num_batches_per_epoch: 49
2022-08-04 18:16:15,925 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 83.896684, lr: 0.100000, avg loss: 0.464951
2022-08-04 18:16:17,455 [dl_trainer.py:822] INFO Epoch 14, lr: 0.100000, val loss: 0.818326, val top-1 acc: 74.713212, top-5 acc: 97.557358
2022-08-04 18:16:18,791 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085130, Speed: 1503.583087 images/s
2022-08-04 18:16:19,123 [dl_trainer.py:731] WARNING [ 14][  720/   49][rank:0] loss: 0.643, average forward (0.010624) and backward (0.022968) time: 0.075601, iotime: 0.002352 
2022-08-04 18:16:19,135 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:19,135 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:20,042 [dl_trainer.py:634] INFO train iter: 735, num_batches_per_epoch: 49
2022-08-04 18:16:20,043 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 84.343112, lr: 0.100000, avg loss: 0.450039
2022-08-04 18:16:21,546 [dl_trainer.py:822] INFO Epoch 15, lr: 0.100000, val loss: 1.174767, val top-1 acc: 67.029272, top-5 acc: 97.804589
2022-08-04 18:16:22,776 [dl_trainer.py:731] WARNING [ 15][  760/   49][rank:0] loss: 0.464, average forward (0.009810) and backward (0.023211) time: 0.077857, iotime: 0.006873 
2022-08-04 18:16:22,830 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084124, Speed: 1521.569176 images/s
2022-08-04 18:16:23,183 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:23,183 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:24,148 [dl_trainer.py:634] INFO train iter: 784, num_batches_per_epoch: 49
2022-08-04 18:16:24,149 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 84.438776, lr: 0.100000, avg loss: 0.437397
2022-08-04 18:16:25,700 [dl_trainer.py:822] INFO Epoch 16, lr: 0.100000, val loss: 0.688872, val top-1 acc: 78.411788, top-5 acc: 98.783623
2022-08-04 18:16:26,489 [dl_trainer.py:731] WARNING [ 16][  800/   49][rank:0] loss: 0.372, average forward (0.009980) and backward (0.022509) time: 0.079732, iotime: 0.007026 
2022-08-04 18:16:26,901 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084794, Speed: 1509.533611 images/s
2022-08-04 18:16:27,225 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:27,225 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:28,219 [dl_trainer.py:634] INFO train iter: 833, num_batches_per_epoch: 49
2022-08-04 18:16:28,220 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 85.507015, lr: 0.100000, avg loss: 0.407032
2022-08-04 18:16:29,834 [dl_trainer.py:822] INFO Epoch 17, lr: 0.100000, val loss: 0.671461, val top-1 acc: 78.837025, top-5 acc: 98.684731
2022-08-04 18:16:30,183 [dl_trainer.py:731] WARNING [ 17][  840/   49][rank:0] loss: 0.472, average forward (0.009795) and backward (0.022173) time: 0.080188, iotime: 0.007483 
2022-08-04 18:16:31,015 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085664, Speed: 1494.216017 images/s
2022-08-04 18:16:31,356 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:31,356 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:32,280 [dl_trainer.py:731] WARNING [ 17][  880/   49][rank:0] loss: 0.502, average forward (0.009090) and backward (0.020642) time: 0.036753, iotime: 0.006736 
2022-08-04 18:16:32,385 [dl_trainer.py:634] INFO train iter: 882, num_batches_per_epoch: 49
2022-08-04 18:16:32,386 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 86.383929, lr: 0.100000, avg loss: 0.393524
2022-08-04 18:16:33,981 [dl_trainer.py:822] INFO Epoch 18, lr: 0.100000, val loss: 0.651902, val top-1 acc: 79.677611, top-5 acc: 98.546282
2022-08-04 18:16:35,166 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086469, Speed: 1480.298115 images/s
2022-08-04 18:16:35,531 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:35,531 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:36,086 [dl_trainer.py:731] WARNING [ 18][  920/   49][rank:0] loss: 0.422, average forward (0.008977) and backward (0.021844) time: 0.078237, iotime: 0.006415 
2022-08-04 18:16:36,641 [dl_trainer.py:634] INFO train iter: 931, num_batches_per_epoch: 49
2022-08-04 18:16:36,642 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 86.399872, lr: 0.100000, avg loss: 0.389232
2022-08-04 18:16:38,165 [dl_trainer.py:822] INFO Epoch 19, lr: 0.100000, val loss: 0.738727, val top-1 acc: 78.065665, top-5 acc: 98.645174
2022-08-04 18:16:39,213 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084279, Speed: 1518.758685 images/s
2022-08-04 18:16:39,530 [dl_trainer.py:731] WARNING [ 19][  960/   49][rank:0] loss: 0.347, average forward (0.010183) and backward (0.022836) time: 0.073756, iotime: 0.002307 
2022-08-04 18:16:39,549 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:39,549 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:40,720 [dl_trainer.py:634] INFO train iter: 980, num_batches_per_epoch: 49
2022-08-04 18:16:40,720 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 87.771046, lr: 0.100000, avg loss: 0.366962
2022-08-04 18:16:42,279 [dl_trainer.py:822] INFO Epoch 20, lr: 0.100000, val loss: 0.678780, val top-1 acc: 78.362342, top-5 acc: 98.823180
2022-08-04 18:16:43,293 [dl_trainer.py:731] WARNING [ 20][ 1000/   49][rank:0] loss: 0.451, average forward (0.010020) and backward (0.021066) time: 0.077875, iotime: 0.006664 
2022-08-04 18:16:43,355 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086277, Speed: 1483.599297 images/s
2022-08-04 18:16:43,696 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:43,696 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:44,871 [dl_trainer.py:634] INFO train iter: 1029, num_batches_per_epoch: 49
2022-08-04 18:16:44,872 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 87.228954, lr: 0.100000, avg loss: 0.355971
2022-08-04 18:16:46,428 [dl_trainer.py:822] INFO Epoch 21, lr: 0.100000, val loss: 0.542975, val top-1 acc: 82.624604, top-5 acc: 99.109968
2022-08-04 18:16:46,938 [dl_trainer.py:731] WARNING [ 21][ 1040/   49][rank:0] loss: 0.334, average forward (0.009766) and backward (0.020665) time: 0.076641, iotime: 0.006947 
2022-08-04 18:16:47,376 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083748, Speed: 1528.397285 images/s
2022-08-04 18:16:47,716 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:47,717 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:48,961 [dl_trainer.py:634] INFO train iter: 1078, num_batches_per_epoch: 49
2022-08-04 18:16:48,962 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 88.313138, lr: 0.100000, avg loss: 0.332624
2022-08-04 18:16:50,503 [dl_trainer.py:822] INFO Epoch 22, lr: 0.100000, val loss: 0.576771, val top-1 acc: 81.230222, top-5 acc: 99.307753
2022-08-04 18:16:50,668 [dl_trainer.py:731] WARNING [ 22][ 1080/   49][rank:0] loss: 0.315, average forward (0.010398) and backward (0.020693) time: 0.078942, iotime: 0.007211 
2022-08-04 18:16:51,458 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085014, Speed: 1505.626264 images/s
2022-08-04 18:16:51,808 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:51,808 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:52,753 [dl_trainer.py:731] WARNING [ 22][ 1120/   49][rank:0] loss: 0.374, average forward (0.009374) and backward (0.019269) time: 0.035953, iotime: 0.007040 
2022-08-04 18:16:53,108 [dl_trainer.py:634] INFO train iter: 1127, num_batches_per_epoch: 49
2022-08-04 18:16:53,108 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 87.659439, lr: 0.100000, avg loss: 0.345939
2022-08-04 18:16:54,630 [dl_trainer.py:822] INFO Epoch 23, lr: 0.100000, val loss: 0.529373, val top-1 acc: 83.761867, top-5 acc: 99.297864
2022-08-04 18:16:55,460 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083348, Speed: 1535.730699 images/s
2022-08-04 18:16:55,815 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:55,816 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:16:56,371 [dl_trainer.py:731] WARNING [ 23][ 1160/   49][rank:0] loss: 0.290, average forward (0.009725) and backward (0.023678) time: 0.078835, iotime: 0.006961 
2022-08-04 18:16:57,157 [dl_trainer.py:634] INFO train iter: 1176, num_batches_per_epoch: 49
2022-08-04 18:16:57,158 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 88.919005, lr: 0.100000, avg loss: 0.325788
2022-08-04 18:16:58,753 [dl_trainer.py:822] INFO Epoch 24, lr: 0.100000, val loss: 0.521683, val top-1 acc: 82.674051, top-5 acc: 98.931962
2022-08-04 18:16:59,590 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086018, Speed: 1488.063112 images/s
2022-08-04 18:16:59,935 [dl_trainer.py:731] WARNING [ 24][ 1200/   49][rank:0] loss: 0.231, average forward (0.010873) and backward (0.023782) time: 0.078202, iotime: 0.002574 
2022-08-04 18:16:59,941 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:16:59,941 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:01,303 [dl_trainer.py:634] INFO train iter: 1225, num_batches_per_epoch: 49
2022-08-04 18:17:01,304 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 88.632015, lr: 0.100000, avg loss: 0.316373
2022-08-04 18:17:02,840 [dl_trainer.py:822] INFO Epoch 25, lr: 0.100000, val loss: 0.914799, val top-1 acc: 73.714399, top-5 acc: 98.763845
2022-08-04 18:17:03,554 [dl_trainer.py:731] WARNING [ 25][ 1240/   49][rank:0] loss: 0.240, average forward (0.009329) and backward (0.021922) time: 0.077444, iotime: 0.007438 
2022-08-04 18:17:03,627 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084079, Speed: 1522.381858 images/s
2022-08-04 18:17:03,978 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:03,978 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:05,401 [dl_trainer.py:634] INFO train iter: 1274, num_batches_per_epoch: 49
2022-08-04 18:17:05,401 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 89.205995, lr: 0.100000, avg loss: 0.301630
2022-08-04 18:17:06,901 [dl_trainer.py:822] INFO Epoch 26, lr: 0.100000, val loss: 0.615358, val top-1 acc: 81.259889, top-5 acc: 98.664953
2022-08-04 18:17:07,191 [dl_trainer.py:731] WARNING [ 26][ 1280/   49][rank:0] loss: 0.260, average forward (0.009544) and backward (0.019451) time: 0.073959, iotime: 0.006647 
2022-08-04 18:17:07,622 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083186, Speed: 1538.724030 images/s
2022-08-04 18:17:07,962 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:07,962 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:09,326 [dl_trainer.py:731] WARNING [ 26][ 1320/   49][rank:0] loss: 0.266, average forward (0.011295) and backward (0.023158) time: 0.041726, iotime: 0.006932 
2022-08-04 18:17:09,473 [dl_trainer.py:634] INFO train iter: 1323, num_batches_per_epoch: 49
2022-08-04 18:17:09,474 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 90.019133, lr: 0.100000, avg loss: 0.288137
2022-08-04 18:17:10,982 [dl_trainer.py:822] INFO Epoch 27, lr: 0.100000, val loss: 0.565063, val top-1 acc: 83.336630, top-5 acc: 99.258307
2022-08-04 18:17:11,611 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083081, Speed: 1540.667767 images/s
2022-08-04 18:17:11,962 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:11,962 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:12,928 [dl_trainer.py:731] WARNING [ 27][ 1360/   49][rank:0] loss: 0.506, average forward (0.010183) and backward (0.021606) time: 0.077575, iotime: 0.007679 
2022-08-04 18:17:13,505 [dl_trainer.py:634] INFO train iter: 1372, num_batches_per_epoch: 49
2022-08-04 18:17:13,506 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 89.588648, lr: 0.100000, avg loss: 0.303907
2022-08-04 18:17:15,045 [dl_trainer.py:822] INFO Epoch 28, lr: 0.100000, val loss: 0.567536, val top-1 acc: 82.674051, top-5 acc: 99.436313
2022-08-04 18:17:15,698 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085112, Speed: 1503.898276 images/s
2022-08-04 18:17:16,034 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:16,034 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:16,600 [dl_trainer.py:731] WARNING [ 28][ 1400/   49][rank:0] loss: 0.213, average forward (0.009948) and backward (0.021687) time: 0.079049, iotime: 0.006901 
2022-08-04 18:17:17,625 [dl_trainer.py:634] INFO train iter: 1421, num_batches_per_epoch: 49
2022-08-04 18:17:17,626 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 89.269770, lr: 0.100000, avg loss: 0.308541
2022-08-04 18:17:19,216 [dl_trainer.py:822] INFO Epoch 29, lr: 0.100000, val loss: 0.532827, val top-1 acc: 84.088212, top-5 acc: 99.119858
2022-08-04 18:17:19,802 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085484, Speed: 1497.363921 images/s
2022-08-04 18:17:20,125 [dl_trainer.py:731] WARNING [ 29][ 1440/   49][rank:0] loss: 0.278, average forward (0.009348) and backward (0.019307) time: 0.071105, iotime: 0.002318 
2022-08-04 18:17:20,138 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:20,138 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:21,780 [dl_trainer.py:634] INFO train iter: 1470, num_batches_per_epoch: 49
2022-08-04 18:17:21,780 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 89.413265, lr: 0.100000, avg loss: 0.304385
2022-08-04 18:17:23,323 [dl_trainer.py:822] INFO Epoch 30, lr: 0.100000, val loss: 0.459167, val top-1 acc: 85.413370, top-5 acc: 99.376978
2022-08-04 18:17:23,815 [dl_trainer.py:731] WARNING [ 30][ 1480/   49][rank:0] loss: 0.265, average forward (0.010597) and backward (0.021372) time: 0.078697, iotime: 0.007074 
2022-08-04 18:17:23,881 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084956, Speed: 1506.655749 images/s
2022-08-04 18:17:24,213 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:24,213 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:25,883 [dl_trainer.py:634] INFO train iter: 1519, num_batches_per_epoch: 49
2022-08-04 18:17:25,883 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 89.620536, lr: 0.100000, avg loss: 0.288773
2022-08-04 18:17:27,497 [dl_trainer.py:822] INFO Epoch 31, lr: 0.100000, val loss: 0.470195, val top-1 acc: 85.136472, top-5 acc: 99.357199
2022-08-04 18:17:27,548 [dl_trainer.py:731] WARNING [ 31][ 1520/   49][rank:0] loss: 0.191, average forward (0.010061) and backward (0.022689) time: 0.080302, iotime: 0.006825 
2022-08-04 18:17:27,985 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085475, Speed: 1497.509668 images/s
2022-08-04 18:17:28,297 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:28,298 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:29,570 [dl_trainer.py:731] WARNING [ 31][ 1560/   49][rank:0] loss: 0.267, average forward (0.009332) and backward (0.019906) time: 0.036874, iotime: 0.007358 
2022-08-04 18:17:29,973 [dl_trainer.py:634] INFO train iter: 1568, num_batches_per_epoch: 49
2022-08-04 18:17:29,973 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 91.087372, lr: 0.100000, avg loss: 0.260598
2022-08-04 18:17:31,582 [dl_trainer.py:822] INFO Epoch 32, lr: 0.100000, val loss: 0.418612, val top-1 acc: 86.085839, top-5 acc: 99.465981
2022-08-04 18:17:32,033 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084311, Speed: 1518.195704 images/s
2022-08-04 18:17:32,375 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:32,375 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:33,311 [dl_trainer.py:731] WARNING [ 32][ 1600/   49][rank:0] loss: 0.309, average forward (0.009537) and backward (0.022951) time: 0.080694, iotime: 0.006804 
2022-08-04 18:17:34,129 [dl_trainer.py:634] INFO train iter: 1617, num_batches_per_epoch: 49
2022-08-04 18:17:34,130 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 90.625000, lr: 0.100000, avg loss: 0.274650
2022-08-04 18:17:35,679 [dl_trainer.py:822] INFO Epoch 33, lr: 0.100000, val loss: 0.624929, val top-1 acc: 80.963212, top-5 acc: 98.664953
2022-08-04 18:17:36,048 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083632, Speed: 1530.506322 images/s
2022-08-04 18:17:36,401 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:36,401 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:36,935 [dl_trainer.py:731] WARNING [ 33][ 1640/   49][rank:0] loss: 0.374, average forward (0.009398) and backward (0.020532) time: 0.075947, iotime: 0.006909 
2022-08-04 18:17:38,111 [dl_trainer.py:634] INFO train iter: 1666, num_batches_per_epoch: 49
2022-08-04 18:17:38,112 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 91.326531, lr: 0.100000, avg loss: 0.249030
2022-08-04 18:17:39,647 [dl_trainer.py:822] INFO Epoch 34, lr: 0.100000, val loss: 0.459820, val top-1 acc: 85.156250, top-5 acc: 99.258307
2022-08-04 18:17:39,971 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081688, Speed: 1566.943871 images/s
2022-08-04 18:17:40,295 [dl_trainer.py:731] WARNING [ 34][ 1680/   49][rank:0] loss: 0.170, average forward (0.009994) and backward (0.022472) time: 0.073975, iotime: 0.002311 
2022-08-04 18:17:40,305 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:40,306 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:42,121 [dl_trainer.py:634] INFO train iter: 1715, num_batches_per_epoch: 49
2022-08-04 18:17:42,122 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 91.039541, lr: 0.100000, avg loss: 0.258045
2022-08-04 18:17:43,663 [dl_trainer.py:822] INFO Epoch 35, lr: 0.100000, val loss: 0.624103, val top-1 acc: 81.991693, top-5 acc: 98.882516
2022-08-04 18:17:43,912 [dl_trainer.py:731] WARNING [ 35][ 1720/   49][rank:0] loss: 0.191, average forward (0.009305) and backward (0.021206) time: 0.076576, iotime: 0.007170 
2022-08-04 18:17:43,973 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083343, Speed: 1535.827442 images/s
2022-08-04 18:17:44,312 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:44,312 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:46,093 [dl_trainer.py:731] WARNING [ 35][ 1760/   49][rank:0] loss: 0.170, average forward (0.010326) and backward (0.022415) time: 0.041388, iotime: 0.008327 
2022-08-04 18:17:46,285 [dl_trainer.py:634] INFO train iter: 1764, num_batches_per_epoch: 49
2022-08-04 18:17:46,285 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 90.720663, lr: 0.100000, avg loss: 0.259844
2022-08-04 18:17:47,852 [dl_trainer.py:822] INFO Epoch 36, lr: 0.100000, val loss: 0.579428, val top-1 acc: 82.703718, top-5 acc: 99.179193
2022-08-04 18:17:48,147 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086951, Speed: 1472.100416 images/s
2022-08-04 18:17:48,486 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:48,486 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:49,856 [dl_trainer.py:731] WARNING [ 36][ 1800/   49][rank:0] loss: 0.287, average forward (0.009441) and backward (0.019545) time: 0.078185, iotime: 0.008421 
2022-08-04 18:17:50,492 [dl_trainer.py:634] INFO train iter: 1813, num_batches_per_epoch: 49
2022-08-04 18:17:50,492 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 91.246811, lr: 0.100000, avg loss: 0.249857
2022-08-04 18:17:52,050 [dl_trainer.py:822] INFO Epoch 37, lr: 0.100000, val loss: 0.522414, val top-1 acc: 84.394778, top-5 acc: 99.505538
2022-08-04 18:17:52,247 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085387, Speed: 1499.065754 images/s
2022-08-04 18:17:52,592 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:52,592 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:53,519 [dl_trainer.py:731] WARNING [ 37][ 1840/   49][rank:0] loss: 0.230, average forward (0.009501) and backward (0.022605) time: 0.078708, iotime: 0.007258 
2022-08-04 18:17:54,604 [dl_trainer.py:634] INFO train iter: 1862, num_batches_per_epoch: 49
2022-08-04 18:17:54,604 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 91.470026, lr: 0.100000, avg loss: 0.246980
2022-08-04 18:17:56,152 [dl_trainer.py:822] INFO Epoch 38, lr: 0.100000, val loss: 0.538085, val top-1 acc: 84.107991, top-5 acc: 99.327532
2022-08-04 18:17:56,328 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084990, Speed: 1506.064118 images/s
2022-08-04 18:17:56,679 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:17:56,680 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:17:57,266 [dl_trainer.py:731] WARNING [ 38][ 1880/   49][rank:0] loss: 0.227, average forward (0.009385) and backward (0.022915) time: 0.079493, iotime: 0.007345 
2022-08-04 18:17:58,842 [dl_trainer.py:634] INFO train iter: 1911, num_batches_per_epoch: 49
2022-08-04 18:17:58,842 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 91.262755, lr: 0.100000, avg loss: 0.244110
2022-08-04 18:18:00,425 [dl_trainer.py:822] INFO Epoch 39, lr: 0.100000, val loss: 0.436594, val top-1 acc: 86.362737, top-5 acc: 99.604430
2022-08-04 18:18:00,525 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087403, Speed: 1464.488707 images/s
2022-08-04 18:18:00,845 [dl_trainer.py:731] WARNING [ 39][ 1920/   49][rank:0] loss: 0.193, average forward (0.008549) and backward (0.021726) time: 0.072090, iotime: 0.001927 
2022-08-04 18:18:00,857 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:00,857 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:03,006 [dl_trainer.py:731] WARNING [ 39][ 1960/   49][rank:0] loss: 0.194, average forward (0.010271) and backward (0.022814) time: 0.040706, iotime: 0.007307 
2022-08-04 18:18:03,014 [dl_trainer.py:634] INFO train iter: 1960, num_batches_per_epoch: 49
2022-08-04 18:18:03,015 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 91.533801, lr: 0.100000, avg loss: 0.239178
2022-08-04 18:18:04,598 [dl_trainer.py:822] INFO Epoch 40, lr: 0.100000, val loss: 0.507542, val top-1 acc: 85.502373, top-5 acc: 99.198972
2022-08-04 18:18:04,687 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086694, Speed: 1476.464748 images/s
2022-08-04 18:18:05,036 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:05,037 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:06,788 [dl_trainer.py:731] WARNING [ 40][ 2000/   49][rank:0] loss: 0.321, average forward (0.009009) and backward (0.021957) time: 0.078936, iotime: 0.006873 
2022-08-04 18:18:07,203 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052382, Speed: 2443.578117 images/s
2022-08-04 18:18:07,203 [dl_trainer.py:634] INFO train iter: 2009, num_batches_per_epoch: 49
2022-08-04 18:18:07,204 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 92.330995, lr: 0.100000, avg loss: 0.224166
2022-08-04 18:18:08,825 [dl_trainer.py:822] INFO Epoch 41, lr: 0.100000, val loss: 0.530884, val top-1 acc: 84.691456, top-5 acc: 99.515427
2022-08-04 18:18:09,155 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:09,155 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:10,468 [dl_trainer.py:731] WARNING [ 41][ 2040/   49][rank:0] loss: 0.209, average forward (0.009774) and backward (0.021002) time: 0.079357, iotime: 0.007720 
2022-08-04 18:18:11,282 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084961, Speed: 1506.579997 images/s
2022-08-04 18:18:11,333 [dl_trainer.py:634] INFO train iter: 2058, num_batches_per_epoch: 49
2022-08-04 18:18:11,333 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 92.059949, lr: 0.100000, avg loss: 0.229222
2022-08-04 18:18:12,979 [dl_trainer.py:822] INFO Epoch 42, lr: 0.100000, val loss: 0.480694, val top-1 acc: 84.909019, top-5 acc: 99.485759
2022-08-04 18:18:13,291 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:13,291 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:14,234 [dl_trainer.py:731] WARNING [ 42][ 2080/   49][rank:0] loss: 0.398, average forward (0.009581) and backward (0.020195) time: 0.079602, iotime: 0.007619 
2022-08-04 18:18:15,431 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086409, Speed: 1481.334456 images/s
2022-08-04 18:18:15,532 [dl_trainer.py:634] INFO train iter: 2107, num_batches_per_epoch: 49
2022-08-04 18:18:15,533 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 91.900510, lr: 0.100000, avg loss: 0.235404
2022-08-04 18:18:17,118 [dl_trainer.py:822] INFO Epoch 43, lr: 0.100000, val loss: 0.595441, val top-1 acc: 83.870649, top-5 acc: 99.278085
2022-08-04 18:18:17,380 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:17,380 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:17,950 [dl_trainer.py:731] WARNING [ 43][ 2120/   49][rank:0] loss: 0.273, average forward (0.008935) and backward (0.019161) time: 0.075384, iotime: 0.007220 
2022-08-04 18:18:19,545 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085687, Speed: 1493.807710 images/s
2022-08-04 18:18:19,690 [dl_trainer.py:634] INFO train iter: 2156, num_batches_per_epoch: 49
2022-08-04 18:18:19,691 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 92.554209, lr: 0.100000, avg loss: 0.208695
2022-08-04 18:18:21,287 [dl_trainer.py:822] INFO Epoch 44, lr: 0.100000, val loss: 0.644473, val top-1 acc: 81.704905, top-5 acc: 98.902294
2022-08-04 18:18:21,520 [dl_trainer.py:731] WARNING [ 44][ 2160/   49][rank:0] loss: 0.274, average forward (0.009095) and backward (0.021266) time: 0.073837, iotime: 0.002099 
2022-08-04 18:18:21,530 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:21,530 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:23,682 [dl_trainer.py:731] WARNING [ 44][ 2200/   49][rank:0] loss: 0.238, average forward (0.009761) and backward (0.020554) time: 0.037754, iotime: 0.007138 
2022-08-04 18:18:23,743 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087431, Speed: 1464.005405 images/s
2022-08-04 18:18:23,933 [dl_trainer.py:634] INFO train iter: 2205, num_batches_per_epoch: 49
2022-08-04 18:18:23,933 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 92.187500, lr: 0.100000, avg loss: 0.225929
2022-08-04 18:18:25,598 [dl_trainer.py:822] INFO Epoch 45, lr: 0.100000, val loss: 0.514913, val top-1 acc: 84.968354, top-5 acc: 99.386867
2022-08-04 18:18:25,760 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:25,760 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:27,492 [dl_trainer.py:731] WARNING [ 45][ 2240/   49][rank:0] loss: 0.169, average forward (0.009568) and backward (0.019130) time: 0.078682, iotime: 0.008012 
2022-08-04 18:18:27,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087253, Speed: 1466.998698 images/s
2022-08-04 18:18:28,152 [dl_trainer.py:634] INFO train iter: 2254, num_batches_per_epoch: 49
2022-08-04 18:18:28,152 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 92.346939, lr: 0.100000, avg loss: 0.220476
2022-08-04 18:18:29,745 [dl_trainer.py:822] INFO Epoch 46, lr: 0.100000, val loss: 0.502266, val top-1 acc: 85.433149, top-5 acc: 99.169304
2022-08-04 18:18:29,898 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:29,899 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:31,262 [dl_trainer.py:731] WARNING [ 46][ 2280/   49][rank:0] loss: 0.181, average forward (0.009570) and backward (0.021739) time: 0.081030, iotime: 0.008313 
2022-08-04 18:18:32,127 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087351, Speed: 1465.355607 images/s
2022-08-04 18:18:32,412 [dl_trainer.py:634] INFO train iter: 2303, num_batches_per_epoch: 49
2022-08-04 18:18:32,412 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 92.394770, lr: 0.100000, avg loss: 0.209823
2022-08-04 18:18:34,017 [dl_trainer.py:822] INFO Epoch 47, lr: 0.100000, val loss: 0.604952, val top-1 acc: 83.484968, top-5 acc: 99.406646
2022-08-04 18:18:34,075 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:34,075 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:35,049 [dl_trainer.py:731] WARNING [ 47][ 2320/   49][rank:0] loss: 0.129, average forward (0.010285) and backward (0.023007) time: 0.080741, iotime: 0.006953 
2022-08-04 18:18:36,289 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086674, Speed: 1476.799559 images/s
2022-08-04 18:18:36,610 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:36,610 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:36,611 [dl_trainer.py:634] INFO train iter: 2352, num_batches_per_epoch: 49
2022-08-04 18:18:36,611 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 92.649872, lr: 0.100000, avg loss: 0.204126
2022-08-04 18:18:38,265 [dl_trainer.py:822] INFO Epoch 48, lr: 0.100000, val loss: 0.489472, val top-1 acc: 86.016614, top-5 acc: 99.446203
2022-08-04 18:18:38,862 [dl_trainer.py:731] WARNING [ 48][ 2360/   49][rank:0] loss: 0.185, average forward (0.009650) and backward (0.022439) time: 0.081433, iotime: 0.006587 
2022-08-04 18:18:40,453 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086737, Speed: 1475.720704 images/s
2022-08-04 18:18:40,756 [dl_trainer.py:731] WARNING [ 48][ 2400/   49][rank:0] loss: 0.173, average forward (0.009780) and backward (0.023246) time: 0.035479, iotime: 0.002189 
2022-08-04 18:18:40,764 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:40,764 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:41,032 [dl_trainer.py:634] INFO train iter: 2401, num_batches_per_epoch: 49
2022-08-04 18:18:41,033 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 93.160077, lr: 0.100000, avg loss: 0.194815
2022-08-04 18:18:42,615 [dl_trainer.py:822] INFO Epoch 49, lr: 0.100000, val loss: 0.536296, val top-1 acc: 84.731013, top-5 acc: 99.663766
2022-08-04 18:18:44,439 [dl_trainer.py:731] WARNING [ 49][ 2440/   49][rank:0] loss: 0.267, average forward (0.009942) and backward (0.020399) time: 0.078064, iotime: 0.007629 
2022-08-04 18:18:44,509 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084464, Speed: 1515.433741 images/s
2022-08-04 18:18:44,827 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:44,827 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:45,137 [dl_trainer.py:634] INFO train iter: 2450, num_batches_per_epoch: 49
2022-08-04 18:18:45,138 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 92.952806, lr: 0.100000, avg loss: 0.201494
2022-08-04 18:18:46,772 [dl_trainer.py:822] INFO Epoch 50, lr: 0.100000, val loss: 0.707372, val top-1 acc: 81.704905, top-5 acc: 99.189082
2022-08-04 18:18:48,188 [dl_trainer.py:731] WARNING [ 50][ 2480/   49][rank:0] loss: 0.236, average forward (0.008888) and backward (0.020606) time: 0.079415, iotime: 0.006978 
2022-08-04 18:18:48,627 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085771, Speed: 1492.343143 images/s
2022-08-04 18:18:48,944 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:48,944 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:49,275 [dl_trainer.py:634] INFO train iter: 2499, num_batches_per_epoch: 49
2022-08-04 18:18:49,276 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 93.431122, lr: 0.100000, avg loss: 0.182591
2022-08-04 18:18:50,819 [dl_trainer.py:822] INFO Epoch 51, lr: 0.100000, val loss: 0.519028, val top-1 acc: 85.601266, top-5 acc: 99.337421
2022-08-04 18:18:51,806 [dl_trainer.py:731] WARNING [ 51][ 2520/   49][rank:0] loss: 0.148, average forward (0.009594) and backward (0.021706) time: 0.077200, iotime: 0.006955 
2022-08-04 18:18:52,634 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083460, Speed: 1533.664567 images/s
2022-08-04 18:18:52,978 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:52,979 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:53,368 [dl_trainer.py:634] INFO train iter: 2548, num_batches_per_epoch: 49
2022-08-04 18:18:53,369 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 93.064413, lr: 0.100000, avg loss: 0.196755
2022-08-04 18:18:54,896 [dl_trainer.py:822] INFO Epoch 52, lr: 0.100000, val loss: 0.540302, val top-1 acc: 84.928797, top-5 acc: 99.456092
2022-08-04 18:18:55,535 [dl_trainer.py:731] WARNING [ 52][ 2560/   49][rank:0] loss: 0.163, average forward (0.009668) and backward (0.021093) time: 0.078424, iotime: 0.007701 
2022-08-04 18:18:56,723 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085176, Speed: 1502.764319 images/s
2022-08-04 18:18:57,064 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:18:57,065 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:18:57,498 [dl_trainer.py:634] INFO train iter: 2597, num_batches_per_epoch: 49
2022-08-04 18:18:57,499 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 93.207908, lr: 0.100000, avg loss: 0.194417
2022-08-04 18:18:59,025 [dl_trainer.py:822] INFO Epoch 53, lr: 0.100000, val loss: 0.455983, val top-1 acc: 86.768196, top-5 acc: 99.446203
2022-08-04 18:18:59,178 [dl_trainer.py:731] WARNING [ 53][ 2600/   49][rank:0] loss: 0.204, average forward (0.009401) and backward (0.021423) time: 0.076594, iotime: 0.007225 
2022-08-04 18:19:00,715 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083130, Speed: 1539.761846 images/s
2022-08-04 18:19:01,054 [dl_trainer.py:731] WARNING [ 53][ 2640/   49][rank:0] loss: 0.139, average forward (0.010374) and backward (0.023528) time: 0.036446, iotime: 0.002272 
2022-08-04 18:19:01,062 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:01,062 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:01,529 [dl_trainer.py:634] INFO train iter: 2646, num_batches_per_epoch: 49
2022-08-04 18:19:01,530 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 93.989158, lr: 0.100000, avg loss: 0.173443
2022-08-04 18:19:03,054 [dl_trainer.py:822] INFO Epoch 54, lr: 0.100000, val loss: 0.456755, val top-1 acc: 87.035206, top-5 acc: 99.653877
2022-08-04 18:19:04,698 [dl_trainer.py:731] WARNING [ 54][ 2680/   49][rank:0] loss: 0.101, average forward (0.008591) and backward (0.022537) time: 0.076657, iotime: 0.006688 
2022-08-04 18:19:04,758 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084204, Speed: 1520.121069 images/s
2022-08-04 18:19:05,094 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:05,094 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:05,603 [dl_trainer.py:634] INFO train iter: 2695, num_batches_per_epoch: 49
2022-08-04 18:19:05,603 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 94.036990, lr: 0.100000, avg loss: 0.175432
2022-08-04 18:19:07,146 [dl_trainer.py:822] INFO Epoch 55, lr: 0.100000, val loss: 0.486122, val top-1 acc: 86.145174, top-5 acc: 99.347310
2022-08-04 18:19:08,278 [dl_trainer.py:731] WARNING [ 55][ 2720/   49][rank:0] loss: 0.209, average forward (0.010026) and backward (0.022088) time: 0.078276, iotime: 0.007201 
2022-08-04 18:19:08,718 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082462, Speed: 1552.231280 images/s
2022-08-04 18:19:09,036 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:09,036 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:09,580 [dl_trainer.py:634] INFO train iter: 2744, num_batches_per_epoch: 49
2022-08-04 18:19:09,580 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 93.829719, lr: 0.100000, avg loss: 0.179417
2022-08-04 18:19:11,133 [dl_trainer.py:822] INFO Epoch 56, lr: 0.100000, val loss: 0.536599, val top-1 acc: 85.284810, top-5 acc: 99.248418
2022-08-04 18:19:11,901 [dl_trainer.py:731] WARNING [ 56][ 2760/   49][rank:0] loss: 0.159, average forward (0.009199) and backward (0.021038) time: 0.076887, iotime: 0.006980 
2022-08-04 18:19:12,765 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084302, Speed: 1518.341688 images/s
2022-08-04 18:19:13,081 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:13,081 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:13,687 [dl_trainer.py:634] INFO train iter: 2793, num_batches_per_epoch: 49
2022-08-04 18:19:13,688 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 93.877551, lr: 0.100000, avg loss: 0.171151
2022-08-04 18:19:15,188 [dl_trainer.py:822] INFO Epoch 57, lr: 0.100000, val loss: 0.475994, val top-1 acc: 86.352848, top-5 acc: 99.465981
2022-08-04 18:19:15,482 [dl_trainer.py:731] WARNING [ 57][ 2800/   49][rank:0] loss: 0.133, average forward (0.010446) and backward (0.021298) time: 0.076810, iotime: 0.007174 
2022-08-04 18:19:16,630 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080486, Speed: 1590.345767 images/s
2022-08-04 18:19:16,955 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:16,955 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:17,493 [dl_trainer.py:731] WARNING [ 57][ 2840/   49][rank:0] loss: 0.208, average forward (0.009387) and backward (0.019211) time: 0.035681, iotime: 0.006823 
2022-08-04 18:19:17,601 [dl_trainer.py:634] INFO train iter: 2842, num_batches_per_epoch: 49
2022-08-04 18:19:17,602 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 93.654337, lr: 0.100000, avg loss: 0.173803
2022-08-04 18:19:19,099 [dl_trainer.py:822] INFO Epoch 58, lr: 0.100000, val loss: 0.444826, val top-1 acc: 86.995649, top-5 acc: 99.505538
2022-08-04 18:19:20,658 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083900, Speed: 1525.630267 images/s
2022-08-04 18:19:20,964 [dl_trainer.py:731] WARNING [ 58][ 2880/   49][rank:0] loss: 0.147, average forward (0.009236) and backward (0.021970) time: 0.071750, iotime: 0.002137 
2022-08-04 18:19:20,978 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:20,978 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:21,708 [dl_trainer.py:634] INFO train iter: 2891, num_batches_per_epoch: 49
2022-08-04 18:19:21,709 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 93.638393, lr: 0.100000, avg loss: 0.174764
2022-08-04 18:19:23,253 [dl_trainer.py:822] INFO Epoch 59, lr: 0.100000, val loss: 0.559878, val top-1 acc: 84.602453, top-5 acc: 99.386867
2022-08-04 18:19:24,613 [dl_trainer.py:731] WARNING [ 59][ 2920/   49][rank:0] loss: 0.187, average forward (0.011317) and backward (0.020626) time: 0.078272, iotime: 0.007312 
2022-08-04 18:19:24,669 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083530, Speed: 1532.383778 images/s
2022-08-04 18:19:25,003 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:25,004 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:25,735 [dl_trainer.py:634] INFO train iter: 2940, num_batches_per_epoch: 49
2022-08-04 18:19:25,736 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 94.212372, lr: 0.100000, avg loss: 0.167281
2022-08-04 18:19:27,268 [dl_trainer.py:822] INFO Epoch 60, lr: 0.100000, val loss: 0.477864, val top-1 acc: 86.886867, top-5 acc: 99.574763
2022-08-04 18:19:28,258 [dl_trainer.py:731] WARNING [ 60][ 2960/   49][rank:0] loss: 0.161, average forward (0.009930) and backward (0.021151) time: 0.078006, iotime: 0.007039 
2022-08-04 18:19:28,698 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083907, Speed: 1525.505364 images/s
2022-08-04 18:19:29,041 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:29,041 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:29,866 [dl_trainer.py:634] INFO train iter: 2989, num_batches_per_epoch: 49
2022-08-04 18:19:29,866 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 93.526786, lr: 0.100000, avg loss: 0.174184
2022-08-04 18:19:31,412 [dl_trainer.py:822] INFO Epoch 61, lr: 0.100000, val loss: 0.644318, val top-1 acc: 82.308149, top-5 acc: 99.426424
2022-08-04 18:19:31,942 [dl_trainer.py:731] WARNING [ 61][ 3000/   49][rank:0] loss: 0.132, average forward (0.010517) and backward (0.022030) time: 0.078475, iotime: 0.006931 
2022-08-04 18:19:32,761 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084636, Speed: 1512.357160 images/s
2022-08-04 18:19:33,090 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:33,090 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:33,911 [dl_trainer.py:634] INFO train iter: 3038, num_batches_per_epoch: 49
2022-08-04 18:19:33,912 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 93.941327, lr: 0.100000, avg loss: 0.166448
2022-08-04 18:19:35,461 [dl_trainer.py:822] INFO Epoch 62, lr: 0.100000, val loss: 0.511412, val top-1 acc: 85.363924, top-5 acc: 99.456092
2022-08-04 18:19:35,583 [dl_trainer.py:731] WARNING [ 62][ 3040/   49][rank:0] loss: 0.119, average forward (0.010720) and backward (0.022716) time: 0.080269, iotime: 0.006907 
2022-08-04 18:19:36,775 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083595, Speed: 1531.183547 images/s
2022-08-04 18:19:37,089 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:37,089 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:37,627 [dl_trainer.py:731] WARNING [ 62][ 3080/   49][rank:0] loss: 0.113, average forward (0.009583) and backward (0.021517) time: 0.038145, iotime: 0.006763 
2022-08-04 18:19:37,969 [dl_trainer.py:634] INFO train iter: 3087, num_batches_per_epoch: 49
2022-08-04 18:19:37,969 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 94.451531, lr: 0.100000, avg loss: 0.153714
2022-08-04 18:19:39,514 [dl_trainer.py:822] INFO Epoch 63, lr: 0.100000, val loss: 0.445163, val top-1 acc: 87.143987, top-5 acc: 99.535206
2022-08-04 18:19:40,808 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084004, Speed: 1523.730121 images/s
2022-08-04 18:19:41,145 [dl_trainer.py:731] WARNING [ 63][ 3120/   49][rank:0] loss: 0.206, average forward (0.009020) and backward (0.023254) time: 0.073305, iotime: 0.002069 
2022-08-04 18:19:41,157 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:41,157 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:42,045 [dl_trainer.py:634] INFO train iter: 3136, num_batches_per_epoch: 49
2022-08-04 18:19:42,045 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 95.025510, lr: 0.100000, avg loss: 0.152728
2022-08-04 18:19:43,615 [dl_trainer.py:822] INFO Epoch 64, lr: 0.100000, val loss: 0.532983, val top-1 acc: 85.057358, top-5 acc: 99.475870
2022-08-04 18:19:44,691 [dl_trainer.py:731] WARNING [ 64][ 3160/   49][rank:0] loss: 0.113, average forward (0.008671) and backward (0.020166) time: 0.075750, iotime: 0.006859 
2022-08-04 18:19:44,761 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082335, Speed: 1554.621582 images/s
2022-08-04 18:19:45,128 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:45,128 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:46,148 [dl_trainer.py:634] INFO train iter: 3185, num_batches_per_epoch: 49
2022-08-04 18:19:46,148 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 95.073342, lr: 0.100000, avg loss: 0.138900
2022-08-04 18:19:47,714 [dl_trainer.py:822] INFO Epoch 65, lr: 0.100000, val loss: 0.463062, val top-1 acc: 87.074763, top-5 acc: 99.495649
2022-08-04 18:19:48,454 [dl_trainer.py:731] WARNING [ 65][ 3200/   49][rank:0] loss: 0.105, average forward (0.009340) and backward (0.020137) time: 0.076063, iotime: 0.007069 
2022-08-04 18:19:48,902 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086239, Speed: 1484.243502 images/s
2022-08-04 18:19:49,229 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:49,229 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:50,285 [dl_trainer.py:634] INFO train iter: 3234, num_batches_per_epoch: 49
2022-08-04 18:19:50,285 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 94.690689, lr: 0.100000, avg loss: 0.152008
2022-08-04 18:19:51,812 [dl_trainer.py:822] INFO Epoch 66, lr: 0.100000, val loss: 0.578190, val top-1 acc: 84.770570, top-5 acc: 99.465981
2022-08-04 18:19:52,163 [dl_trainer.py:731] WARNING [ 66][ 3240/   49][rank:0] loss: 0.136, average forward (0.009380) and backward (0.019742) time: 0.076219, iotime: 0.007390 
2022-08-04 18:19:52,964 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084589, Speed: 1513.205344 images/s
2022-08-04 18:19:53,307 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:53,307 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:54,216 [dl_trainer.py:731] WARNING [ 66][ 3280/   49][rank:0] loss: 0.097, average forward (0.009672) and backward (0.022030) time: 0.039045, iotime: 0.007062 
2022-08-04 18:19:54,380 [dl_trainer.py:634] INFO train iter: 3283, num_batches_per_epoch: 49
2022-08-04 18:19:54,380 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 94.610969, lr: 0.100000, avg loss: 0.155469
2022-08-04 18:19:55,950 [dl_trainer.py:822] INFO Epoch 67, lr: 0.100000, val loss: 0.544278, val top-1 acc: 85.314478, top-5 acc: 99.624209
2022-08-04 18:19:56,996 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083965, Speed: 1524.446990 images/s
2022-08-04 18:19:57,313 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:19:57,313 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:19:57,886 [dl_trainer.py:731] WARNING [ 67][ 3320/   49][rank:0] loss: 0.108, average forward (0.009761) and backward (0.021932) time: 0.078398, iotime: 0.007076 
2022-08-04 18:19:58,476 [dl_trainer.py:634] INFO train iter: 3332, num_batches_per_epoch: 49
2022-08-04 18:19:58,477 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 94.642857, lr: 0.100000, avg loss: 0.152745
2022-08-04 18:20:00,064 [dl_trainer.py:822] INFO Epoch 68, lr: 0.100000, val loss: 0.484513, val top-1 acc: 86.708861, top-5 acc: 99.584652
2022-08-04 18:20:01,085 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085173, Speed: 1502.827681 images/s
2022-08-04 18:20:01,408 [dl_trainer.py:731] WARNING [ 68][ 3360/   49][rank:0] loss: 0.222, average forward (0.008771) and backward (0.023284) time: 0.075457, iotime: 0.002046 
2022-08-04 18:20:01,422 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:01,422 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:02,619 [dl_trainer.py:634] INFO train iter: 3381, num_batches_per_epoch: 49
2022-08-04 18:20:02,620 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 94.610969, lr: 0.100000, avg loss: 0.148594
2022-08-04 18:20:04,198 [dl_trainer.py:822] INFO Epoch 69, lr: 0.100000, val loss: 0.414761, val top-1 acc: 88.073576, top-5 acc: 99.545095
2022-08-04 18:20:05,057 [dl_trainer.py:731] WARNING [ 69][ 3400/   49][rank:0] loss: 0.102, average forward (0.009255) and backward (0.020462) time: 0.076439, iotime: 0.006904 
2022-08-04 18:20:05,127 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084182, Speed: 1520.517243 images/s
2022-08-04 18:20:05,460 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:05,460 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:06,625 [dl_trainer.py:634] INFO train iter: 3430, num_batches_per_epoch: 49
2022-08-04 18:20:06,626 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 94.451531, lr: 0.100000, avg loss: 0.158418
2022-08-04 18:20:08,220 [dl_trainer.py:822] INFO Epoch 70, lr: 0.100000, val loss: 0.548644, val top-1 acc: 85.255142, top-5 acc: 99.436313
2022-08-04 18:20:08,719 [dl_trainer.py:731] WARNING [ 70][ 3440/   49][rank:0] loss: 0.099, average forward (0.008911) and backward (0.019910) time: 0.077235, iotime: 0.007406 
2022-08-04 18:20:09,123 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083222, Speed: 1538.061785 images/s
2022-08-04 18:20:09,473 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:09,473 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:10,801 [dl_trainer.py:634] INFO train iter: 3479, num_batches_per_epoch: 49
2022-08-04 18:20:10,801 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 94.706633, lr: 0.100000, avg loss: 0.152815
2022-08-04 18:20:12,352 [dl_trainer.py:822] INFO Epoch 71, lr: 0.100000, val loss: 0.464018, val top-1 acc: 87.124209, top-5 acc: 99.594541
2022-08-04 18:20:12,403 [dl_trainer.py:731] WARNING [ 71][ 3480/   49][rank:0] loss: 0.070, average forward (0.009073) and backward (0.021124) time: 0.075655, iotime: 0.006351 
2022-08-04 18:20:13,225 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085432, Speed: 1498.264005 images/s
2022-08-04 18:20:13,552 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:13,552 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:14,499 [dl_trainer.py:731] WARNING [ 71][ 3520/   49][rank:0] loss: 0.120, average forward (0.008841) and backward (0.021699) time: 0.037943, iotime: 0.007137 
2022-08-04 18:20:14,883 [dl_trainer.py:634] INFO train iter: 3528, num_batches_per_epoch: 49
2022-08-04 18:20:14,883 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 95.248724, lr: 0.100000, avg loss: 0.127662
2022-08-04 18:20:16,396 [dl_trainer.py:822] INFO Epoch 72, lr: 0.100000, val loss: 0.662683, val top-1 acc: 84.325554, top-5 acc: 99.258307
2022-08-04 18:20:17,221 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083238, Speed: 1537.754412 images/s
2022-08-04 18:20:17,553 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:17,553 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:18,096 [dl_trainer.py:731] WARNING [ 72][ 3560/   49][rank:0] loss: 0.092, average forward (0.010424) and backward (0.021650) time: 0.077786, iotime: 0.007050 
2022-08-04 18:20:18,932 [dl_trainer.py:634] INFO train iter: 3577, num_batches_per_epoch: 49
2022-08-04 18:20:18,933 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 94.658801, lr: 0.100000, avg loss: 0.145228
2022-08-04 18:20:20,508 [dl_trainer.py:822] INFO Epoch 73, lr: 0.100000, val loss: 0.651725, val top-1 acc: 83.702532, top-5 acc: 99.495649
2022-08-04 18:20:21,290 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084745, Speed: 1510.418024 images/s
2022-08-04 18:20:21,608 [dl_trainer.py:731] WARNING [ 73][ 3600/   49][rank:0] loss: 0.200, average forward (0.009923) and backward (0.023600) time: 0.075430, iotime: 0.002188 
2022-08-04 18:20:21,615 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:21,615 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:23,039 [dl_trainer.py:634] INFO train iter: 3626, num_batches_per_epoch: 49
2022-08-04 18:20:23,040 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 94.834184, lr: 0.100000, avg loss: 0.144005
2022-08-04 18:20:24,613 [dl_trainer.py:822] INFO Epoch 74, lr: 0.100000, val loss: 0.586871, val top-1 acc: 84.899130, top-5 acc: 99.208861
2022-08-04 18:20:25,360 [dl_trainer.py:731] WARNING [ 74][ 3640/   49][rank:0] loss: 0.143, average forward (0.010014) and backward (0.022663) time: 0.080740, iotime: 0.006864 
2022-08-04 18:20:25,407 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085751, Speed: 1492.690728 images/s
2022-08-04 18:20:25,744 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:25,744 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:27,220 [dl_trainer.py:634] INFO train iter: 3675, num_batches_per_epoch: 49
2022-08-04 18:20:27,221 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 95.264668, lr: 0.100000, avg loss: 0.131159
2022-08-04 18:20:28,740 [dl_trainer.py:822] INFO Epoch 75, lr: 0.100000, val loss: 0.495664, val top-1 acc: 86.956092, top-5 acc: 99.495649
2022-08-04 18:20:28,951 [dl_trainer.py:731] WARNING [ 75][ 3680/   49][rank:0] loss: 0.145, average forward (0.009841) and backward (0.019749) time: 0.075096, iotime: 0.007157 
2022-08-04 18:20:29,369 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082516, Speed: 1551.214876 images/s
2022-08-04 18:20:29,683 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:29,683 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:30,941 [dl_trainer.py:731] WARNING [ 75][ 3720/   49][rank:0] loss: 0.238, average forward (0.009482) and backward (0.020778) time: 0.037502, iotime: 0.006963 
2022-08-04 18:20:31,128 [dl_trainer.py:634] INFO train iter: 3724, num_batches_per_epoch: 49
2022-08-04 18:20:31,129 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 94.945791, lr: 0.100000, avg loss: 0.136132
2022-08-04 18:20:32,666 [dl_trainer.py:822] INFO Epoch 76, lr: 0.100000, val loss: 0.480556, val top-1 acc: 87.302215, top-5 acc: 99.614320
2022-08-04 18:20:33,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082537, Speed: 1550.814026 images/s
2022-08-04 18:20:33,668 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:33,669 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:34,614 [dl_trainer.py:731] WARNING [ 76][ 3760/   49][rank:0] loss: 0.107, average forward (0.010953) and backward (0.022986) time: 0.080851, iotime: 0.007539 
2022-08-04 18:20:35,236 [dl_trainer.py:634] INFO train iter: 3773, num_batches_per_epoch: 49
2022-08-04 18:20:35,236 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 95.280612, lr: 0.100000, avg loss: 0.130212
2022-08-04 18:20:36,754 [dl_trainer.py:822] INFO Epoch 77, lr: 0.100000, val loss: 0.553577, val top-1 acc: 86.431962, top-5 acc: 99.416535
2022-08-04 18:20:37,341 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083478, Speed: 1533.337418 images/s
2022-08-04 18:20:37,688 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:37,688 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:38,205 [dl_trainer.py:731] WARNING [ 77][ 3800/   49][rank:0] loss: 0.066, average forward (0.010717) and backward (0.023239) time: 0.079726, iotime: 0.007414 
2022-08-04 18:20:39,255 [dl_trainer.py:634] INFO train iter: 3822, num_batches_per_epoch: 49
2022-08-04 18:20:39,256 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 95.009566, lr: 0.100000, avg loss: 0.134599
2022-08-04 18:20:40,793 [dl_trainer.py:822] INFO Epoch 78, lr: 0.100000, val loss: 0.525076, val top-1 acc: 85.977057, top-5 acc: 99.505538
2022-08-04 18:20:41,354 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083558, Speed: 1531.877670 images/s
2022-08-04 18:20:41,667 [dl_trainer.py:731] WARNING [ 78][ 3840/   49][rank:0] loss: 0.177, average forward (0.009511) and backward (0.021087) time: 0.072252, iotime: 0.002142 
2022-08-04 18:20:41,676 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:41,676 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:43,389 [dl_trainer.py:634] INFO train iter: 3871, num_batches_per_epoch: 49
2022-08-04 18:20:43,389 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 95.487883, lr: 0.100000, avg loss: 0.129502
2022-08-04 18:20:44,956 [dl_trainer.py:822] INFO Epoch 79, lr: 0.100000, val loss: 0.491267, val top-1 acc: 86.283623, top-5 acc: 99.624209
2022-08-04 18:20:45,371 [dl_trainer.py:731] WARNING [ 79][ 3880/   49][rank:0] loss: 0.115, average forward (0.011272) and backward (0.022115) time: 0.080078, iotime: 0.007118 
2022-08-04 18:20:45,425 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084797, Speed: 1509.491433 images/s
2022-08-04 18:20:45,769 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:45,770 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:47,425 [dl_trainer.py:731] WARNING [ 79][ 3920/   49][rank:0] loss: 0.177, average forward (0.009356) and backward (0.021282) time: 0.037922, iotime: 0.007001 
2022-08-04 18:20:47,437 [dl_trainer.py:634] INFO train iter: 3920, num_batches_per_epoch: 49
2022-08-04 18:20:47,438 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 94.770408, lr: 0.100000, avg loss: 0.130372
2022-08-04 18:20:49,020 [dl_trainer.py:822] INFO Epoch 80, lr: 0.100000, val loss: 0.547646, val top-1 acc: 86.303402, top-5 acc: 99.436313
2022-08-04 18:20:49,468 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084196, Speed: 1520.269576 images/s
2022-08-04 18:20:49,820 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:49,821 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:51,158 [dl_trainer.py:731] WARNING [ 80][ 3960/   49][rank:0] loss: 0.115, average forward (0.010251) and backward (0.020751) time: 0.078297, iotime: 0.006768 
2022-08-04 18:20:51,581 [dl_trainer.py:634] INFO train iter: 3969, num_batches_per_epoch: 49
2022-08-04 18:20:51,581 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 95.934311, lr: 0.100000, avg loss: 0.120496
2022-08-04 18:20:53,176 [dl_trainer.py:822] INFO Epoch 81, lr: 0.100000, val loss: 0.522493, val top-1 acc: 86.619858, top-5 acc: 99.228639
2022-08-04 18:20:53,545 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084900, Speed: 1507.664857 images/s
2022-08-04 18:20:53,890 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:53,891 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:54,792 [dl_trainer.py:731] WARNING [ 81][ 4000/   49][rank:0] loss: 0.111, average forward (0.009767) and backward (0.020022) time: 0.076789, iotime: 0.006776 
2022-08-04 18:20:55,666 [dl_trainer.py:634] INFO train iter: 4018, num_batches_per_epoch: 49
2022-08-04 18:20:55,667 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 96.731505, lr: 0.010000, avg loss: 0.096909
2022-08-04 18:20:57,193 [dl_trainer.py:822] INFO Epoch 82, lr: 0.010000, val loss: 0.359695, val top-1 acc: 89.893196, top-5 acc: 99.693434
2022-08-04 18:20:57,576 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083959, Speed: 1524.552960 images/s
2022-08-04 18:20:57,920 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:20:57,920 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:20:58,492 [dl_trainer.py:731] WARNING [ 82][ 4040/   49][rank:0] loss: 0.095, average forward (0.009564) and backward (0.021494) time: 0.077364, iotime: 0.006631 
2022-08-04 18:20:59,806 [dl_trainer.py:634] INFO train iter: 4067, num_batches_per_epoch: 49
2022-08-04 18:20:59,807 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 97.321429, lr: 0.010000, avg loss: 0.079950
2022-08-04 18:21:01,364 [dl_trainer.py:822] INFO Epoch 83, lr: 0.010000, val loss: 0.360334, val top-1 acc: 90.041535, top-5 acc: 99.732991
2022-08-04 18:21:01,629 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084415, Speed: 1516.309559 images/s
2022-08-04 18:21:01,950 [dl_trainer.py:731] WARNING [ 83][ 4080/   49][rank:0] loss: 0.143, average forward (0.010006) and backward (0.021705) time: 0.073144, iotime: 0.002161 
2022-08-04 18:21:01,965 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:01,965 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:03,820 [dl_trainer.py:634] INFO train iter: 4116, num_batches_per_epoch: 49
2022-08-04 18:21:03,821 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 97.608418, lr: 0.010000, avg loss: 0.073615
2022-08-04 18:21:05,315 [dl_trainer.py:822] INFO Epoch 84, lr: 0.010000, val loss: 0.360873, val top-1 acc: 89.952532, top-5 acc: 99.693434
2022-08-04 18:21:05,536 [dl_trainer.py:731] WARNING [ 84][ 4120/   49][rank:0] loss: 0.090, average forward (0.009906) and backward (0.022022) time: 0.077990, iotime: 0.007322 
2022-08-04 18:21:05,586 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082403, Speed: 1553.340115 images/s
2022-08-04 18:21:05,902 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:05,903 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:07,511 [dl_trainer.py:731] WARNING [ 84][ 4160/   49][rank:0] loss: 0.078, average forward (0.009944) and backward (0.020892) time: 0.038510, iotime: 0.007360 
2022-08-04 18:21:07,771 [dl_trainer.py:634] INFO train iter: 4165, num_batches_per_epoch: 49
2022-08-04 18:21:07,771 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 97.863520, lr: 0.010000, avg loss: 0.063434
2022-08-04 18:21:09,291 [dl_trainer.py:822] INFO Epoch 85, lr: 0.010000, val loss: 0.358955, val top-1 acc: 90.051424, top-5 acc: 99.713212
2022-08-04 18:21:09,503 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081581, Speed: 1568.991032 images/s
2022-08-04 18:21:09,852 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:09,852 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:11,140 [dl_trainer.py:731] WARNING [ 85][ 4200/   49][rank:0] loss: 0.073, average forward (0.010092) and backward (0.022860) time: 0.078395, iotime: 0.007060 
2022-08-04 18:21:11,824 [dl_trainer.py:634] INFO train iter: 4214, num_batches_per_epoch: 49
2022-08-04 18:21:11,824 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 97.863520, lr: 0.010000, avg loss: 0.066579
2022-08-04 18:21:13,337 [dl_trainer.py:822] INFO Epoch 86, lr: 0.010000, val loss: 0.363016, val top-1 acc: 90.071203, top-5 acc: 99.683544
2022-08-04 18:21:13,569 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084686, Speed: 1511.463738 images/s
2022-08-04 18:21:13,890 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:13,890 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:14,758 [dl_trainer.py:731] WARNING [ 86][ 4240/   49][rank:0] loss: 0.051, average forward (0.009806) and backward (0.020270) time: 0.077079, iotime: 0.007191 
2022-08-04 18:21:15,906 [dl_trainer.py:634] INFO train iter: 4263, num_batches_per_epoch: 49
2022-08-04 18:21:15,906 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 98.086735, lr: 0.010000, avg loss: 0.058512
2022-08-04 18:21:17,415 [dl_trainer.py:822] INFO Epoch 87, lr: 0.010000, val loss: 0.370124, val top-1 acc: 90.011867, top-5 acc: 99.693434
2022-08-04 18:21:17,526 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082404, Speed: 1553.326071 images/s
2022-08-04 18:21:17,871 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:17,871 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:18,430 [dl_trainer.py:731] WARNING [ 87][ 4280/   49][rank:0] loss: 0.040, average forward (0.009856) and backward (0.022434) time: 0.077740, iotime: 0.007352 
2022-08-04 18:21:19,922 [dl_trainer.py:634] INFO train iter: 4312, num_batches_per_epoch: 49
2022-08-04 18:21:19,923 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 97.943240, lr: 0.010000, avg loss: 0.060638
2022-08-04 18:21:21,427 [dl_trainer.py:822] INFO Epoch 88, lr: 0.010000, val loss: 0.369621, val top-1 acc: 90.100870, top-5 acc: 99.703323
2022-08-04 18:21:21,537 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083549, Speed: 1532.028211 images/s
2022-08-04 18:21:21,880 [dl_trainer.py:731] WARNING [ 88][ 4320/   49][rank:0] loss: 0.042, average forward (0.009770) and backward (0.021506) time: 0.072905, iotime: 0.002189 
2022-08-04 18:21:21,887 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:21,887 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:23,936 [dl_trainer.py:731] WARNING [ 88][ 4360/   49][rank:0] loss: 0.058, average forward (0.009671) and backward (0.022477) time: 0.039595, iotime: 0.007157 
2022-08-04 18:21:24,012 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051524, Speed: 2484.271585 images/s
2022-08-04 18:21:24,013 [dl_trainer.py:634] INFO train iter: 4361, num_batches_per_epoch: 49
2022-08-04 18:21:24,013 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 98.102679, lr: 0.010000, avg loss: 0.060816
2022-08-04 18:21:25,540 [dl_trainer.py:822] INFO Epoch 89, lr: 0.010000, val loss: 0.375783, val top-1 acc: 90.090981, top-5 acc: 99.663766
2022-08-04 18:21:25,868 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:25,868 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:27,621 [dl_trainer.py:731] WARNING [ 89][ 4400/   49][rank:0] loss: 0.055, average forward (0.010805) and backward (0.024068) time: 0.080513, iotime: 0.007036 
2022-08-04 18:21:28,041 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083909, Speed: 1525.464727 images/s
2022-08-04 18:21:28,094 [dl_trainer.py:634] INFO train iter: 4410, num_batches_per_epoch: 49
2022-08-04 18:21:28,094 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 98.182398, lr: 0.010000, avg loss: 0.054434
2022-08-04 18:21:29,653 [dl_trainer.py:822] INFO Epoch 90, lr: 0.010000, val loss: 0.376562, val top-1 acc: 90.071203, top-5 acc: 99.643987
2022-08-04 18:21:30,010 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:30,010 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:31,342 [dl_trainer.py:731] WARNING [ 90][ 4440/   49][rank:0] loss: 0.031, average forward (0.010591) and backward (0.021319) time: 0.079803, iotime: 0.006919 
2022-08-04 18:21:32,139 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085342, Speed: 1499.846538 images/s
2022-08-04 18:21:32,239 [dl_trainer.py:634] INFO train iter: 4459, num_batches_per_epoch: 49
2022-08-04 18:21:32,240 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 97.927296, lr: 0.010000, avg loss: 0.059151
2022-08-04 18:21:33,765 [dl_trainer.py:822] INFO Epoch 91, lr: 0.010000, val loss: 0.378893, val top-1 acc: 89.843750, top-5 acc: 99.653877
2022-08-04 18:21:34,017 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:34,018 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:34,948 [dl_trainer.py:731] WARNING [ 91][ 4480/   49][rank:0] loss: 0.070, average forward (0.010101) and backward (0.022318) time: 0.077982, iotime: 0.007033 
2022-08-04 18:21:36,129 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083096, Speed: 1540.381635 images/s
2022-08-04 18:21:36,274 [dl_trainer.py:634] INFO train iter: 4508, num_batches_per_epoch: 49
2022-08-04 18:21:36,275 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 97.975128, lr: 0.010000, avg loss: 0.061717
2022-08-04 18:21:37,849 [dl_trainer.py:822] INFO Epoch 92, lr: 0.010000, val loss: 0.378981, val top-1 acc: 89.962421, top-5 acc: 99.663766
2022-08-04 18:21:38,214 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:38,215 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:38,750 [dl_trainer.py:731] WARNING [ 92][ 4520/   49][rank:0] loss: 0.065, average forward (0.010156) and backward (0.021772) time: 0.082946, iotime: 0.007130 
2022-08-04 18:21:40,266 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086152, Speed: 1485.748997 images/s
2022-08-04 18:21:40,463 [dl_trainer.py:634] INFO train iter: 4557, num_batches_per_epoch: 49
2022-08-04 18:21:40,464 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 98.325893, lr: 0.010000, avg loss: 0.053958
2022-08-04 18:21:41,984 [dl_trainer.py:822] INFO Epoch 93, lr: 0.010000, val loss: 0.380634, val top-1 acc: 90.160206, top-5 acc: 99.683544
2022-08-04 18:21:42,145 [dl_trainer.py:731] WARNING [ 93][ 4560/   49][rank:0] loss: 0.060, average forward (0.010646) and backward (0.022257) time: 0.073582, iotime: 0.002304 
2022-08-04 18:21:42,150 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:42,151 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:44,119 [dl_trainer.py:731] WARNING [ 93][ 4600/   49][rank:0] loss: 0.051, average forward (0.009361) and backward (0.020865) time: 0.037424, iotime: 0.006916 
2022-08-04 18:21:44,184 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081595, Speed: 1568.720640 images/s
2022-08-04 18:21:44,409 [dl_trainer.py:634] INFO train iter: 4606, num_batches_per_epoch: 49
2022-08-04 18:21:44,409 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 98.421556, lr: 0.010000, avg loss: 0.051933
2022-08-04 18:21:45,963 [dl_trainer.py:822] INFO Epoch 94, lr: 0.010000, val loss: 0.386617, val top-1 acc: 89.912975, top-5 acc: 99.634098
2022-08-04 18:21:46,102 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:46,102 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:47,773 [dl_trainer.py:731] WARNING [ 94][ 4640/   49][rank:0] loss: 0.048, average forward (0.010580) and backward (0.022888) time: 0.080661, iotime: 0.007074 
2022-08-04 18:21:48,237 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084420, Speed: 1516.232120 images/s
2022-08-04 18:21:48,534 [dl_trainer.py:634] INFO train iter: 4655, num_batches_per_epoch: 49
2022-08-04 18:21:48,534 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 98.389668, lr: 0.010000, avg loss: 0.052314
2022-08-04 18:21:50,060 [dl_trainer.py:822] INFO Epoch 95, lr: 0.010000, val loss: 0.380684, val top-1 acc: 89.972310, top-5 acc: 99.683544
2022-08-04 18:21:50,109 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:50,109 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:51,400 [dl_trainer.py:731] WARNING [ 95][ 4680/   49][rank:0] loss: 0.031, average forward (0.009887) and backward (0.021690) time: 0.077260, iotime: 0.007174 
2022-08-04 18:21:52,215 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082831, Speed: 1545.323309 images/s
2022-08-04 18:21:52,564 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:52,564 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:52,564 [dl_trainer.py:634] INFO train iter: 4704, num_batches_per_epoch: 49
2022-08-04 18:21:52,565 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 98.246173, lr: 0.010000, avg loss: 0.052792
2022-08-04 18:21:54,206 [dl_trainer.py:822] INFO Epoch 96, lr: 0.010000, val loss: 0.383743, val top-1 acc: 90.061313, top-5 acc: 99.653877
2022-08-04 18:21:55,224 [dl_trainer.py:731] WARNING [ 96][ 4720/   49][rank:0] loss: 0.030, average forward (0.009244) and backward (0.023694) time: 0.083288, iotime: 0.007438 
2022-08-04 18:21:56,456 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088319, Speed: 1449.286741 images/s
2022-08-04 18:21:56,799 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:21:56,800 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:21:57,055 [dl_trainer.py:634] INFO train iter: 4753, num_batches_per_epoch: 49
2022-08-04 18:21:57,056 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 98.628827, lr: 0.010000, avg loss: 0.046888
2022-08-04 18:21:58,572 [dl_trainer.py:822] INFO Epoch 97, lr: 0.010000, val loss: 0.382271, val top-1 acc: 90.179984, top-5 acc: 99.703323
2022-08-04 18:21:58,927 [dl_trainer.py:731] WARNING [ 97][ 4760/   49][rank:0] loss: 0.035, average forward (0.011457) and backward (0.023560) time: 0.080494, iotime: 0.007141 
2022-08-04 18:22:00,475 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083681, Speed: 1529.609856 images/s
2022-08-04 18:22:00,780 [dl_trainer.py:731] WARNING [ 97][ 4800/   49][rank:0] loss: 0.041, average forward (0.009854) and backward (0.020912) time: 0.033308, iotime: 0.002240 
2022-08-04 18:22:00,792 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:00,792 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:01,070 [dl_trainer.py:634] INFO train iter: 4802, num_batches_per_epoch: 49
2022-08-04 18:22:01,071 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 98.437500, lr: 0.010000, avg loss: 0.048166
2022-08-04 18:22:02,622 [dl_trainer.py:822] INFO Epoch 98, lr: 0.010000, val loss: 0.389506, val top-1 acc: 90.011867, top-5 acc: 99.643987
2022-08-04 18:22:04,456 [dl_trainer.py:731] WARNING [ 98][ 4840/   49][rank:0] loss: 0.049, average forward (0.010158) and backward (0.022176) time: 0.079323, iotime: 0.007158 
2022-08-04 18:22:04,499 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083815, Speed: 1527.171249 images/s
2022-08-04 18:22:04,812 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:04,812 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:05,142 [dl_trainer.py:634] INFO train iter: 4851, num_batches_per_epoch: 49
2022-08-04 18:22:05,143 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 98.469388, lr: 0.010000, avg loss: 0.047233
2022-08-04 18:22:06,669 [dl_trainer.py:822] INFO Epoch 99, lr: 0.010000, val loss: 0.398986, val top-1 acc: 89.734968, top-5 acc: 99.693434
2022-08-04 18:22:07,965 [dl_trainer.py:731] WARNING [ 99][ 4880/   49][rank:0] loss: 0.059, average forward (0.010464) and backward (0.020433) time: 0.076510, iotime: 0.007059 
2022-08-04 18:22:08,411 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081480, Speed: 1570.940212 images/s
2022-08-04 18:22:08,741 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:08,741 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:09,121 [dl_trainer.py:634] INFO train iter: 4900, num_batches_per_epoch: 49
2022-08-04 18:22:09,121 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 98.070791, lr: 0.010000, avg loss: 0.055744
2022-08-04 18:22:10,677 [dl_trainer.py:822] INFO Epoch 100, lr: 0.010000, val loss: 0.397548, val top-1 acc: 90.051424, top-5 acc: 99.673655
2022-08-04 18:22:11,772 [dl_trainer.py:731] WARNING [100][ 4920/   49][rank:0] loss: 0.027, average forward (0.011069) and backward (0.023174) time: 0.085051, iotime: 0.007630 
2022-08-04 18:22:12,594 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087104, Speed: 1469.504486 images/s
2022-08-04 18:22:12,920 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:12,920 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:13,387 [dl_trainer.py:634] INFO train iter: 4949, num_batches_per_epoch: 49
2022-08-04 18:22:13,388 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 98.533163, lr: 0.010000, avg loss: 0.050045
2022-08-04 18:22:14,961 [dl_trainer.py:822] INFO Epoch 101, lr: 0.010000, val loss: 0.395310, val top-1 acc: 90.071203, top-5 acc: 99.683544
2022-08-04 18:22:15,423 [dl_trainer.py:731] WARNING [101][ 4960/   49][rank:0] loss: 0.050, average forward (0.010983) and backward (0.022596) time: 0.080927, iotime: 0.007599 
2022-08-04 18:22:16,691 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085341, Speed: 1499.871854 images/s
2022-08-04 18:22:17,051 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:17,051 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:17,497 [dl_trainer.py:634] INFO train iter: 4998, num_batches_per_epoch: 49
2022-08-04 18:22:17,498 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 98.692602, lr: 0.010000, avg loss: 0.044875
2022-08-04 18:22:19,050 [dl_trainer.py:822] INFO Epoch 102, lr: 0.010000, val loss: 0.397752, val top-1 acc: 89.912975, top-5 acc: 99.713212
2022-08-04 18:22:19,174 [dl_trainer.py:731] WARNING [102][ 5000/   49][rank:0] loss: 0.031, average forward (0.009722) and backward (0.019686) time: 0.076898, iotime: 0.007536 
2022-08-04 18:22:20,736 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084254, Speed: 1519.216124 images/s
2022-08-04 18:22:21,057 [dl_trainer.py:731] WARNING [102][ 5040/   49][rank:0] loss: 0.031, average forward (0.008475) and backward (0.019178) time: 0.029876, iotime: 0.001993 
2022-08-04 18:22:21,069 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:21,069 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:21,578 [dl_trainer.py:634] INFO train iter: 5047, num_batches_per_epoch: 49
2022-08-04 18:22:21,579 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 98.565051, lr: 0.010000, avg loss: 0.045325
2022-08-04 18:22:23,132 [dl_trainer.py:822] INFO Epoch 103, lr: 0.010000, val loss: 0.389901, val top-1 acc: 90.031646, top-5 acc: 99.683544
2022-08-04 18:22:24,699 [dl_trainer.py:731] WARNING [103][ 5080/   49][rank:0] loss: 0.051, average forward (0.009969) and backward (0.021632) time: 0.077837, iotime: 0.007034 
2022-08-04 18:22:24,767 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083944, Speed: 1524.819889 images/s
2022-08-04 18:22:25,118 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:25,119 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:25,694 [dl_trainer.py:634] INFO train iter: 5096, num_batches_per_epoch: 49
2022-08-04 18:22:25,695 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 98.644770, lr: 0.010000, avg loss: 0.044355
2022-08-04 18:22:27,259 [dl_trainer.py:822] INFO Epoch 104, lr: 0.010000, val loss: 0.389190, val top-1 acc: 90.071203, top-5 acc: 99.693434
2022-08-04 18:22:28,439 [dl_trainer.py:731] WARNING [104][ 5120/   49][rank:0] loss: 0.018, average forward (0.010285) and backward (0.022335) time: 0.079674, iotime: 0.006631 
2022-08-04 18:22:28,862 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085293, Speed: 1500.713865 images/s
2022-08-04 18:22:29,214 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:29,214 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:29,819 [dl_trainer.py:634] INFO train iter: 5145, num_batches_per_epoch: 49
2022-08-04 18:22:29,820 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 98.724490, lr: 0.010000, avg loss: 0.043136
2022-08-04 18:22:31,486 [dl_trainer.py:822] INFO Epoch 105, lr: 0.010000, val loss: 0.401003, val top-1 acc: 89.893196, top-5 acc: 99.713212
2022-08-04 18:22:32,212 [dl_trainer.py:731] WARNING [105][ 5160/   49][rank:0] loss: 0.047, average forward (0.010064) and backward (0.020171) time: 0.079296, iotime: 0.007027 
2022-08-04 18:22:33,023 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086652, Speed: 1477.166019 images/s
2022-08-04 18:22:33,378 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:33,378 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:34,013 [dl_trainer.py:634] INFO train iter: 5194, num_batches_per_epoch: 49
2022-08-04 18:22:34,013 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 98.676658, lr: 0.010000, avg loss: 0.042485
2022-08-04 18:22:35,567 [dl_trainer.py:822] INFO Epoch 106, lr: 0.010000, val loss: 0.401427, val top-1 acc: 90.081092, top-5 acc: 99.703323
2022-08-04 18:22:35,882 [dl_trainer.py:731] WARNING [106][ 5200/   49][rank:0] loss: 0.034, average forward (0.010005) and backward (0.022147) time: 0.079333, iotime: 0.007129 
2022-08-04 18:22:37,069 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084282, Speed: 1518.714917 images/s
2022-08-04 18:22:37,421 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:37,422 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:37,957 [dl_trainer.py:731] WARNING [106][ 5240/   49][rank:0] loss: 0.025, average forward (0.010043) and backward (0.023742) time: 0.040756, iotime: 0.006661 
2022-08-04 18:22:38,110 [dl_trainer.py:634] INFO train iter: 5243, num_batches_per_epoch: 49
2022-08-04 18:22:38,110 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 98.915816, lr: 0.010000, avg loss: 0.040324
2022-08-04 18:22:39,685 [dl_trainer.py:822] INFO Epoch 107, lr: 0.010000, val loss: 0.401619, val top-1 acc: 90.100870, top-5 acc: 99.703323
2022-08-04 18:22:41,138 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084736, Speed: 1510.565615 images/s
2022-08-04 18:22:41,451 [dl_trainer.py:731] WARNING [107][ 5280/   49][rank:0] loss: 0.073, average forward (0.009961) and backward (0.022606) time: 0.074579, iotime: 0.002276 
2022-08-04 18:22:41,464 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:41,464 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:42,210 [dl_trainer.py:634] INFO train iter: 5292, num_batches_per_epoch: 49
2022-08-04 18:22:42,211 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 98.772321, lr: 0.010000, avg loss: 0.038198
2022-08-04 18:22:43,769 [dl_trainer.py:822] INFO Epoch 108, lr: 0.010000, val loss: 0.404780, val top-1 acc: 90.041535, top-5 acc: 99.713212
2022-08-04 18:22:45,181 [dl_trainer.py:731] WARNING [108][ 5320/   49][rank:0] loss: 0.034, average forward (0.008786) and backward (0.020742) time: 0.076733, iotime: 0.006727 
2022-08-04 18:22:45,245 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085533, Speed: 1496.495681 images/s
2022-08-04 18:22:45,596 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:45,596 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:46,380 [dl_trainer.py:634] INFO train iter: 5341, num_batches_per_epoch: 49
2022-08-04 18:22:46,381 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 98.788265, lr: 0.010000, avg loss: 0.040099
2022-08-04 18:22:47,901 [dl_trainer.py:822] INFO Epoch 109, lr: 0.010000, val loss: 0.408005, val top-1 acc: 90.239320, top-5 acc: 99.663766
2022-08-04 18:22:48,782 [dl_trainer.py:731] WARNING [109][ 5360/   49][rank:0] loss: 0.032, average forward (0.009856) and backward (0.022092) time: 0.077187, iotime: 0.006828 
2022-08-04 18:22:49,243 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083278, Speed: 1537.013333 images/s
2022-08-04 18:22:49,565 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:49,566 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:50,414 [dl_trainer.py:634] INFO train iter: 5390, num_batches_per_epoch: 49
2022-08-04 18:22:50,414 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 98.979592, lr: 0.010000, avg loss: 0.038503
2022-08-04 18:22:51,970 [dl_trainer.py:822] INFO Epoch 110, lr: 0.010000, val loss: 0.396957, val top-1 acc: 90.061313, top-5 acc: 99.713212
2022-08-04 18:22:52,485 [dl_trainer.py:731] WARNING [110][ 5400/   49][rank:0] loss: 0.027, average forward (0.010643) and backward (0.023483) time: 0.081453, iotime: 0.007157 
2022-08-04 18:22:53,306 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084609, Speed: 1512.833663 images/s
2022-08-04 18:22:53,666 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:53,666 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:54,560 [dl_trainer.py:634] INFO train iter: 5439, num_batches_per_epoch: 49
2022-08-04 18:22:54,561 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 98.756378, lr: 0.010000, avg loss: 0.043652
2022-08-04 18:22:56,119 [dl_trainer.py:822] INFO Epoch 111, lr: 0.010000, val loss: 0.404288, val top-1 acc: 90.239320, top-5 acc: 99.693434
2022-08-04 18:22:56,169 [dl_trainer.py:731] WARNING [111][ 5440/   49][rank:0] loss: 0.037, average forward (0.011249) and backward (0.022525) time: 0.080277, iotime: 0.007160 
2022-08-04 18:22:57,360 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084414, Speed: 1516.337664 images/s
2022-08-04 18:22:57,704 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:22:57,705 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:22:58,229 [dl_trainer.py:731] WARNING [111][ 5480/   49][rank:0] loss: 0.064, average forward (0.009948) and backward (0.021905) time: 0.039248, iotime: 0.007093 
2022-08-04 18:22:58,573 [dl_trainer.py:634] INFO train iter: 5488, num_batches_per_epoch: 49
2022-08-04 18:22:58,574 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 98.660714, lr: 0.010000, avg loss: 0.041240
2022-08-04 18:23:00,098 [dl_trainer.py:822] INFO Epoch 112, lr: 0.010000, val loss: 0.410061, val top-1 acc: 90.209652, top-5 acc: 99.683544
2022-08-04 18:23:01,346 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083021, Speed: 1541.771575 images/s
2022-08-04 18:23:01,665 [dl_trainer.py:731] WARNING [112][ 5520/   49][rank:0] loss: 0.026, average forward (0.009914) and backward (0.023785) time: 0.075278, iotime: 0.002181 
2022-08-04 18:23:01,672 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:01,672 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:02,673 [dl_trainer.py:634] INFO train iter: 5537, num_batches_per_epoch: 49
2022-08-04 18:23:02,673 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 98.852041, lr: 0.010000, avg loss: 0.039568
2022-08-04 18:23:04,247 [dl_trainer.py:822] INFO Epoch 113, lr: 0.010000, val loss: 0.403844, val top-1 acc: 90.170095, top-5 acc: 99.713212
2022-08-04 18:23:05,286 [dl_trainer.py:731] WARNING [113][ 5560/   49][rank:0] loss: 0.078, average forward (0.009137) and backward (0.022105) time: 0.077849, iotime: 0.006924 
2022-08-04 18:23:05,344 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083256, Speed: 1537.427351 images/s
2022-08-04 18:23:05,664 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:05,664 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:06,719 [dl_trainer.py:634] INFO train iter: 5586, num_batches_per_epoch: 49
2022-08-04 18:23:06,720 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 98.628827, lr: 0.010000, avg loss: 0.043360
2022-08-04 18:23:08,276 [dl_trainer.py:822] INFO Epoch 114, lr: 0.010000, val loss: 0.401229, val top-1 acc: 89.972310, top-5 acc: 99.683544
2022-08-04 18:23:09,033 [dl_trainer.py:731] WARNING [114][ 5600/   49][rank:0] loss: 0.034, average forward (0.009737) and backward (0.021502) time: 0.078829, iotime: 0.007017 
2022-08-04 18:23:09,481 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086165, Speed: 1485.518692 images/s
2022-08-04 18:23:09,823 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:09,824 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:10,877 [dl_trainer.py:634] INFO train iter: 5635, num_batches_per_epoch: 49
2022-08-04 18:23:10,878 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 98.947704, lr: 0.010000, avg loss: 0.035919
2022-08-04 18:23:12,393 [dl_trainer.py:822] INFO Epoch 115, lr: 0.010000, val loss: 0.403207, val top-1 acc: 89.952532, top-5 acc: 99.663766
2022-08-04 18:23:12,635 [dl_trainer.py:731] WARNING [115][ 5640/   49][rank:0] loss: 0.071, average forward (0.010644) and backward (0.022295) time: 0.078222, iotime: 0.007043 
2022-08-04 18:23:13,385 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081312, Speed: 1574.185205 images/s
2022-08-04 18:23:13,717 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:13,717 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:14,695 [dl_trainer.py:731] WARNING [115][ 5680/   49][rank:0] loss: 0.063, average forward (0.009428) and backward (0.021858) time: 0.038870, iotime: 0.007306 
2022-08-04 18:23:14,901 [dl_trainer.py:634] INFO train iter: 5684, num_batches_per_epoch: 49
2022-08-04 18:23:14,901 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 98.262117, lr: 0.010000, avg loss: 0.044931
2022-08-04 18:23:16,436 [dl_trainer.py:822] INFO Epoch 116, lr: 0.010000, val loss: 0.416213, val top-1 acc: 90.130538, top-5 acc: 99.673655
2022-08-04 18:23:17,420 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084039, Speed: 1523.093589 images/s
2022-08-04 18:23:17,757 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:17,757 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:18,318 [dl_trainer.py:731] WARNING [116][ 5720/   49][rank:0] loss: 0.028, average forward (0.009123) and backward (0.022379) time: 0.078277, iotime: 0.007174 
2022-08-04 18:23:18,980 [dl_trainer.py:634] INFO train iter: 5733, num_batches_per_epoch: 49
2022-08-04 18:23:18,981 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 98.740434, lr: 0.010000, avg loss: 0.040397
2022-08-04 18:23:20,572 [dl_trainer.py:822] INFO Epoch 117, lr: 0.010000, val loss: 0.404011, val top-1 acc: 90.041535, top-5 acc: 99.693434
2022-08-04 18:23:21,578 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086601, Speed: 1478.049438 images/s
2022-08-04 18:23:21,908 [dl_trainer.py:731] WARNING [117][ 5760/   49][rank:0] loss: 0.028, average forward (0.009755) and backward (0.023362) time: 0.075514, iotime: 0.002269 
2022-08-04 18:23:21,916 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:21,917 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:23,092 [dl_trainer.py:634] INFO train iter: 5782, num_batches_per_epoch: 49
2022-08-04 18:23:23,092 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 98.899872, lr: 0.010000, avg loss: 0.039309
2022-08-04 18:23:24,649 [dl_trainer.py:822] INFO Epoch 118, lr: 0.010000, val loss: 0.415608, val top-1 acc: 90.100870, top-5 acc: 99.663766
2022-08-04 18:23:25,500 [dl_trainer.py:731] WARNING [118][ 5800/   49][rank:0] loss: 0.024, average forward (0.009429) and backward (0.021298) time: 0.077993, iotime: 0.006868 
2022-08-04 18:23:25,552 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082766, Speed: 1546.527442 images/s
2022-08-04 18:23:25,871 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:25,872 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:27,197 [dl_trainer.py:634] INFO train iter: 5831, num_batches_per_epoch: 49
2022-08-04 18:23:27,198 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 99.043367, lr: 0.010000, avg loss: 0.035629
2022-08-04 18:23:28,793 [dl_trainer.py:822] INFO Epoch 119, lr: 0.010000, val loss: 0.409269, val top-1 acc: 89.952532, top-5 acc: 99.673655
2022-08-04 18:23:29,217 [dl_trainer.py:731] WARNING [119][ 5840/   49][rank:0] loss: 0.053, average forward (0.009188) and backward (0.020830) time: 0.077672, iotime: 0.007418 
2022-08-04 18:23:29,660 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085553, Speed: 1496.145453 images/s
2022-08-04 18:23:30,006 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:30,006 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:31,329 [dl_trainer.py:731] WARNING [119][ 5880/   49][rank:0] loss: 0.032, average forward (0.008773) and backward (0.020268) time: 0.036246, iotime: 0.006925 
2022-08-04 18:23:31,340 [dl_trainer.py:634] INFO train iter: 5880, num_batches_per_epoch: 49
2022-08-04 18:23:31,340 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 98.565051, lr: 0.010000, avg loss: 0.042955
2022-08-04 18:23:32,916 [dl_trainer.py:822] INFO Epoch 120, lr: 0.010000, val loss: 0.413659, val top-1 acc: 90.150316, top-5 acc: 99.683544
2022-08-04 18:23:33,741 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084986, Speed: 1506.125998 images/s
2022-08-04 18:23:34,061 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:34,061 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:35,048 [dl_trainer.py:731] WARNING [120][ 5920/   49][rank:0] loss: 0.064, average forward (0.011202) and backward (0.023465) time: 0.082786, iotime: 0.007457 
2022-08-04 18:23:35,499 [dl_trainer.py:634] INFO train iter: 5929, num_batches_per_epoch: 49
2022-08-04 18:23:35,500 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 98.804209, lr: 0.010000, avg loss: 0.038009
2022-08-04 18:23:37,030 [dl_trainer.py:822] INFO Epoch 121, lr: 0.010000, val loss: 0.408843, val top-1 acc: 90.090981, top-5 acc: 99.683544
2022-08-04 18:23:37,826 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085086, Speed: 1504.355412 images/s
2022-08-04 18:23:38,178 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:38,178 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:38,767 [dl_trainer.py:731] WARNING [121][ 5960/   49][rank:0] loss: 0.066, average forward (0.009960) and backward (0.023575) time: 0.079080, iotime: 0.006913 
2022-08-04 18:23:39,674 [dl_trainer.py:634] INFO train iter: 5978, num_batches_per_epoch: 49
2022-08-04 18:23:39,674 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 98.867985, lr: 0.010000, avg loss: 0.036534
2022-08-04 18:23:41,216 [dl_trainer.py:822] INFO Epoch 122, lr: 0.010000, val loss: 0.417877, val top-1 acc: 89.952532, top-5 acc: 99.683544
2022-08-04 18:23:41,972 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086346, Speed: 1482.413096 images/s
2022-08-04 18:23:42,301 [dl_trainer.py:731] WARNING [122][ 6000/   49][rank:0] loss: 0.036, average forward (0.009401) and backward (0.022928) time: 0.074281, iotime: 0.002193 
2022-08-04 18:23:42,313 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:42,314 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:43,762 [dl_trainer.py:634] INFO train iter: 6027, num_batches_per_epoch: 49
2022-08-04 18:23:43,763 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 98.565051, lr: 0.001000, avg loss: 0.041377
2022-08-04 18:23:45,351 [dl_trainer.py:822] INFO Epoch 123, lr: 0.001000, val loss: 0.411567, val top-1 acc: 90.081092, top-5 acc: 99.653877
2022-08-04 18:23:45,997 [dl_trainer.py:731] WARNING [123][ 6040/   49][rank:0] loss: 0.054, average forward (0.009993) and backward (0.020623) time: 0.077599, iotime: 0.006893 
2022-08-04 18:23:46,043 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084791, Speed: 1509.585429 images/s
2022-08-04 18:23:46,373 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:46,373 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:47,840 [dl_trainer.py:634] INFO train iter: 6076, num_batches_per_epoch: 49
2022-08-04 18:23:47,841 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 98.867985, lr: 0.001000, avg loss: 0.037207
2022-08-04 18:23:49,453 [dl_trainer.py:822] INFO Epoch 124, lr: 0.001000, val loss: 0.414168, val top-1 acc: 90.021756, top-5 acc: 99.673655
2022-08-04 18:23:49,656 [dl_trainer.py:731] WARNING [124][ 6080/   49][rank:0] loss: 0.023, average forward (0.009922) and backward (0.021702) time: 0.079965, iotime: 0.007175 
2022-08-04 18:23:50,146 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085455, Speed: 1497.872550 images/s
2022-08-04 18:23:50,483 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:50,483 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:51,797 [dl_trainer.py:731] WARNING [124][ 6120/   49][rank:0] loss: 0.041, average forward (0.009536) and backward (0.024030) time: 0.040574, iotime: 0.006714 
2022-08-04 18:23:52,042 [dl_trainer.py:634] INFO train iter: 6125, num_batches_per_epoch: 49
2022-08-04 18:23:52,042 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 98.883929, lr: 0.001000, avg loss: 0.037728
2022-08-04 18:23:53,616 [dl_trainer.py:822] INFO Epoch 125, lr: 0.001000, val loss: 0.412564, val top-1 acc: 89.982199, top-5 acc: 99.663766
2022-08-04 18:23:54,214 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084716, Speed: 1510.926791 images/s
2022-08-04 18:23:54,554 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:54,555 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:55,440 [dl_trainer.py:731] WARNING [125][ 6160/   49][rank:0] loss: 0.026, average forward (0.008809) and backward (0.021025) time: 0.076139, iotime: 0.006651 
2022-08-04 18:23:56,086 [dl_trainer.py:634] INFO train iter: 6174, num_batches_per_epoch: 49
2022-08-04 18:23:56,086 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 98.899872, lr: 0.001000, avg loss: 0.036714
2022-08-04 18:23:57,647 [dl_trainer.py:822] INFO Epoch 126, lr: 0.001000, val loss: 0.411142, val top-1 acc: 90.090981, top-5 acc: 99.653877
2022-08-04 18:23:58,221 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083458, Speed: 1533.701534 images/s
2022-08-04 18:23:58,573 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:23:58,573 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:23:59,125 [dl_trainer.py:731] WARNING [126][ 6200/   49][rank:0] loss: 0.047, average forward (0.010323) and backward (0.022184) time: 0.079541, iotime: 0.007119 
2022-08-04 18:24:00,222 [dl_trainer.py:634] INFO train iter: 6223, num_batches_per_epoch: 49
2022-08-04 18:24:00,223 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 98.899872, lr: 0.001000, avg loss: 0.036496
2022-08-04 18:24:01,806 [dl_trainer.py:822] INFO Epoch 127, lr: 0.001000, val loss: 0.408374, val top-1 acc: 90.061313, top-5 acc: 99.673655
2022-08-04 18:24:02,271 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084351, Speed: 1517.477283 images/s
2022-08-04 18:24:02,617 [dl_trainer.py:731] WARNING [127][ 6240/   49][rank:0] loss: 0.015, average forward (0.010332) and backward (0.022456) time: 0.075080, iotime: 0.002359 
2022-08-04 18:24:02,631 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:02,631 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:04,360 [dl_trainer.py:634] INFO train iter: 6272, num_batches_per_epoch: 49
2022-08-04 18:24:04,360 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 98.995536, lr: 0.001000, avg loss: 0.035901
2022-08-04 18:24:05,857 [dl_trainer.py:822] INFO Epoch 128, lr: 0.001000, val loss: 0.413455, val top-1 acc: 89.932753, top-5 acc: 99.683544
2022-08-04 18:24:06,266 [dl_trainer.py:731] WARNING [128][ 6280/   49][rank:0] loss: 0.039, average forward (0.010625) and backward (0.020823) time: 0.077195, iotime: 0.007215 
2022-08-04 18:24:06,339 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084728, Speed: 1510.723066 images/s
2022-08-04 18:24:06,688 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:06,689 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:08,397 [dl_trainer.py:731] WARNING [128][ 6320/   49][rank:0] loss: 0.028, average forward (0.008423) and backward (0.022106) time: 0.037562, iotime: 0.006792 
2022-08-04 18:24:08,449 [dl_trainer.py:634] INFO train iter: 6321, num_batches_per_epoch: 49
2022-08-04 18:24:08,450 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 99.011480, lr: 0.001000, avg loss: 0.034741
2022-08-04 18:24:09,968 [dl_trainer.py:822] INFO Epoch 129, lr: 0.001000, val loss: 0.409517, val top-1 acc: 90.090981, top-5 acc: 99.673655
2022-08-04 18:24:10,352 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083581, Speed: 1531.455077 images/s
2022-08-04 18:24:10,687 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:10,688 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:12,008 [dl_trainer.py:731] WARNING [129][ 6360/   49][rank:0] loss: 0.037, average forward (0.010095) and backward (0.023558) time: 0.078801, iotime: 0.006840 
2022-08-04 18:24:12,519 [dl_trainer.py:634] INFO train iter: 6370, num_batches_per_epoch: 49
2022-08-04 18:24:12,520 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 98.485332, lr: 0.001000, avg loss: 0.042943
2022-08-04 18:24:14,048 [dl_trainer.py:822] INFO Epoch 130, lr: 0.001000, val loss: 0.409818, val top-1 acc: 90.189873, top-5 acc: 99.663766
2022-08-04 18:24:14,392 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084157, Speed: 1520.958237 images/s
2022-08-04 18:24:14,729 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:14,729 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:15,667 [dl_trainer.py:731] WARNING [130][ 6400/   49][rank:0] loss: 0.017, average forward (0.010450) and backward (0.023376) time: 0.080164, iotime: 0.007070 
2022-08-04 18:24:16,578 [dl_trainer.py:634] INFO train iter: 6419, num_batches_per_epoch: 49
2022-08-04 18:24:16,579 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 99.107143, lr: 0.001000, avg loss: 0.032043
2022-08-04 18:24:18,105 [dl_trainer.py:822] INFO Epoch 131, lr: 0.001000, val loss: 0.409555, val top-1 acc: 90.041535, top-5 acc: 99.663766
2022-08-04 18:24:18,422 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083931, Speed: 1525.067776 images/s
2022-08-04 18:24:18,761 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:18,761 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:19,288 [dl_trainer.py:731] WARNING [131][ 6440/   49][rank:0] loss: 0.039, average forward (0.009765) and backward (0.021117) time: 0.076162, iotime: 0.006757 
2022-08-04 18:24:20,712 [dl_trainer.py:634] INFO train iter: 6468, num_batches_per_epoch: 49
2022-08-04 18:24:20,713 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 98.788265, lr: 0.001000, avg loss: 0.035916
2022-08-04 18:24:22,243 [dl_trainer.py:822] INFO Epoch 132, lr: 0.001000, val loss: 0.412534, val top-1 acc: 90.021756, top-5 acc: 99.693434
2022-08-04 18:24:22,503 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084987, Speed: 1506.116843 images/s
2022-08-04 18:24:22,829 [dl_trainer.py:731] WARNING [132][ 6480/   49][rank:0] loss: 0.021, average forward (0.009238) and backward (0.019429) time: 0.070178, iotime: 0.002135 
2022-08-04 18:24:22,844 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:22,844 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:24,816 [dl_trainer.py:634] INFO train iter: 6517, num_batches_per_epoch: 49
2022-08-04 18:24:24,817 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 98.867985, lr: 0.001000, avg loss: 0.034067
2022-08-04 18:24:26,310 [dl_trainer.py:822] INFO Epoch 133, lr: 0.001000, val loss: 0.411789, val top-1 acc: 90.130538, top-5 acc: 99.663766
2022-08-04 18:24:26,455 [dl_trainer.py:731] WARNING [133][ 6520/   49][rank:0] loss: 0.016, average forward (0.010845) and backward (0.022607) time: 0.078144, iotime: 0.006993 
2022-08-04 18:24:26,530 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083872, Speed: 1526.137406 images/s
2022-08-04 18:24:26,847 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:26,847 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:28,539 [dl_trainer.py:731] WARNING [133][ 6560/   49][rank:0] loss: 0.041, average forward (0.009111) and backward (0.023297) time: 0.039250, iotime: 0.006563 
2022-08-04 18:24:28,830 [dl_trainer.py:634] INFO train iter: 6566, num_batches_per_epoch: 49
2022-08-04 18:24:28,831 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 98.963648, lr: 0.001000, avg loss: 0.034390
2022-08-04 18:24:30,380 [dl_trainer.py:822] INFO Epoch 134, lr: 0.001000, val loss: 0.410972, val top-1 acc: 90.011867, top-5 acc: 99.653877
2022-08-04 18:24:30,597 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084705, Speed: 1511.130306 images/s
2022-08-04 18:24:30,929 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:30,929 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:32,193 [dl_trainer.py:731] WARNING [134][ 6600/   49][rank:0] loss: 0.016, average forward (0.010690) and backward (0.021147) time: 0.079666, iotime: 0.007074 
2022-08-04 18:24:32,903 [dl_trainer.py:634] INFO train iter: 6615, num_batches_per_epoch: 49
2022-08-04 18:24:32,904 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 98.852041, lr: 0.001000, avg loss: 0.035906
2022-08-04 18:24:34,438 [dl_trainer.py:822] INFO Epoch 135, lr: 0.001000, val loss: 0.412846, val top-1 acc: 90.021756, top-5 acc: 99.673655
2022-08-04 18:24:34,537 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082045, Speed: 1560.111630 images/s
2022-08-04 18:24:34,859 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:34,860 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:35,760 [dl_trainer.py:731] WARNING [135][ 6640/   49][rank:0] loss: 0.017, average forward (0.010498) and backward (0.021546) time: 0.077836, iotime: 0.007018 
2022-08-04 18:24:36,847 [dl_trainer.py:634] INFO train iter: 6664, num_batches_per_epoch: 49
2022-08-04 18:24:36,848 [dl_trainer.py:635] INFO Epoch 136, avg train acc: 99.202806, lr: 0.001000, avg loss: 0.032493
2022-08-04 18:24:38,408 [dl_trainer.py:822] INFO Epoch 136, lr: 0.001000, val loss: 0.411630, val top-1 acc: 90.140427, top-5 acc: 99.683544
2022-08-04 18:24:38,490 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082328, Speed: 1554.764807 images/s
2022-08-04 18:24:38,822 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:38,822 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:39,376 [dl_trainer.py:731] WARNING [136][ 6680/   49][rank:0] loss: 0.028, average forward (0.009280) and backward (0.020182) time: 0.076581, iotime: 0.007041 
2022-08-04 18:24:41,026 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052799, Speed: 2424.268208 images/s
2022-08-04 18:24:41,027 [dl_trainer.py:634] INFO train iter: 6713, num_batches_per_epoch: 49
2022-08-04 18:24:41,027 [dl_trainer.py:635] INFO Epoch 137, avg train acc: 98.995536, lr: 0.001000, avg loss: 0.033746
2022-08-04 18:24:42,639 [dl_trainer.py:822] INFO Epoch 137, lr: 0.001000, val loss: 0.412687, val top-1 acc: 90.031646, top-5 acc: 99.663766
2022-08-04 18:24:42,979 [dl_trainer.py:731] WARNING [137][ 6720/   49][rank:0] loss: 0.028, average forward (0.008813) and backward (0.023225) time: 0.074691, iotime: 0.002045 
2022-08-04 18:24:42,991 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:42,992 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:24:45,104 [dl_trainer.py:731] WARNING [137][ 6760/   49][rank:0] loss: 0.029, average forward (0.008591) and backward (0.019929) time: 0.035333, iotime: 0.006566 
2022-08-04 18:24:45,181 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.086530, Speed: 1479.256769 images/s
2022-08-04 18:24:45,232 [dl_trainer.py:634] INFO train iter: 6762, num_batches_per_epoch: 49
2022-08-04 18:24:45,232 [dl_trainer.py:635] INFO Epoch 138, avg train acc: 98.931760, lr: 0.001000, avg loss: 0.034045
2022-08-04 18:24:46,765 [dl_trainer.py:822] INFO Epoch 138, lr: 0.001000, val loss: 0.414680, val top-1 acc: 89.903085, top-5 acc: 99.683544
2022-08-04 18:24:47,066 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:24:47,067 [distributed_optimizer.py:143] INFO The number of selected gradients: []

2022-08-04 17:58:43,052 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=64, compressor='topk', data_dir='./data', dataset='cifar10', density=1.0, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 17:59:04,259 [dl_trainer.py:254] INFO num_batches_per_epoch: 98
2022-08-04 17:59:04,271 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 17:59:04,273 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 17:59:04,273 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 17:59:04,273 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 17:59:05,017 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 17:59:08,219 [dl_trainer.py:731] WARNING [  0][   40/   98][rank:0] loss: 1.944, average forward (0.039338) and backward (0.024008) time: 0.065430, iotime: 0.001579 
2022-08-04 17:59:08,278 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079485, Speed: 805.184039 images/s
2022-08-04 17:59:10,179 [dl_trainer.py:731] WARNING [  0][   80/   98][rank:0] loss: 1.793, average forward (0.010168) and backward (0.021999) time: 0.034226, iotime: 0.001779 
2022-08-04 17:59:10,238 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048982, Speed: 1306.612115 images/s
2022-08-04 17:59:11,047 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 1078
2022-08-04 17:59:11,047 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:11,244 [dl_trainer.py:634] INFO train iter: 98, num_batches_per_epoch: 98
2022-08-04 17:59:11,244 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 30.851403, lr: 0.020163, avg loss: 1.930105
2022-08-04 17:59:12,885 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020163, val loss: 2.165219, val top-1 acc: 31.439092, top-5 acc: 86.882962
2022-08-04 17:59:13,948 [dl_trainer.py:731] WARNING [  1][  120/   98][rank:0] loss: 1.483, average forward (0.012604) and backward (0.023060) time: 0.083085, iotime: 0.005994 
2022-08-04 17:59:14,823 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080322, Speed: 796.789361 images/s
2022-08-04 17:59:15,897 [dl_trainer.py:731] WARNING [  1][  160/   98][rank:0] loss: 1.524, average forward (0.010693) and backward (0.019669) time: 0.032363, iotime: 0.001730 
2022-08-04 17:59:16,788 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049113, Speed: 1303.107511 images/s
2022-08-04 17:59:17,552 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:17,552 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:17,821 [dl_trainer.py:634] INFO train iter: 196, num_batches_per_epoch: 98
2022-08-04 17:59:17,822 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 50.430485, lr: 0.040122, avg loss: 1.367784
2022-08-04 17:59:19,462 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040122, val loss: 1.635932, val top-1 acc: 45.989252, top-5 acc: 92.097930
2022-08-04 17:59:19,678 [dl_trainer.py:731] WARNING [  2][  200/   98][rank:0] loss: 1.443, average forward (0.010423) and backward (0.020122) time: 0.078782, iotime: 0.006184 
2022-08-04 17:59:21,402 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080926, Speed: 790.843832 images/s
2022-08-04 17:59:21,650 [dl_trainer.py:731] WARNING [  2][  240/   98][rank:0] loss: 1.018, average forward (0.011390) and backward (0.022850) time: 0.036381, iotime: 0.001840 
2022-08-04 17:59:23,376 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049334, Speed: 1297.277061 images/s
2022-08-04 17:59:23,606 [dl_trainer.py:731] WARNING [  2][  280/   98][rank:0] loss: 1.092, average forward (0.010320) and backward (0.022342) time: 0.034706, iotime: 0.001779 
2022-08-04 17:59:24,165 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:24,165 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:24,486 [dl_trainer.py:634] INFO train iter: 294, num_batches_per_epoch: 98
2022-08-04 17:59:24,487 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 61.240434, lr: 0.060082, avg loss: 1.079569
2022-08-04 17:59:26,095 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060082, val loss: 1.895542, val top-1 acc: 46.178344, top-5 acc: 90.505573
2022-08-04 17:59:27,314 [dl_trainer.py:731] WARNING [  3][  320/   98][rank:0] loss: 0.951, average forward (0.011105) and backward (0.020646) time: 0.078591, iotime: 0.006257 
2022-08-04 17:59:27,891 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079185, Speed: 808.237249 images/s
2022-08-04 17:59:29,219 [dl_trainer.py:731] WARNING [  3][  360/   98][rank:0] loss: 0.749, average forward (0.010968) and backward (0.022061) time: 0.035111, iotime: 0.001816 
2022-08-04 17:59:29,788 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047403, Speed: 1350.118339 images/s
2022-08-04 17:59:30,560 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:30,560 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:30,928 [dl_trainer.py:634] INFO train iter: 392, num_batches_per_epoch: 98
2022-08-04 17:59:30,929 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 66.485969, lr: 0.080041, avg loss: 0.937023
2022-08-04 17:59:32,570 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080041, val loss: 1.007089, val top-1 acc: 66.062898, top-5 acc: 97.024283
2022-08-04 17:59:32,975 [dl_trainer.py:731] WARNING [  4][  400/   98][rank:0] loss: 0.757, average forward (0.011095) and backward (0.020695) time: 0.080122, iotime: 0.006271 
2022-08-04 17:59:34,390 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080712, Speed: 792.946625 images/s
2022-08-04 17:59:34,912 [dl_trainer.py:731] WARNING [  4][  440/   98][rank:0] loss: 0.740, average forward (0.011331) and backward (0.020762) time: 0.034218, iotime: 0.001843 
2022-08-04 17:59:36,350 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048981, Speed: 1306.618316 images/s
2022-08-04 17:59:36,889 [dl_trainer.py:731] WARNING [  4][  480/   98][rank:0] loss: 0.786, average forward (0.011775) and backward (0.023505) time: 0.037518, iotime: 0.001937 
2022-08-04 17:59:37,148 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:37,149 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:37,551 [dl_trainer.py:634] INFO train iter: 490, num_batches_per_epoch: 98
2022-08-04 17:59:37,552 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 71.412628, lr: 0.100000, avg loss: 0.814111
2022-08-04 17:59:39,164 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 2.021382, val top-1 acc: 51.482882, top-5 acc: 90.734475
2022-08-04 17:59:40,581 [dl_trainer.py:731] WARNING [  5][  520/   98][rank:0] loss: 0.870, average forward (0.012201) and backward (0.022623) time: 0.081926, iotime: 0.006420 
2022-08-04 17:59:40,881 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079448, Speed: 805.553715 images/s
2022-08-04 17:59:42,456 [dl_trainer.py:731] WARNING [  5][  560/   98][rank:0] loss: 0.459, average forward (0.010257) and backward (0.021637) time: 0.033844, iotime: 0.001674 
2022-08-04 17:59:42,778 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047413, Speed: 1349.833027 images/s
2022-08-04 17:59:43,534 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:43,534 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:43,988 [dl_trainer.py:634] INFO train iter: 588, num_batches_per_epoch: 98
2022-08-04 17:59:43,989 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 74.681122, lr: 0.100000, avg loss: 0.729823
2022-08-04 17:59:45,560 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 1.146609, val top-1 acc: 63.694268, top-5 acc: 96.825239
2022-08-04 17:59:46,165 [dl_trainer.py:731] WARNING [  6][  600/   98][rank:0] loss: 0.670, average forward (0.011699) and backward (0.020886) time: 0.079161, iotime: 0.006280 
2022-08-04 17:59:47,308 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079461, Speed: 805.431591 images/s
2022-08-04 17:59:48,097 [dl_trainer.py:731] WARNING [  6][  640/   98][rank:0] loss: 0.802, average forward (0.008553) and backward (0.020024) time: 0.030316, iotime: 0.001470 
2022-08-04 17:59:49,202 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047336, Speed: 1352.039904 images/s
2022-08-04 17:59:49,969 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:49,970 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:50,176 [dl_trainer.py:731] WARNING [  6][  680/   98][rank:0] loss: 0.718, average forward (0.010270) and backward (0.021279) time: 0.037894, iotime: 0.006072 
2022-08-04 17:59:50,502 [dl_trainer.py:634] INFO train iter: 686, num_batches_per_epoch: 98
2022-08-04 17:59:50,503 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 77.375638, lr: 0.100000, avg loss: 0.639215
2022-08-04 17:59:52,096 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 0.907398, val top-1 acc: 71.367436, top-5 acc: 97.123806
2022-08-04 17:59:53,740 [dl_trainer.py:731] WARNING [  7][  720/   98][rank:0] loss: 0.440, average forward (0.011414) and backward (0.022130) time: 0.075522, iotime: 0.001809 
2022-08-04 17:59:53,748 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079729, Speed: 802.715582 images/s
2022-08-04 17:59:55,633 [dl_trainer.py:731] WARNING [  7][  760/   98][rank:0] loss: 0.666, average forward (0.010723) and backward (0.020798) time: 0.033551, iotime: 0.001766 
2022-08-04 17:59:55,660 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047792, Speed: 1339.146123 images/s
2022-08-04 17:59:56,460 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 17:59:56,460 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 17:59:56,994 [dl_trainer.py:634] INFO train iter: 784, num_batches_per_epoch: 98
2022-08-04 17:59:56,995 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 79.687500, lr: 0.100000, avg loss: 0.579264
2022-08-04 17:59:58,587 [dl_trainer.py:822] INFO Epoch 8, lr: 0.100000, val loss: 0.794882, val top-1 acc: 73.666401, top-5 acc: 98.566879
2022-08-04 17:59:59,380 [dl_trainer.py:731] WARNING [  8][  800/   98][rank:0] loss: 0.472, average forward (0.010995) and backward (0.024177) time: 0.082312, iotime: 0.006525 
2022-08-04 18:00:00,251 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080524, Speed: 794.791279 images/s
2022-08-04 18:00:01,388 [dl_trainer.py:731] WARNING [  8][  840/   98][rank:0] loss: 0.617, average forward (0.008317) and backward (0.024522) time: 0.034589, iotime: 0.001535 
2022-08-04 18:00:02,243 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049783, Speed: 1285.583331 images/s
2022-08-04 18:00:03,036 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:03,036 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:03,528 [dl_trainer.py:731] WARNING [  8][  880/   98][rank:0] loss: 0.534, average forward (0.009703) and backward (0.023329) time: 0.039490, iotime: 0.006214 
2022-08-04 18:00:03,642 [dl_trainer.py:634] INFO train iter: 882, num_batches_per_epoch: 98
2022-08-04 18:00:03,643 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 80.676020, lr: 0.100000, avg loss: 0.544131
2022-08-04 18:00:05,335 [dl_trainer.py:822] INFO Epoch 9, lr: 0.100000, val loss: 0.658462, val top-1 acc: 78.353901, top-5 acc: 98.507166
2022-08-04 18:00:06,900 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081675, Speed: 783.593849 images/s
2022-08-04 18:00:07,172 [dl_trainer.py:731] WARNING [  9][  920/   98][rank:0] loss: 0.362, average forward (0.011494) and backward (0.022219) time: 0.078173, iotime: 0.001845 
2022-08-04 18:00:08,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048671, Speed: 1314.951537 images/s
2022-08-04 18:00:09,120 [dl_trainer.py:731] WARNING [  9][  960/   98][rank:0] loss: 0.411, average forward (0.011159) and backward (0.022981) time: 0.036234, iotime: 0.001829 
2022-08-04 18:00:09,610 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:09,611 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:10,257 [dl_trainer.py:634] INFO train iter: 980, num_batches_per_epoch: 98
2022-08-04 18:00:10,258 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 81.393495, lr: 0.100000, avg loss: 0.512805
2022-08-04 18:00:11,877 [dl_trainer.py:822] INFO Epoch 10, lr: 0.100000, val loss: 0.612712, val top-1 acc: 79.040605, top-5 acc: 98.964968
2022-08-04 18:00:12,926 [dl_trainer.py:731] WARNING [ 10][ 1000/   98][rank:0] loss: 0.493, average forward (0.010329) and backward (0.021031) time: 0.080289, iotime: 0.005963 
2022-08-04 18:00:13,478 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081223, Speed: 787.956479 images/s
2022-08-04 18:00:14,882 [dl_trainer.py:731] WARNING [ 10][ 1040/   98][rank:0] loss: 0.351, average forward (0.010407) and backward (0.022776) time: 0.035304, iotime: 0.001850 
2022-08-04 18:00:15,460 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049534, Speed: 1292.043412 images/s
2022-08-04 18:00:16,261 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:16,261 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:16,954 [dl_trainer.py:634] INFO train iter: 1078, num_batches_per_epoch: 98
2022-08-04 18:00:16,954 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 83.832908, lr: 0.100000, avg loss: 0.466581
2022-08-04 18:00:18,533 [dl_trainer.py:822] INFO Epoch 11, lr: 0.100000, val loss: 0.626522, val top-1 acc: 79.349124, top-5 acc: 98.596736
2022-08-04 18:00:18,629 [dl_trainer.py:731] WARNING [ 11][ 1080/   98][rank:0] loss: 0.483, average forward (0.011799) and backward (0.021814) time: 0.079667, iotime: 0.006235 
2022-08-04 18:00:19,947 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078706, Speed: 813.152779 images/s
2022-08-04 18:00:20,522 [dl_trainer.py:731] WARNING [ 11][ 1120/   98][rank:0] loss: 0.430, average forward (0.011308) and backward (0.022115) time: 0.035506, iotime: 0.001822 
2022-08-04 18:00:21,886 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048464, Speed: 1320.556144 images/s
2022-08-04 18:00:22,463 [dl_trainer.py:731] WARNING [ 11][ 1160/   98][rank:0] loss: 0.307, average forward (0.011630) and backward (0.021649) time: 0.035392, iotime: 0.001838 
2022-08-04 18:00:22,661 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:22,661 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:23,393 [dl_trainer.py:634] INFO train iter: 1176, num_batches_per_epoch: 98
2022-08-04 18:00:23,393 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 83.641582, lr: 0.100000, avg loss: 0.460425
2022-08-04 18:00:24,946 [dl_trainer.py:822] INFO Epoch 12, lr: 0.100000, val loss: 0.560981, val top-1 acc: 81.538615, top-5 acc: 98.964968
2022-08-04 18:00:26,124 [dl_trainer.py:731] WARNING [ 12][ 1200/   98][rank:0] loss: 0.378, average forward (0.012820) and backward (0.022597) time: 0.081772, iotime: 0.006373 
2022-08-04 18:00:26,371 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078649, Speed: 813.738498 images/s
2022-08-04 18:00:28,050 [dl_trainer.py:731] WARNING [ 12][ 1240/   98][rank:0] loss: 0.372, average forward (0.011010) and backward (0.021017) time: 0.034135, iotime: 0.001833 
2022-08-04 18:00:28,335 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049082, Speed: 1303.935778 images/s
2022-08-04 18:00:29,103 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:29,104 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:29,868 [dl_trainer.py:634] INFO train iter: 1274, num_batches_per_epoch: 98
2022-08-04 18:00:29,868 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 84.853316, lr: 0.100000, avg loss: 0.441756
2022-08-04 18:00:31,562 [dl_trainer.py:822] INFO Epoch 13, lr: 0.100000, val loss: 0.561583, val top-1 acc: 81.389331, top-5 acc: 99.183917
2022-08-04 18:00:31,845 [dl_trainer.py:731] WARNING [ 13][ 1280/   98][rank:0] loss: 0.467, average forward (0.012049) and backward (0.023207) time: 0.084167, iotime: 0.006179 
2022-08-04 18:00:32,931 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080611, Speed: 793.933809 images/s
2022-08-04 18:00:33,794 [dl_trainer.py:731] WARNING [ 13][ 1320/   98][rank:0] loss: 0.564, average forward (0.010344) and backward (0.018948) time: 0.031353, iotime: 0.001805 
2022-08-04 18:00:34,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047897, Speed: 1336.209933 images/s
2022-08-04 18:00:35,621 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:35,621 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:35,879 [dl_trainer.py:731] WARNING [ 13][ 1360/   98][rank:0] loss: 0.422, average forward (0.012667) and backward (0.022522) time: 0.041902, iotime: 0.006373 
2022-08-04 18:00:36,464 [dl_trainer.py:634] INFO train iter: 1372, num_batches_per_epoch: 98
2022-08-04 18:00:36,465 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 85.267857, lr: 0.100000, avg loss: 0.436111
2022-08-04 18:00:38,073 [dl_trainer.py:822] INFO Epoch 14, lr: 0.100000, val loss: 0.607399, val top-1 acc: 80.483678, top-5 acc: 98.676354
2022-08-04 18:00:39,438 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080518, Speed: 794.848463 images/s
2022-08-04 18:00:39,474 [dl_trainer.py:731] WARNING [ 14][ 1400/   98][rank:0] loss: 0.377, average forward (0.011763) and backward (0.021457) time: 0.076980, iotime: 0.001882 
2022-08-04 18:00:41,401 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049061, Speed: 1304.503227 images/s
2022-08-04 18:00:41,435 [dl_trainer.py:731] WARNING [ 14][ 1440/   98][rank:0] loss: 0.379, average forward (0.009355) and backward (0.019476) time: 0.030683, iotime: 0.001601 
2022-08-04 18:00:42,181 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:42,182 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:43,051 [dl_trainer.py:634] INFO train iter: 1470, num_batches_per_epoch: 98
2022-08-04 18:00:43,051 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 85.841837, lr: 0.100000, avg loss: 0.408651
2022-08-04 18:00:44,608 [dl_trainer.py:822] INFO Epoch 15, lr: 0.100000, val loss: 0.593386, val top-1 acc: 81.259952, top-5 acc: 98.974920
2022-08-04 18:00:45,095 [dl_trainer.py:731] WARNING [ 15][ 1480/   98][rank:0] loss: 0.177, average forward (0.010943) and backward (0.022170) time: 0.078143, iotime: 0.005752 
2022-08-04 18:00:45,903 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078955, Speed: 810.590308 images/s
2022-08-04 18:00:47,044 [dl_trainer.py:731] WARNING [ 15][ 1520/   98][rank:0] loss: 0.300, average forward (0.010516) and backward (0.020991) time: 0.033478, iotime: 0.001702 
2022-08-04 18:00:47,829 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048152, Speed: 1329.127244 images/s
2022-08-04 18:00:48,581 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:48,581 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:49,111 [dl_trainer.py:731] WARNING [ 15][ 1560/   98][rank:0] loss: 0.384, average forward (0.010537) and backward (0.020459) time: 0.037422, iotime: 0.006174 
2022-08-04 18:00:49,504 [dl_trainer.py:634] INFO train iter: 1568, num_batches_per_epoch: 98
2022-08-04 18:00:49,504 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 85.507015, lr: 0.100000, avg loss: 0.403063
2022-08-04 18:00:51,103 [dl_trainer.py:822] INFO Epoch 16, lr: 0.100000, val loss: 0.586764, val top-1 acc: 80.981290, top-5 acc: 99.044586
2022-08-04 18:00:52,355 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079370, Speed: 806.348382 images/s
2022-08-04 18:00:52,688 [dl_trainer.py:731] WARNING [ 16][ 1600/   98][rank:0] loss: 0.480, average forward (0.010380) and backward (0.020437) time: 0.073478, iotime: 0.001715 
2022-08-04 18:00:54,325 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049249, Speed: 1299.514542 images/s
2022-08-04 18:00:54,665 [dl_trainer.py:731] WARNING [ 16][ 1640/   98][rank:0] loss: 0.249, average forward (0.010761) and backward (0.022990) time: 0.035832, iotime: 0.001819 
2022-08-04 18:00:55,121 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:00:55,121 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:00:56,131 [dl_trainer.py:634] INFO train iter: 1666, num_batches_per_epoch: 98
2022-08-04 18:00:56,132 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 87.021684, lr: 0.100000, avg loss: 0.383850
2022-08-04 18:00:57,763 [dl_trainer.py:822] INFO Epoch 17, lr: 0.100000, val loss: 0.624582, val top-1 acc: 80.593153, top-5 acc: 98.945064
2022-08-04 18:00:58,385 [dl_trainer.py:731] WARNING [ 17][ 1680/   98][rank:0] loss: 0.341, average forward (0.009984) and backward (0.020628) time: 0.077694, iotime: 0.005970 
2022-08-04 18:00:58,879 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079877, Speed: 801.232522 images/s
2022-08-04 18:01:00,333 [dl_trainer.py:731] WARNING [ 17][ 1720/   98][rank:0] loss: 0.345, average forward (0.009243) and backward (0.020907) time: 0.031983, iotime: 0.001586 
2022-08-04 18:01:00,852 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049302, Speed: 1298.123355 images/s
2022-08-04 18:01:01,658 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:01,658 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:02,494 [dl_trainer.py:731] WARNING [ 17][ 1760/   98][rank:0] loss: 0.434, average forward (0.009581) and backward (0.021475) time: 0.037135, iotime: 0.005826 
2022-08-04 18:01:02,699 [dl_trainer.py:634] INFO train iter: 1764, num_batches_per_epoch: 98
2022-08-04 18:01:02,700 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 87.021684, lr: 0.100000, avg loss: 0.366932
2022-08-04 18:01:04,268 [dl_trainer.py:822] INFO Epoch 18, lr: 0.100000, val loss: 0.539171, val top-1 acc: 83.021497, top-5 acc: 99.243631
2022-08-04 18:01:05,474 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081081, Speed: 789.334272 images/s
2022-08-04 18:01:06,089 [dl_trainer.py:731] WARNING [ 18][ 1800/   98][rank:0] loss: 0.365, average forward (0.009163) and backward (0.019964) time: 0.071331, iotime: 0.001604 
2022-08-04 18:01:07,398 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048086, Speed: 1330.935368 images/s
2022-08-04 18:01:08,006 [dl_trainer.py:731] WARNING [ 18][ 1840/   98][rank:0] loss: 0.294, average forward (0.011061) and backward (0.021124) time: 0.034307, iotime: 0.001848 
2022-08-04 18:01:08,158 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:08,158 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:09,211 [dl_trainer.py:634] INFO train iter: 1862, num_batches_per_epoch: 98
2022-08-04 18:01:09,212 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 87.930485, lr: 0.100000, avg loss: 0.351520
2022-08-04 18:01:10,812 [dl_trainer.py:822] INFO Epoch 19, lr: 0.100000, val loss: 0.570558, val top-1 acc: 81.568471, top-5 acc: 98.974920
2022-08-04 18:01:11,662 [dl_trainer.py:731] WARNING [ 19][ 1880/   98][rank:0] loss: 0.451, average forward (0.011616) and backward (0.020049) time: 0.077858, iotime: 0.005832 
2022-08-04 18:01:11,874 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078509, Speed: 815.197199 images/s
2022-08-04 18:01:13,553 [dl_trainer.py:731] WARNING [ 19][ 1920/   98][rank:0] loss: 0.609, average forward (0.010919) and backward (0.020839) time: 0.033827, iotime: 0.001790 
2022-08-04 18:01:13,761 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047138, Speed: 1357.729563 images/s
2022-08-04 18:01:14,539 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:14,539 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:15,663 [dl_trainer.py:731] WARNING [ 19][ 1960/   98][rank:0] loss: 0.356, average forward (0.010883) and backward (0.021646) time: 0.038742, iotime: 0.005908 
2022-08-04 18:01:15,676 [dl_trainer.py:634] INFO train iter: 1960, num_batches_per_epoch: 98
2022-08-04 18:01:15,677 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 88.137755, lr: 0.100000, avg loss: 0.356862
2022-08-04 18:01:17,336 [dl_trainer.py:822] INFO Epoch 20, lr: 0.100000, val loss: 0.539695, val top-1 acc: 82.613455, top-5 acc: 98.964968
2022-08-04 18:01:18,397 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081320, Speed: 787.017652 images/s
2022-08-04 18:01:19,267 [dl_trainer.py:731] WARNING [ 20][ 2000/   98][rank:0] loss: 0.214, average forward (0.010948) and backward (0.019234) time: 0.074839, iotime: 0.001779 
2022-08-04 18:01:20,305 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047698, Speed: 1341.784976 images/s
2022-08-04 18:01:21,072 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:21,072 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:21,380 [dl_trainer.py:731] WARNING [ 20][ 2040/   98][rank:0] loss: 0.416, average forward (0.009664) and backward (0.018674) time: 0.034820, iotime: 0.006235 
2022-08-04 18:01:22,302 [dl_trainer.py:634] INFO train iter: 2058, num_batches_per_epoch: 98
2022-08-04 18:01:22,303 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 88.408801, lr: 0.100000, avg loss: 0.333863
2022-08-04 18:01:23,883 [dl_trainer.py:822] INFO Epoch 21, lr: 0.100000, val loss: 0.484020, val top-1 acc: 83.857484, top-5 acc: 99.402866
2022-08-04 18:01:24,839 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079521, Speed: 804.823051 images/s
2022-08-04 18:01:24,924 [dl_trainer.py:731] WARNING [ 21][ 2080/   98][rank:0] loss: 0.335, average forward (0.011041) and backward (0.023304) time: 0.075989, iotime: 0.001795 
2022-08-04 18:01:26,785 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048628, Speed: 1316.101525 images/s
2022-08-04 18:01:26,867 [dl_trainer.py:731] WARNING [ 21][ 2120/   98][rank:0] loss: 0.391, average forward (0.012218) and backward (0.023586) time: 0.038064, iotime: 0.001970 
2022-08-04 18:01:27,551 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:27,551 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:28,792 [dl_trainer.py:634] INFO train iter: 2156, num_batches_per_epoch: 98
2022-08-04 18:01:28,793 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 88.616071, lr: 0.100000, avg loss: 0.327083
2022-08-04 18:01:30,346 [dl_trainer.py:822] INFO Epoch 22, lr: 0.100000, val loss: 0.488739, val top-1 acc: 84.215764, top-5 acc: 99.183917
2022-08-04 18:01:30,589 [dl_trainer.py:731] WARNING [ 22][ 2160/   98][rank:0] loss: 0.226, average forward (0.011056) and backward (0.022480) time: 0.080341, iotime: 0.006341 
2022-08-04 18:01:31,324 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079623, Speed: 803.792784 images/s
2022-08-04 18:01:32,514 [dl_trainer.py:731] WARNING [ 22][ 2200/   98][rank:0] loss: 0.289, average forward (0.011322) and backward (0.021796) time: 0.035204, iotime: 0.001805 
2022-08-04 18:01:33,266 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048533, Speed: 1318.678106 images/s
2022-08-04 18:01:34,028 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:34,029 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:34,606 [dl_trainer.py:731] WARNING [ 22][ 2240/   98][rank:0] loss: 0.407, average forward (0.010926) and backward (0.021323) time: 0.038957, iotime: 0.006403 
2022-08-04 18:01:35,273 [dl_trainer.py:634] INFO train iter: 2254, num_batches_per_epoch: 98
2022-08-04 18:01:35,274 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 88.233418, lr: 0.100000, avg loss: 0.344782
2022-08-04 18:01:36,862 [dl_trainer.py:822] INFO Epoch 23, lr: 0.100000, val loss: 0.461549, val top-1 acc: 85.071656, top-5 acc: 99.412818
2022-08-04 18:01:37,738 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078419, Speed: 816.133017 images/s
2022-08-04 18:01:38,113 [dl_trainer.py:731] WARNING [ 23][ 2280/   98][rank:0] loss: 0.357, average forward (0.009792) and backward (0.021063) time: 0.072550, iotime: 0.001677 
2022-08-04 18:01:39,652 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047832, Speed: 1338.022053 images/s
2022-08-04 18:01:40,047 [dl_trainer.py:731] WARNING [ 23][ 2320/   98][rank:0] loss: 0.360, average forward (0.011483) and backward (0.021271) time: 0.034859, iotime: 0.001838 
2022-08-04 18:01:40,458 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:40,458 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:41,783 [dl_trainer.py:634] INFO train iter: 2352, num_batches_per_epoch: 98
2022-08-04 18:01:41,784 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 88.998724, lr: 0.100000, avg loss: 0.305688
2022-08-04 18:01:43,411 [dl_trainer.py:822] INFO Epoch 24, lr: 0.100000, val loss: 0.589339, val top-1 acc: 80.752389, top-5 acc: 99.183917
2022-08-04 18:01:43,808 [dl_trainer.py:731] WARNING [ 24][ 2360/   98][rank:0] loss: 0.270, average forward (0.010346) and backward (0.021996) time: 0.079975, iotime: 0.005916 
2022-08-04 18:01:44,270 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081005, Speed: 790.070686 images/s
2022-08-04 18:01:45,754 [dl_trainer.py:731] WARNING [ 24][ 2400/   98][rank:0] loss: 0.232, average forward (0.010841) and backward (0.021898) time: 0.034786, iotime: 0.001784 
2022-08-04 18:01:46,173 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047554, Speed: 1345.827332 images/s
2022-08-04 18:01:46,952 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:46,953 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:47,862 [dl_trainer.py:731] WARNING [ 24][ 2440/   98][rank:0] loss: 0.466, average forward (0.011464) and backward (0.022723) time: 0.040403, iotime: 0.005930 
2022-08-04 18:01:48,372 [dl_trainer.py:634] INFO train iter: 2450, num_batches_per_epoch: 98
2022-08-04 18:01:48,372 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 88.982781, lr: 0.100000, avg loss: 0.307560
2022-08-04 18:01:49,979 [dl_trainer.py:822] INFO Epoch 25, lr: 0.100000, val loss: 0.478832, val top-1 acc: 85.111465, top-5 acc: 99.392914
2022-08-04 18:01:50,764 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080518, Speed: 794.858167 images/s
2022-08-04 18:01:51,400 [dl_trainer.py:731] WARNING [ 25][ 2480/   98][rank:0] loss: 0.445, average forward (0.011174) and backward (0.021337) time: 0.074971, iotime: 0.001932 
2022-08-04 18:01:52,686 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048049, Speed: 1331.978818 images/s
2022-08-04 18:01:53,382 [dl_trainer.py:731] WARNING [ 25][ 2520/   98][rank:0] loss: 0.377, average forward (0.010686) and backward (0.022886) time: 0.035701, iotime: 0.001848 
2022-08-04 18:01:53,496 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:01:53,497 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:01:54,945 [dl_trainer.py:634] INFO train iter: 2548, num_batches_per_epoch: 98
2022-08-04 18:01:54,946 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 89.062500, lr: 0.100000, avg loss: 0.300473
2022-08-04 18:01:56,595 [dl_trainer.py:822] INFO Epoch 26, lr: 0.100000, val loss: 0.631227, val top-1 acc: 81.160430, top-5 acc: 99.064490
2022-08-04 18:01:57,250 [dl_trainer.py:731] WARNING [ 26][ 2560/   98][rank:0] loss: 0.128, average forward (0.009559) and backward (0.019030) time: 0.078538, iotime: 0.005894 
2022-08-04 18:01:57,420 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083018, Speed: 770.912745 images/s
2022-08-04 18:01:59,202 [dl_trainer.py:731] WARNING [ 26][ 2600/   98][rank:0] loss: 0.317, average forward (0.009370) and backward (0.020203) time: 0.031411, iotime: 0.001601 
2022-08-04 18:01:59,369 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048730, Speed: 1313.361312 images/s
2022-08-04 18:02:00,174 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:00,174 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:01,311 [dl_trainer.py:731] WARNING [ 26][ 2640/   98][rank:0] loss: 0.260, average forward (0.009639) and backward (0.020094) time: 0.036035, iotime: 0.006057 
2022-08-04 18:02:01,598 [dl_trainer.py:634] INFO train iter: 2646, num_batches_per_epoch: 98
2022-08-04 18:02:01,599 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 89.971301, lr: 0.100000, avg loss: 0.294174
2022-08-04 18:02:03,257 [dl_trainer.py:822] INFO Epoch 27, lr: 0.100000, val loss: 0.436659, val top-1 acc: 85.708599, top-5 acc: 99.472532
2022-08-04 18:02:03,946 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080271, Speed: 797.297971 images/s
2022-08-04 18:02:04,898 [dl_trainer.py:731] WARNING [ 27][ 2680/   98][rank:0] loss: 0.465, average forward (0.010714) and backward (0.022767) time: 0.077007, iotime: 0.001727 
2022-08-04 18:02:05,852 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047631, Speed: 1343.660681 images/s
2022-08-04 18:02:06,635 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:06,636 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:06,977 [dl_trainer.py:731] WARNING [ 27][ 2720/   98][rank:0] loss: 0.183, average forward (0.010865) and backward (0.021899) time: 0.039089, iotime: 0.006042 
2022-08-04 18:02:08,125 [dl_trainer.py:634] INFO train iter: 2744, num_batches_per_epoch: 98
2022-08-04 18:02:08,126 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 91.007653, lr: 0.100000, avg loss: 0.257110
2022-08-04 18:02:09,765 [dl_trainer.py:822] INFO Epoch 28, lr: 0.100000, val loss: 0.604250, val top-1 acc: 81.747611, top-5 acc: 99.203822
2022-08-04 18:02:10,431 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080313, Speed: 796.878290 images/s
2022-08-04 18:02:10,563 [dl_trainer.py:731] WARNING [ 28][ 2760/   98][rank:0] loss: 0.381, average forward (0.010584) and backward (0.021523) time: 0.076479, iotime: 0.001751 
2022-08-04 18:02:12,360 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048229, Speed: 1327.001525 images/s
2022-08-04 18:02:12,480 [dl_trainer.py:731] WARNING [ 28][ 2800/   98][rank:0] loss: 0.372, average forward (0.011258) and backward (0.021571) time: 0.034906, iotime: 0.001812 
2022-08-04 18:02:13,109 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:13,109 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:14,603 [dl_trainer.py:731] WARNING [ 28][ 2840/   98][rank:0] loss: 0.295, average forward (0.010988) and backward (0.020624) time: 0.038076, iotime: 0.006184 
2022-08-04 18:02:14,702 [dl_trainer.py:634] INFO train iter: 2842, num_batches_per_epoch: 98
2022-08-04 18:02:14,702 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 89.604592, lr: 0.100000, avg loss: 0.296392
2022-08-04 18:02:16,366 [dl_trainer.py:822] INFO Epoch 29, lr: 0.100000, val loss: 0.476657, val top-1 acc: 84.812898, top-5 acc: 99.313296
2022-08-04 18:02:16,944 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080397, Speed: 796.046214 images/s
2022-08-04 18:02:18,130 [dl_trainer.py:731] WARNING [ 29][ 2880/   98][rank:0] loss: 0.317, average forward (0.011628) and backward (0.021193) time: 0.076655, iotime: 0.001892 
2022-08-04 18:02:18,803 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046448, Speed: 1377.898613 images/s
2022-08-04 18:02:19,617 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:19,617 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:20,238 [dl_trainer.py:731] WARNING [ 29][ 2920/   98][rank:0] loss: 0.543, average forward (0.011115) and backward (0.022331) time: 0.039843, iotime: 0.006119 
2022-08-04 18:02:21,198 [dl_trainer.py:634] INFO train iter: 2940, num_batches_per_epoch: 98
2022-08-04 18:02:21,199 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 90.098852, lr: 0.100000, avg loss: 0.278868
2022-08-04 18:02:22,814 [dl_trainer.py:822] INFO Epoch 30, lr: 0.100000, val loss: 0.524022, val top-1 acc: 83.419586, top-5 acc: 99.323248
2022-08-04 18:02:23,367 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080056, Speed: 799.443711 images/s
2022-08-04 18:02:23,761 [dl_trainer.py:731] WARNING [ 30][ 2960/   98][rank:0] loss: 0.149, average forward (0.011179) and backward (0.021722) time: 0.076125, iotime: 0.001827 
2022-08-04 18:02:25,235 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046689, Speed: 1370.758275 images/s
2022-08-04 18:02:25,649 [dl_trainer.py:731] WARNING [ 30][ 3000/   98][rank:0] loss: 0.340, average forward (0.010212) and backward (0.021168) time: 0.033361, iotime: 0.001723 
2022-08-04 18:02:26,012 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:26,012 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:27,683 [dl_trainer.py:634] INFO train iter: 3038, num_batches_per_epoch: 98
2022-08-04 18:02:27,683 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 90.577168, lr: 0.100000, avg loss: 0.267372
2022-08-04 18:02:29,380 [dl_trainer.py:822] INFO Epoch 31, lr: 0.100000, val loss: 0.614739, val top-1 acc: 82.474124, top-5 acc: 98.925159
2022-08-04 18:02:29,485 [dl_trainer.py:731] WARNING [ 31][ 3040/   98][rank:0] loss: 0.327, average forward (0.009418) and backward (0.020520) time: 0.078654, iotime: 0.005982 
2022-08-04 18:02:29,890 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081645, Speed: 783.877147 images/s
2022-08-04 18:02:31,461 [dl_trainer.py:731] WARNING [ 31][ 3080/   98][rank:0] loss: 0.286, average forward (0.009808) and backward (0.020309) time: 0.032067, iotime: 0.001694 
2022-08-04 18:02:31,820 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048231, Speed: 1326.950031 images/s
2022-08-04 18:02:32,574 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:32,574 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:33,454 [dl_trainer.py:731] WARNING [ 31][ 3120/   98][rank:0] loss: 0.210, average forward (0.010462) and backward (0.018576) time: 0.035206, iotime: 0.005882 
2022-08-04 18:02:34,279 [dl_trainer.py:634] INFO train iter: 3136, num_batches_per_epoch: 98
2022-08-04 18:02:34,280 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 90.896046, lr: 0.100000, avg loss: 0.254054
2022-08-04 18:02:36,036 [dl_trainer.py:822] INFO Epoch 32, lr: 0.100000, val loss: 0.513722, val top-1 acc: 83.459395, top-5 acc: 99.472532
2022-08-04 18:02:36,520 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082443, Speed: 776.294014 images/s
2022-08-04 18:02:37,257 [dl_trainer.py:731] WARNING [ 32][ 3160/   98][rank:0] loss: 0.317, average forward (0.011301) and backward (0.022053) time: 0.080378, iotime: 0.001855 
2022-08-04 18:02:38,466 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048641, Speed: 1315.774940 images/s
2022-08-04 18:02:39,166 [dl_trainer.py:731] WARNING [ 32][ 3200/   98][rank:0] loss: 0.354, average forward (0.010937) and backward (0.022809) time: 0.035888, iotime: 0.001866 
2022-08-04 18:02:39,230 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:39,231 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:41,017 [dl_trainer.py:634] INFO train iter: 3234, num_batches_per_epoch: 98
2022-08-04 18:02:41,018 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 90.545281, lr: 0.100000, avg loss: 0.260145
2022-08-04 18:02:42,640 [dl_trainer.py:822] INFO Epoch 33, lr: 0.100000, val loss: 0.494379, val top-1 acc: 84.504379, top-5 acc: 99.572054
2022-08-04 18:02:42,919 [dl_trainer.py:731] WARNING [ 33][ 3240/   98][rank:0] loss: 0.169, average forward (0.010255) and backward (0.023310) time: 0.080311, iotime: 0.005848 
2022-08-04 18:02:43,020 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079863, Speed: 801.369283 images/s
2022-08-04 18:02:44,875 [dl_trainer.py:731] WARNING [ 33][ 3280/   98][rank:0] loss: 0.147, average forward (0.008683) and backward (0.021498) time: 0.031888, iotime: 0.001472 
2022-08-04 18:02:44,986 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049137, Speed: 1302.485816 images/s
2022-08-04 18:02:45,763 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:45,763 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:46,987 [dl_trainer.py:731] WARNING [ 33][ 3320/   98][rank:0] loss: 0.422, average forward (0.010726) and backward (0.022243) time: 0.039169, iotime: 0.005938 
2022-08-04 18:02:47,589 [dl_trainer.py:634] INFO train iter: 3332, num_batches_per_epoch: 98
2022-08-04 18:02:47,589 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 91.597577, lr: 0.100000, avg loss: 0.241567
2022-08-04 18:02:49,222 [dl_trainer.py:822] INFO Epoch 34, lr: 0.100000, val loss: 0.521516, val top-1 acc: 84.325239, top-5 acc: 99.472532
2022-08-04 18:02:49,596 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080857, Speed: 791.522626 images/s
2022-08-04 18:02:50,552 [dl_trainer.py:731] WARNING [ 34][ 3360/   98][rank:0] loss: 0.252, average forward (0.009286) and backward (0.021352) time: 0.074478, iotime: 0.001609 
2022-08-04 18:02:51,454 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046454, Speed: 1377.715096 images/s
2022-08-04 18:02:52,238 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:52,238 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:02:52,623 [dl_trainer.py:731] WARNING [ 34][ 3400/   98][rank:0] loss: 0.140, average forward (0.011623) and backward (0.020830) time: 0.039025, iotime: 0.006275 
2022-08-04 18:02:54,081 [dl_trainer.py:634] INFO train iter: 3430, num_batches_per_epoch: 98
2022-08-04 18:02:54,081 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 91.198980, lr: 0.100000, avg loss: 0.247547
2022-08-04 18:02:55,752 [dl_trainer.py:822] INFO Epoch 35, lr: 0.100000, val loss: 0.449068, val top-1 acc: 86.126592, top-5 acc: 99.373010
2022-08-04 18:02:56,037 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080371, Speed: 796.305973 images/s
2022-08-04 18:02:56,206 [dl_trainer.py:731] WARNING [ 35][ 3440/   98][rank:0] loss: 0.193, average forward (0.011461) and backward (0.023694) time: 0.079027, iotime: 0.001758 
2022-08-04 18:02:58,004 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049151, Speed: 1302.120473 images/s
2022-08-04 18:02:58,201 [dl_trainer.py:731] WARNING [ 35][ 3480/   98][rank:0] loss: 0.194, average forward (0.013316) and backward (0.025052) time: 0.040862, iotime: 0.002169 
2022-08-04 18:02:58,798 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:02:58,798 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:00,302 [dl_trainer.py:731] WARNING [ 35][ 3520/   98][rank:0] loss: 0.243, average forward (0.011538) and backward (0.021878) time: 0.040209, iotime: 0.006518 
2022-08-04 18:03:00,701 [dl_trainer.py:634] INFO train iter: 3528, num_batches_per_epoch: 98
2022-08-04 18:03:00,702 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 91.358418, lr: 0.100000, avg loss: 0.247976
2022-08-04 18:03:02,432 [dl_trainer.py:822] INFO Epoch 36, lr: 0.100000, val loss: 0.524051, val top-1 acc: 84.225717, top-5 acc: 99.363057
2022-08-04 18:03:02,699 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082360, Speed: 777.074673 images/s
2022-08-04 18:03:03,982 [dl_trainer.py:731] WARNING [ 36][ 3560/   98][rank:0] loss: 0.306, average forward (0.011179) and backward (0.021170) time: 0.079118, iotime: 0.002165 
2022-08-04 18:03:04,630 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048266, Speed: 1325.975844 images/s
2022-08-04 18:03:05,369 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:05,369 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:06,034 [dl_trainer.py:731] WARNING [ 36][ 3600/   98][rank:0] loss: 0.094, average forward (0.009976) and backward (0.020989) time: 0.037099, iotime: 0.005881 
2022-08-04 18:03:07,251 [dl_trainer.py:634] INFO train iter: 3626, num_batches_per_epoch: 98
2022-08-04 18:03:07,252 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 91.709184, lr: 0.100000, avg loss: 0.241016
2022-08-04 18:03:08,861 [dl_trainer.py:822] INFO Epoch 37, lr: 0.100000, val loss: 0.402328, val top-1 acc: 87.201433, top-5 acc: 99.591959
2022-08-04 18:03:09,044 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.077416, Speed: 826.702204 images/s
2022-08-04 18:03:09,519 [dl_trainer.py:731] WARNING [ 37][ 3640/   98][rank:0] loss: 0.178, average forward (0.010141) and backward (0.019844) time: 0.072252, iotime: 0.001687 
2022-08-04 18:03:11,030 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049622, Speed: 1289.741083 images/s
2022-08-04 18:03:11,517 [dl_trainer.py:731] WARNING [ 37][ 3680/   98][rank:0] loss: 0.252, average forward (0.009390) and backward (0.018356) time: 0.029511, iotime: 0.001530 
2022-08-04 18:03:11,808 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:11,809 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:13,612 [dl_trainer.py:731] WARNING [ 37][ 3720/   98][rank:0] loss: 0.539, average forward (0.009801) and backward (0.021095) time: 0.037321, iotime: 0.006174 
2022-08-04 18:03:13,833 [dl_trainer.py:634] INFO train iter: 3724, num_batches_per_epoch: 98
2022-08-04 18:03:13,833 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 91.565689, lr: 0.100000, avg loss: 0.241596
2022-08-04 18:03:15,599 [dl_trainer.py:822] INFO Epoch 38, lr: 0.100000, val loss: 0.559525, val top-1 acc: 83.638535, top-5 acc: 99.313296
2022-08-04 18:03:15,832 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084240, Speed: 759.738040 images/s
2022-08-04 18:03:17,406 [dl_trainer.py:731] WARNING [ 38][ 3760/   98][rank:0] loss: 0.203, average forward (0.010659) and backward (0.020695) time: 0.079689, iotime: 0.001756 
2022-08-04 18:03:17,758 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048138, Speed: 1329.499340 images/s
2022-08-04 18:03:18,558 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:18,558 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:19,549 [dl_trainer.py:731] WARNING [ 38][ 3800/   98][rank:0] loss: 0.145, average forward (0.010284) and backward (0.020594) time: 0.037127, iotime: 0.005976 
2022-08-04 18:03:20,661 [dl_trainer.py:634] INFO train iter: 3822, num_batches_per_epoch: 98
2022-08-04 18:03:20,662 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 91.454082, lr: 0.100000, avg loss: 0.237540
2022-08-04 18:03:22,294 [dl_trainer.py:822] INFO Epoch 39, lr: 0.100000, val loss: 0.605739, val top-1 acc: 83.529061, top-5 acc: 98.785828
2022-08-04 18:03:22,385 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081145, Speed: 788.707967 images/s
2022-08-04 18:03:23,134 [dl_trainer.py:731] WARNING [ 39][ 3840/   98][rank:0] loss: 0.160, average forward (0.011318) and backward (0.023061) time: 0.077621, iotime: 0.001996 
2022-08-04 18:03:24,330 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048624, Speed: 1316.223814 images/s
2022-08-04 18:03:25,039 [dl_trainer.py:731] WARNING [ 39][ 3880/   98][rank:0] loss: 0.228, average forward (0.010053) and backward (0.022336) time: 0.034359, iotime: 0.001719 
2022-08-04 18:03:25,057 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:25,058 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:27,140 [dl_trainer.py:731] WARNING [ 39][ 3920/   98][rank:0] loss: 0.213, average forward (0.010311) and backward (0.020500) time: 0.036856, iotime: 0.005773 
2022-08-04 18:03:27,150 [dl_trainer.py:634] INFO train iter: 3920, num_batches_per_epoch: 98
2022-08-04 18:03:27,151 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 91.757015, lr: 0.100000, avg loss: 0.226138
2022-08-04 18:03:28,775 [dl_trainer.py:822] INFO Epoch 40, lr: 0.100000, val loss: 0.451658, val top-1 acc: 86.892914, top-5 acc: 99.432723
2022-08-04 18:03:28,859 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079427, Speed: 805.768582 images/s
2022-08-04 18:03:30,723 [dl_trainer.py:731] WARNING [ 40][ 3960/   98][rank:0] loss: 0.351, average forward (0.010618) and backward (0.020496) time: 0.074637, iotime: 0.001760 
2022-08-04 18:03:30,778 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047980, Speed: 1333.902251 images/s
2022-08-04 18:03:31,516 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:31,516 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:32,809 [dl_trainer.py:731] WARNING [ 40][ 4000/   98][rank:0] loss: 0.273, average forward (0.009703) and backward (0.023335) time: 0.039475, iotime: 0.006191 
2022-08-04 18:03:33,694 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051124, Speed: 1251.865506 images/s
2022-08-04 18:03:33,696 [dl_trainer.py:634] INFO train iter: 4018, num_batches_per_epoch: 98
2022-08-04 18:03:33,697 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 91.948342, lr: 0.100000, avg loss: 0.219838
2022-08-04 18:03:35,388 [dl_trainer.py:822] INFO Epoch 41, lr: 0.100000, val loss: 0.467865, val top-1 acc: 86.564490, top-5 acc: 99.562102
2022-08-04 18:03:36,432 [dl_trainer.py:731] WARNING [ 41][ 4040/   98][rank:0] loss: 0.239, average forward (0.009876) and backward (0.021260) time: 0.075473, iotime: 0.001723 
2022-08-04 18:03:37,322 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090682, Speed: 705.762301 images/s
2022-08-04 18:03:38,116 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:38,117 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:38,571 [dl_trainer.py:731] WARNING [ 41][ 4080/   98][rank:0] loss: 0.232, average forward (0.009754) and backward (0.023448) time: 0.039344, iotime: 0.005904 
2022-08-04 18:03:40,201 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050486, Speed: 1267.683211 images/s
2022-08-04 18:03:40,251 [dl_trainer.py:634] INFO train iter: 4116, num_batches_per_epoch: 98
2022-08-04 18:03:40,251 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 91.788903, lr: 0.100000, avg loss: 0.223343
2022-08-04 18:03:41,857 [dl_trainer.py:822] INFO Epoch 42, lr: 0.100000, val loss: 0.457355, val top-1 acc: 86.116640, top-5 acc: 99.502389
2022-08-04 18:03:42,074 [dl_trainer.py:731] WARNING [ 42][ 4120/   98][rank:0] loss: 0.192, average forward (0.010984) and backward (0.019676) time: 0.073985, iotime: 0.002166 
2022-08-04 18:03:43,800 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089967, Speed: 711.369069 images/s
2022-08-04 18:03:44,049 [dl_trainer.py:731] WARNING [ 42][ 4160/   98][rank:0] loss: 0.106, average forward (0.012016) and backward (0.022434) time: 0.036761, iotime: 0.002005 
2022-08-04 18:03:44,599 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:44,599 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:46,141 [dl_trainer.py:731] WARNING [ 42][ 4200/   98][rank:0] loss: 0.414, average forward (0.010304) and backward (0.020773) time: 0.037725, iotime: 0.006377 
2022-08-04 18:03:46,719 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051180, Speed: 1250.498415 images/s
2022-08-04 18:03:46,820 [dl_trainer.py:634] INFO train iter: 4214, num_batches_per_epoch: 98
2022-08-04 18:03:46,820 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 92.841199, lr: 0.100000, avg loss: 0.206516
2022-08-04 18:03:48,520 [dl_trainer.py:822] INFO Epoch 43, lr: 0.100000, val loss: 0.411790, val top-1 acc: 87.619427, top-5 acc: 99.552150
2022-08-04 18:03:49,746 [dl_trainer.py:731] WARNING [ 43][ 4240/   98][rank:0] loss: 0.296, average forward (0.011198) and backward (0.020429) time: 0.076311, iotime: 0.001819 
2022-08-04 18:03:50,298 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089465, Speed: 715.360657 images/s
2022-08-04 18:03:51,087 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:51,087 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:51,827 [dl_trainer.py:731] WARNING [ 43][ 4280/   98][rank:0] loss: 0.183, average forward (0.010715) and backward (0.021038) time: 0.038313, iotime: 0.006289 
2022-08-04 18:03:53,220 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051231, Speed: 1249.246695 images/s
2022-08-04 18:03:53,354 [dl_trainer.py:634] INFO train iter: 4312, num_batches_per_epoch: 98
2022-08-04 18:03:53,354 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 92.171556, lr: 0.100000, avg loss: 0.220213
2022-08-04 18:03:54,986 [dl_trainer.py:822] INFO Epoch 44, lr: 0.100000, val loss: 0.490477, val top-1 acc: 85.688694, top-5 acc: 99.432723
2022-08-04 18:03:55,374 [dl_trainer.py:731] WARNING [ 44][ 4320/   98][rank:0] loss: 0.312, average forward (0.009608) and backward (0.021139) time: 0.073936, iotime: 0.001661 
2022-08-04 18:03:56,795 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089343, Speed: 716.337772 images/s
2022-08-04 18:03:57,315 [dl_trainer.py:731] WARNING [ 44][ 4360/   98][rank:0] loss: 0.286, average forward (0.012553) and backward (0.022303) time: 0.037651, iotime: 0.002500 
2022-08-04 18:03:57,570 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:03:57,570 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:03:59,411 [dl_trainer.py:731] WARNING [ 44][ 4400/   98][rank:0] loss: 0.297, average forward (0.011128) and backward (0.023391) time: 0.040995, iotime: 0.006198 
2022-08-04 18:03:59,708 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051090, Speed: 1252.698765 images/s
2022-08-04 18:03:59,904 [dl_trainer.py:634] INFO train iter: 4410, num_batches_per_epoch: 98
2022-08-04 18:03:59,904 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 92.330995, lr: 0.100000, avg loss: 0.219053
2022-08-04 18:04:01,583 [dl_trainer.py:822] INFO Epoch 45, lr: 0.100000, val loss: 0.490162, val top-1 acc: 86.007166, top-5 acc: 99.442675
2022-08-04 18:04:03,022 [dl_trainer.py:731] WARNING [ 45][ 4440/   98][rank:0] loss: 0.214, average forward (0.010227) and backward (0.021991) time: 0.076226, iotime: 0.001717 
2022-08-04 18:04:03,333 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090609, Speed: 706.334558 images/s
2022-08-04 18:04:04,130 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:04,131 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:05,165 [dl_trainer.py:731] WARNING [ 45][ 4480/   98][rank:0] loss: 0.160, average forward (0.011875) and backward (0.022251) time: 0.040867, iotime: 0.006432 
2022-08-04 18:04:06,278 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051637, Speed: 1239.415895 images/s
2022-08-04 18:04:06,507 [dl_trainer.py:634] INFO train iter: 4508, num_batches_per_epoch: 98
2022-08-04 18:04:06,507 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 91.916454, lr: 0.100000, avg loss: 0.214146
2022-08-04 18:04:08,149 [dl_trainer.py:822] INFO Epoch 46, lr: 0.100000, val loss: 0.504289, val top-1 acc: 85.031847, top-5 acc: 99.442675
2022-08-04 18:04:08,811 [dl_trainer.py:731] WARNING [ 46][ 4520/   98][rank:0] loss: 0.294, average forward (0.010601) and backward (0.021584) time: 0.077333, iotime: 0.001727 
2022-08-04 18:04:09,910 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090798, Speed: 704.858638 images/s
2022-08-04 18:04:10,669 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:10,669 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:10,882 [dl_trainer.py:731] WARNING [ 46][ 4560/   98][rank:0] loss: 0.235, average forward (0.009279) and backward (0.021945) time: 0.037614, iotime: 0.006125 
2022-08-04 18:04:12,743 [dl_trainer.py:731] WARNING [ 46][ 4600/   98][rank:0] loss: 0.195, average forward (0.008620) and backward (0.020285) time: 0.030642, iotime: 0.001522 
2022-08-04 18:04:12,758 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049930, Speed: 1281.786365 images/s
2022-08-04 18:04:13,017 [dl_trainer.py:634] INFO train iter: 4606, num_batches_per_epoch: 98
2022-08-04 18:04:13,017 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 92.506378, lr: 0.100000, avg loss: 0.206654
2022-08-04 18:04:14,589 [dl_trainer.py:822] INFO Epoch 47, lr: 0.100000, val loss: 0.440222, val top-1 acc: 86.574443, top-5 acc: 99.492436
2022-08-04 18:04:16,225 [dl_trainer.py:731] WARNING [ 47][ 4640/   98][rank:0] loss: 0.424, average forward (0.010684) and backward (0.021474) time: 0.073614, iotime: 0.001836 
2022-08-04 18:04:16,239 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087017, Speed: 735.490888 images/s
2022-08-04 18:04:17,011 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:17,012 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:18,334 [dl_trainer.py:731] WARNING [ 47][ 4680/   98][rank:0] loss: 0.157, average forward (0.011073) and backward (0.021545) time: 0.038890, iotime: 0.005986 
2022-08-04 18:04:19,153 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051085, Speed: 1252.810566 images/s
2022-08-04 18:04:19,502 [dl_trainer.py:634] INFO train iter: 4704, num_batches_per_epoch: 98
2022-08-04 18:04:19,503 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 93.160077, lr: 0.100000, avg loss: 0.196382
2022-08-04 18:04:21,171 [dl_trainer.py:822] INFO Epoch 48, lr: 0.100000, val loss: 0.469364, val top-1 acc: 86.534634, top-5 acc: 99.392914
2022-08-04 18:04:21,939 [dl_trainer.py:731] WARNING [ 48][ 4720/   98][rank:0] loss: 0.135, average forward (0.010367) and backward (0.019177) time: 0.074143, iotime: 0.001754 
2022-08-04 18:04:22,778 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090626, Speed: 706.199558 images/s
2022-08-04 18:04:23,566 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:23,566 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:24,074 [dl_trainer.py:731] WARNING [ 48][ 4760/   98][rank:0] loss: 0.364, average forward (0.009542) and backward (0.020486) time: 0.036332, iotime: 0.006051 
2022-08-04 18:04:25,724 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051654, Speed: 1239.019856 images/s
2022-08-04 18:04:25,981 [dl_trainer.py:731] WARNING [ 48][ 4800/   98][rank:0] loss: 0.244, average forward (0.008944) and backward (0.018415) time: 0.029188, iotime: 0.001587 
2022-08-04 18:04:26,100 [dl_trainer.py:634] INFO train iter: 4802, num_batches_per_epoch: 98
2022-08-04 18:04:26,100 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 92.649872, lr: 0.100000, avg loss: 0.221072
2022-08-04 18:04:27,701 [dl_trainer.py:822] INFO Epoch 49, lr: 0.100000, val loss: 0.500356, val top-1 acc: 85.837978, top-5 acc: 99.512341
2022-08-04 18:04:29,280 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088882, Speed: 720.057645 images/s
2022-08-04 18:04:29,548 [dl_trainer.py:731] WARNING [ 49][ 4840/   98][rank:0] loss: 0.227, average forward (0.010544) and backward (0.022525) time: 0.075339, iotime: 0.001926 
2022-08-04 18:04:30,037 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:30,037 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:31,614 [dl_trainer.py:731] WARNING [ 49][ 4880/   98][rank:0] loss: 0.261, average forward (0.009122) and backward (0.018493) time: 0.033633, iotime: 0.005784 
2022-08-04 18:04:32,171 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050696, Speed: 1262.415957 images/s
2022-08-04 18:04:32,560 [dl_trainer.py:634] INFO train iter: 4900, num_batches_per_epoch: 98
2022-08-04 18:04:32,560 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 92.841199, lr: 0.100000, avg loss: 0.204301
2022-08-04 18:04:34,264 [dl_trainer.py:822] INFO Epoch 50, lr: 0.100000, val loss: 0.646157, val top-1 acc: 82.344745, top-5 acc: 99.253583
2022-08-04 18:04:35,260 [dl_trainer.py:731] WARNING [ 50][ 4920/   98][rank:0] loss: 0.176, average forward (0.010952) and backward (0.020561) time: 0.077352, iotime: 0.001774 
2022-08-04 18:04:35,781 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090247, Speed: 709.167710 images/s
2022-08-04 18:04:36,521 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:36,521 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:37,308 [dl_trainer.py:731] WARNING [ 50][ 4960/   98][rank:0] loss: 0.229, average forward (0.010544) and backward (0.019837) time: 0.036904, iotime: 0.006269 
2022-08-04 18:04:38,644 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050186, Speed: 1275.260236 images/s
2022-08-04 18:04:39,122 [dl_trainer.py:634] INFO train iter: 4998, num_batches_per_epoch: 98
2022-08-04 18:04:39,122 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 92.952806, lr: 0.100000, avg loss: 0.196563
2022-08-04 18:04:40,816 [dl_trainer.py:822] INFO Epoch 51, lr: 0.100000, val loss: 0.470305, val top-1 acc: 86.534634, top-5 acc: 99.482484
2022-08-04 18:04:40,919 [dl_trainer.py:731] WARNING [ 51][ 5000/   98][rank:0] loss: 0.148, average forward (0.010152) and backward (0.023316) time: 0.077831, iotime: 0.001675 
2022-08-04 18:04:42,293 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091220, Speed: 701.600363 images/s
2022-08-04 18:04:42,852 [dl_trainer.py:731] WARNING [ 51][ 5040/   98][rank:0] loss: 0.208, average forward (0.011412) and backward (0.020745) time: 0.034200, iotime: 0.001772 
2022-08-04 18:04:43,060 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:43,060 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:44,936 [dl_trainer.py:731] WARNING [ 51][ 5080/   98][rank:0] loss: 0.150, average forward (0.012008) and backward (0.020102) time: 0.038237, iotime: 0.005854 
2022-08-04 18:04:45,179 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050612, Speed: 1264.524495 images/s
2022-08-04 18:04:45,702 [dl_trainer.py:634] INFO train iter: 5096, num_batches_per_epoch: 98
2022-08-04 18:04:45,702 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 93.622449, lr: 0.100000, avg loss: 0.179634
2022-08-04 18:04:47,398 [dl_trainer.py:822] INFO Epoch 52, lr: 0.100000, val loss: 0.451018, val top-1 acc: 86.952627, top-5 acc: 99.472532
2022-08-04 18:04:48,605 [dl_trainer.py:731] WARNING [ 52][ 5120/   98][rank:0] loss: 0.149, average forward (0.009826) and backward (0.021132) time: 0.076129, iotime: 0.001590 
2022-08-04 18:04:48,851 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091768, Speed: 697.411160 images/s
2022-08-04 18:04:49,638 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:49,639 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:50,701 [dl_trainer.py:731] WARNING [ 52][ 5160/   98][rank:0] loss: 0.073, average forward (0.011487) and backward (0.019449) time: 0.037140, iotime: 0.005928 
2022-08-04 18:04:51,785 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051466, Speed: 1243.536779 images/s
2022-08-04 18:04:52,381 [dl_trainer.py:634] INFO train iter: 5194, num_batches_per_epoch: 98
2022-08-04 18:04:52,381 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 93.319515, lr: 0.100000, avg loss: 0.194321
2022-08-04 18:04:54,007 [dl_trainer.py:822] INFO Epoch 53, lr: 0.100000, val loss: 0.486442, val top-1 acc: 86.644108, top-5 acc: 99.502389
2022-08-04 18:04:54,280 [dl_trainer.py:731] WARNING [ 53][ 5200/   98][rank:0] loss: 0.279, average forward (0.009545) and backward (0.019803) time: 0.071967, iotime: 0.001651 
2022-08-04 18:04:55,371 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089632, Speed: 714.029006 images/s
2022-08-04 18:04:56,168 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:04:56,168 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:04:56,425 [dl_trainer.py:731] WARNING [ 53][ 5240/   98][rank:0] loss: 0.127, average forward (0.011259) and backward (0.021285) time: 0.038955, iotime: 0.006132 
2022-08-04 18:04:58,333 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051936, Speed: 1232.285417 images/s
2022-08-04 18:04:58,367 [dl_trainer.py:731] WARNING [ 53][ 5280/   98][rank:0] loss: 0.142, average forward (0.009447) and backward (0.021768) time: 0.033128, iotime: 0.001671 
2022-08-04 18:04:58,974 [dl_trainer.py:634] INFO train iter: 5292, num_batches_per_epoch: 98
2022-08-04 18:04:58,975 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 93.160077, lr: 0.100000, avg loss: 0.195010
2022-08-04 18:05:00,681 [dl_trainer.py:822] INFO Epoch 54, lr: 0.100000, val loss: 0.424494, val top-1 acc: 87.728901, top-5 acc: 99.492436
2022-08-04 18:05:02,055 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.093031, Speed: 687.941839 images/s
2022-08-04 18:05:02,099 [dl_trainer.py:731] WARNING [ 54][ 5320/   98][rank:0] loss: 0.325, average forward (0.010241) and backward (0.021742) time: 0.078532, iotime: 0.001711 
2022-08-04 18:05:02,866 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:02,866 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:04,240 [dl_trainer.py:731] WARNING [ 54][ 5360/   98][rank:0] loss: 0.111, average forward (0.011030) and backward (0.020755) time: 0.037981, iotime: 0.005923 
2022-08-04 18:05:05,033 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052231, Speed: 1225.332968 images/s
2022-08-04 18:05:05,720 [dl_trainer.py:634] INFO train iter: 5390, num_batches_per_epoch: 98
2022-08-04 18:05:05,720 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 93.670281, lr: 0.100000, avg loss: 0.180880
2022-08-04 18:05:07,423 [dl_trainer.py:822] INFO Epoch 55, lr: 0.100000, val loss: 0.467504, val top-1 acc: 86.644108, top-5 acc: 99.353105
2022-08-04 18:05:07,922 [dl_trainer.py:731] WARNING [ 55][ 5400/   98][rank:0] loss: 0.276, average forward (0.010322) and backward (0.019600) time: 0.074555, iotime: 0.001745 
2022-08-04 18:05:08,721 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.092188, Speed: 694.235044 images/s
2022-08-04 18:05:09,508 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:09,508 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:10,064 [dl_trainer.py:731] WARNING [ 55][ 5440/   98][rank:0] loss: 0.172, average forward (0.009200) and backward (0.020884) time: 0.036209, iotime: 0.005878 
2022-08-04 18:05:11,744 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052991, Speed: 1207.754043 images/s
2022-08-04 18:05:12,062 [dl_trainer.py:731] WARNING [ 55][ 5480/   98][rank:0] loss: 0.207, average forward (0.011272) and backward (0.020737) time: 0.034259, iotime: 0.001958 
2022-08-04 18:05:12,470 [dl_trainer.py:634] INFO train iter: 5488, num_batches_per_epoch: 98
2022-08-04 18:05:12,470 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 93.415179, lr: 0.100000, avg loss: 0.181902
2022-08-04 18:05:14,153 [dl_trainer.py:822] INFO Epoch 56, lr: 0.100000, val loss: 0.467222, val top-1 acc: 85.818073, top-5 acc: 99.522293
2022-08-04 18:05:15,419 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091868, Speed: 696.653110 images/s
2022-08-04 18:05:15,743 [dl_trainer.py:731] WARNING [ 56][ 5520/   98][rank:0] loss: 0.155, average forward (0.009395) and backward (0.018869) time: 0.073291, iotime: 0.001736 
2022-08-04 18:05:16,194 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:16,194 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:17,832 [dl_trainer.py:731] WARNING [ 56][ 5560/   98][rank:0] loss: 0.162, average forward (0.009494) and backward (0.021380) time: 0.037234, iotime: 0.006128 
2022-08-04 18:05:18,327 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050995, Speed: 1255.016186 images/s
2022-08-04 18:05:19,084 [dl_trainer.py:634] INFO train iter: 5586, num_batches_per_epoch: 98
2022-08-04 18:05:19,084 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 93.702168, lr: 0.100000, avg loss: 0.178635
2022-08-04 18:05:20,762 [dl_trainer.py:822] INFO Epoch 57, lr: 0.100000, val loss: 0.577739, val top-1 acc: 84.225717, top-5 acc: 99.034634
2022-08-04 18:05:21,441 [dl_trainer.py:731] WARNING [ 57][ 5600/   98][rank:0] loss: 0.110, average forward (0.011149) and backward (0.021022) time: 0.076326, iotime: 0.001834 
2022-08-04 18:05:21,952 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090611, Speed: 706.315973 images/s
2022-08-04 18:05:22,675 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:22,676 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:23,528 [dl_trainer.py:731] WARNING [ 57][ 5640/   98][rank:0] loss: 0.217, average forward (0.010073) and backward (0.019351) time: 0.035778, iotime: 0.006097 
2022-08-04 18:05:24,850 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050811, Speed: 1259.573043 images/s
2022-08-04 18:05:25,473 [dl_trainer.py:731] WARNING [ 57][ 5680/   98][rank:0] loss: 0.119, average forward (0.009699) and backward (0.022579) time: 0.034250, iotime: 0.001723 
2022-08-04 18:05:25,677 [dl_trainer.py:634] INFO train iter: 5684, num_batches_per_epoch: 98
2022-08-04 18:05:25,677 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 93.335459, lr: 0.100000, avg loss: 0.187563
2022-08-04 18:05:27,360 [dl_trainer.py:822] INFO Epoch 58, lr: 0.100000, val loss: 0.472944, val top-1 acc: 86.584395, top-5 acc: 99.601911
2022-08-04 18:05:28,516 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091628, Speed: 698.479969 images/s
2022-08-04 18:05:29,133 [dl_trainer.py:731] WARNING [ 58][ 5720/   98][rank:0] loss: 0.109, average forward (0.011190) and backward (0.022995) time: 0.079196, iotime: 0.001937 
2022-08-04 18:05:29,287 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:29,287 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:31,244 [dl_trainer.py:731] WARNING [ 58][ 5760/   98][rank:0] loss: 0.158, average forward (0.009134) and backward (0.021166) time: 0.036068, iotime: 0.005543 
2022-08-04 18:05:31,456 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051555, Speed: 1241.398784 images/s
2022-08-04 18:05:32,351 [dl_trainer.py:634] INFO train iter: 5782, num_batches_per_epoch: 98
2022-08-04 18:05:32,352 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 93.813776, lr: 0.100000, avg loss: 0.169162
2022-08-04 18:05:34,039 [dl_trainer.py:822] INFO Epoch 59, lr: 0.100000, val loss: 0.577172, val top-1 acc: 83.867436, top-5 acc: 99.631768
2022-08-04 18:05:34,873 [dl_trainer.py:731] WARNING [ 59][ 5800/   98][rank:0] loss: 0.383, average forward (0.011458) and backward (0.022813) time: 0.078691, iotime: 0.001872 
2022-08-04 18:05:35,083 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090676, Speed: 705.811430 images/s
2022-08-04 18:05:35,869 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:35,869 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:36,986 [dl_trainer.py:731] WARNING [ 59][ 5840/   98][rank:0] loss: 0.234, average forward (0.010861) and backward (0.022072) time: 0.039263, iotime: 0.006053 
2022-08-04 18:05:38,019 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051468, Speed: 1243.485440 images/s
2022-08-04 18:05:38,927 [dl_trainer.py:731] WARNING [ 59][ 5880/   98][rank:0] loss: 0.130, average forward (0.009467) and backward (0.022134) time: 0.033439, iotime: 0.001584 
2022-08-04 18:05:38,943 [dl_trainer.py:634] INFO train iter: 5880, num_batches_per_epoch: 98
2022-08-04 18:05:38,943 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 92.904974, lr: 0.100000, avg loss: 0.194178
2022-08-04 18:05:40,613 [dl_trainer.py:822] INFO Epoch 60, lr: 0.100000, val loss: 0.475354, val top-1 acc: 86.773487, top-5 acc: 99.392914
2022-08-04 18:05:41,694 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091855, Speed: 696.752201 images/s
2022-08-04 18:05:42,460 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:42,460 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:42,761 [dl_trainer.py:731] WARNING [ 60][ 5920/   98][rank:0] loss: 0.151, average forward (0.010741) and backward (0.020235) time: 0.080442, iotime: 0.006245 
2022-08-04 18:05:44,626 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051415, Speed: 1244.783457 images/s
2022-08-04 18:05:44,719 [dl_trainer.py:731] WARNING [ 60][ 5960/   98][rank:0] loss: 0.156, average forward (0.010471) and backward (0.022642) time: 0.035077, iotime: 0.001708 
2022-08-04 18:05:45,571 [dl_trainer.py:634] INFO train iter: 5978, num_batches_per_epoch: 98
2022-08-04 18:05:45,571 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 93.925383, lr: 0.100000, avg loss: 0.180560
2022-08-04 18:05:47,202 [dl_trainer.py:822] INFO Epoch 61, lr: 0.100000, val loss: 0.454600, val top-1 acc: 87.121815, top-5 acc: 99.442675
2022-08-04 18:05:48,165 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088467, Speed: 723.432070 images/s
2022-08-04 18:05:48,255 [dl_trainer.py:731] WARNING [ 61][ 6000/   98][rank:0] loss: 0.231, average forward (0.010391) and backward (0.022430) time: 0.075639, iotime: 0.001745 
2022-08-04 18:05:48,955 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:48,955 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:50,361 [dl_trainer.py:731] WARNING [ 61][ 6040/   98][rank:0] loss: 0.074, average forward (0.010197) and backward (0.021370) time: 0.037662, iotime: 0.005843 
2022-08-04 18:05:51,129 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051973, Speed: 1231.404159 images/s
2022-08-04 18:05:52,169 [dl_trainer.py:634] INFO train iter: 6076, num_batches_per_epoch: 98
2022-08-04 18:05:52,169 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 94.021046, lr: 0.100000, avg loss: 0.162747
2022-08-04 18:05:53,729 [dl_trainer.py:822] INFO Epoch 62, lr: 0.100000, val loss: 0.479972, val top-1 acc: 86.932723, top-5 acc: 99.432723
2022-08-04 18:05:53,932 [dl_trainer.py:731] WARNING [ 62][ 6080/   98][rank:0] loss: 0.239, average forward (0.009399) and backward (0.023562) time: 0.074486, iotime: 0.001608 
2022-08-04 18:05:54,668 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088449, Speed: 723.581834 images/s
2022-08-04 18:05:55,403 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:05:55,403 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:05:56,031 [dl_trainer.py:731] WARNING [ 62][ 6120/   98][rank:0] loss: 0.068, average forward (0.010537) and backward (0.021300) time: 0.037838, iotime: 0.005749 
2022-08-04 18:05:57,624 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051846, Speed: 1234.436626 images/s
2022-08-04 18:05:58,014 [dl_trainer.py:731] WARNING [ 62][ 6160/   98][rank:0] loss: 0.241, average forward (0.011036) and backward (0.022717) time: 0.035876, iotime: 0.001832 
2022-08-04 18:05:58,724 [dl_trainer.py:634] INFO train iter: 6174, num_batches_per_epoch: 98
2022-08-04 18:05:58,724 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 94.148597, lr: 0.100000, avg loss: 0.162794
2022-08-04 18:06:00,314 [dl_trainer.py:822] INFO Epoch 63, lr: 0.100000, val loss: 0.539680, val top-1 acc: 85.201035, top-5 acc: 99.482484
2022-08-04 18:06:01,185 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089000, Speed: 719.104410 images/s
2022-08-04 18:06:01,580 [dl_trainer.py:731] WARNING [ 63][ 6200/   98][rank:0] loss: 0.061, average forward (0.010774) and backward (0.022504) time: 0.075171, iotime: 0.001805 
2022-08-04 18:06:01,976 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:01,976 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:03,688 [dl_trainer.py:731] WARNING [ 63][ 6240/   98][rank:0] loss: 0.296, average forward (0.010701) and backward (0.022300) time: 0.039562, iotime: 0.006283 
2022-08-04 18:06:04,158 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052135, Speed: 1227.582852 images/s
2022-08-04 18:06:05,292 [dl_trainer.py:634] INFO train iter: 6272, num_batches_per_epoch: 98
2022-08-04 18:06:05,293 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 94.355867, lr: 0.100000, avg loss: 0.158574
2022-08-04 18:06:07,040 [dl_trainer.py:822] INFO Epoch 64, lr: 0.100000, val loss: 0.507872, val top-1 acc: 86.753583, top-5 acc: 99.353105
2022-08-04 18:06:07,526 [dl_trainer.py:731] WARNING [ 64][ 6280/   98][rank:0] loss: 0.151, average forward (0.010422) and backward (0.018831) time: 0.077883, iotime: 0.001671 
2022-08-04 18:06:08,005 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.096176, Speed: 665.449084 images/s
2022-08-04 18:06:08,791 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:08,792 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:09,637 [dl_trainer.py:731] WARNING [ 64][ 6320/   98][rank:0] loss: 0.043, average forward (0.009078) and backward (0.021304) time: 0.036665, iotime: 0.005990 
2022-08-04 18:06:10,923 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051162, Speed: 1250.928519 images/s
2022-08-04 18:06:11,615 [dl_trainer.py:731] WARNING [ 64][ 6360/   98][rank:0] loss: 0.099, average forward (0.009301) and backward (0.019529) time: 0.030684, iotime: 0.001620 
2022-08-04 18:06:12,135 [dl_trainer.py:634] INFO train iter: 6370, num_batches_per_epoch: 98
2022-08-04 18:06:12,135 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 94.308036, lr: 0.100000, avg loss: 0.155586
2022-08-04 18:06:13,776 [dl_trainer.py:822] INFO Epoch 65, lr: 0.100000, val loss: 0.484058, val top-1 acc: 86.395303, top-5 acc: 99.353105
2022-08-04 18:06:14,570 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091173, Speed: 701.959870 images/s
2022-08-04 18:06:15,240 [dl_trainer.py:731] WARNING [ 65][ 6400/   98][rank:0] loss: 0.225, average forward (0.009282) and backward (0.019146) time: 0.071337, iotime: 0.001604 
2022-08-04 18:06:15,356 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:15,356 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:17,352 [dl_trainer.py:731] WARNING [ 65][ 6440/   98][rank:0] loss: 0.135, average forward (0.010562) and backward (0.020985) time: 0.038077, iotime: 0.006258 
2022-08-04 18:06:17,516 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051657, Speed: 1238.935081 images/s
2022-08-04 18:06:18,726 [dl_trainer.py:634] INFO train iter: 6468, num_batches_per_epoch: 98
2022-08-04 18:06:18,726 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 94.419643, lr: 0.100000, avg loss: 0.158085
2022-08-04 18:06:20,341 [dl_trainer.py:822] INFO Epoch 66, lr: 0.100000, val loss: 0.557187, val top-1 acc: 84.852707, top-5 acc: 99.442675
2022-08-04 18:06:20,980 [dl_trainer.py:731] WARNING [ 66][ 6480/   98][rank:0] loss: 0.139, average forward (0.011836) and backward (0.022608) time: 0.078345, iotime: 0.001886 
2022-08-04 18:06:21,119 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090061, Speed: 710.630474 images/s
2022-08-04 18:06:21,897 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:21,898 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:23,021 [dl_trainer.py:731] WARNING [ 66][ 6520/   98][rank:0] loss: 0.116, average forward (0.009796) and backward (0.021347) time: 0.037480, iotime: 0.006093 
2022-08-04 18:06:24,011 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050717, Speed: 1261.903086 images/s
2022-08-04 18:06:24,982 [dl_trainer.py:731] WARNING [ 66][ 6560/   98][rank:0] loss: 0.229, average forward (0.010659) and backward (0.023499) time: 0.036219, iotime: 0.001800 
2022-08-04 18:06:25,305 [dl_trainer.py:634] INFO train iter: 6566, num_batches_per_epoch: 98
2022-08-04 18:06:25,305 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 94.148597, lr: 0.100000, avg loss: 0.162250
2022-08-04 18:06:26,998 [dl_trainer.py:822] INFO Epoch 67, lr: 0.100000, val loss: 0.420698, val top-1 acc: 87.509952, top-5 acc: 99.591959
2022-08-04 18:06:27,706 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.092361, Speed: 692.933275 images/s
2022-08-04 18:06:28,480 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:28,480 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:28,839 [dl_trainer.py:731] WARNING [ 67][ 6600/   98][rank:0] loss: 0.159, average forward (0.011061) and backward (0.022828) time: 0.082922, iotime: 0.006346 
2022-08-04 18:06:30,654 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051687, Speed: 1238.211102 images/s
2022-08-04 18:06:30,792 [dl_trainer.py:731] WARNING [ 67][ 6640/   98][rank:0] loss: 0.149, average forward (0.009358) and backward (0.019535) time: 0.030820, iotime: 0.001672 
2022-08-04 18:06:31,930 [dl_trainer.py:634] INFO train iter: 6664, num_batches_per_epoch: 98
2022-08-04 18:06:31,930 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 94.132653, lr: 0.100000, avg loss: 0.169439
2022-08-04 18:06:33,561 [dl_trainer.py:822] INFO Epoch 68, lr: 0.100000, val loss: 0.465189, val top-1 acc: 86.713774, top-5 acc: 99.601911
2022-08-04 18:06:34,211 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088926, Speed: 719.700688 images/s
2022-08-04 18:06:34,346 [dl_trainer.py:731] WARNING [ 68][ 6680/   98][rank:0] loss: 0.121, average forward (0.009542) and backward (0.020860) time: 0.074045, iotime: 0.001700 
2022-08-04 18:06:34,993 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:34,994 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:36,453 [dl_trainer.py:731] WARNING [ 68][ 6720/   98][rank:0] loss: 0.222, average forward (0.010382) and backward (0.020895) time: 0.037549, iotime: 0.006019 
2022-08-04 18:06:37,114 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050886, Speed: 1257.711474 images/s
2022-08-04 18:06:38,346 [dl_trainer.py:731] WARNING [ 68][ 6760/   98][rank:0] loss: 0.258, average forward (0.009233) and backward (0.021434) time: 0.032503, iotime: 0.001593 
2022-08-04 18:06:38,449 [dl_trainer.py:634] INFO train iter: 6762, num_batches_per_epoch: 98
2022-08-04 18:06:38,450 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 93.973214, lr: 0.100000, avg loss: 0.163534
2022-08-04 18:06:40,111 [dl_trainer.py:822] INFO Epoch 69, lr: 0.100000, val loss: 0.429761, val top-1 acc: 88.017516, top-5 acc: 99.552150
2022-08-04 18:06:40,710 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089907, Speed: 711.845015 images/s
2022-08-04 18:06:41,444 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:41,444 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:42,101 [dl_trainer.py:731] WARNING [ 69][ 6800/   98][rank:0] loss: 0.195, average forward (0.010700) and backward (0.021372) time: 0.080190, iotime: 0.006248 
2022-08-04 18:06:43,627 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051153, Speed: 1251.159283 images/s
2022-08-04 18:06:44,045 [dl_trainer.py:731] WARNING [ 69][ 6840/   98][rank:0] loss: 0.363, average forward (0.010899) and backward (0.022281) time: 0.035307, iotime: 0.001842 
2022-08-04 18:06:45,051 [dl_trainer.py:634] INFO train iter: 6860, num_batches_per_epoch: 98
2022-08-04 18:06:45,051 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 94.355867, lr: 0.100000, avg loss: 0.165194
2022-08-04 18:06:46,726 [dl_trainer.py:822] INFO Epoch 70, lr: 0.100000, val loss: 0.439736, val top-1 acc: 87.539809, top-5 acc: 99.522293
2022-08-04 18:06:47,286 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091466, Speed: 699.711338 images/s
2022-08-04 18:06:47,675 [dl_trainer.py:731] WARNING [ 70][ 6880/   98][rank:0] loss: 0.137, average forward (0.010146) and backward (0.021378) time: 0.075908, iotime: 0.001722 
2022-08-04 18:06:48,036 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:48,037 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:49,774 [dl_trainer.py:731] WARNING [ 70][ 6920/   98][rank:0] loss: 0.178, average forward (0.010336) and backward (0.022549) time: 0.039154, iotime: 0.006005 
2022-08-04 18:06:50,186 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050845, Speed: 1258.733516 images/s
2022-08-04 18:06:51,614 [dl_trainer.py:634] INFO train iter: 6958, num_batches_per_epoch: 98
2022-08-04 18:06:51,614 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 94.339923, lr: 0.100000, avg loss: 0.162148
2022-08-04 18:06:53,264 [dl_trainer.py:822] INFO Epoch 71, lr: 0.100000, val loss: 0.450045, val top-1 acc: 87.549761, top-5 acc: 99.601911
2022-08-04 18:06:53,355 [dl_trainer.py:731] WARNING [ 71][ 6960/   98][rank:0] loss: 0.135, average forward (0.010090) and backward (0.021745) time: 0.075211, iotime: 0.001770 
2022-08-04 18:06:53,758 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089292, Speed: 716.752111 images/s
2022-08-04 18:06:54,504 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:06:54,504 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:06:55,434 [dl_trainer.py:731] WARNING [ 71][ 7000/   98][rank:0] loss: 0.197, average forward (0.009481) and backward (0.019251) time: 0.035007, iotime: 0.006043 
2022-08-04 18:06:56,636 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050463, Speed: 1268.262392 images/s
2022-08-04 18:06:57,325 [dl_trainer.py:731] WARNING [ 71][ 7040/   98][rank:0] loss: 0.236, average forward (0.009220) and backward (0.020341) time: 0.031477, iotime: 0.001679 
2022-08-04 18:06:58,100 [dl_trainer.py:634] INFO train iter: 7056, num_batches_per_epoch: 98
2022-08-04 18:06:58,100 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 94.945791, lr: 0.100000, avg loss: 0.143386
2022-08-04 18:06:59,668 [dl_trainer.py:822] INFO Epoch 72, lr: 0.100000, val loss: 0.413455, val top-1 acc: 88.445462, top-5 acc: 99.582006
2022-08-04 18:07:00,137 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087512, Speed: 731.330131 images/s
2022-08-04 18:07:00,839 [dl_trainer.py:731] WARNING [ 72][ 7080/   98][rank:0] loss: 0.223, average forward (0.010356) and backward (0.021232) time: 0.073720, iotime: 0.001768 
2022-08-04 18:07:00,909 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:00,909 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:02,988 [dl_trainer.py:731] WARNING [ 72][ 7120/   98][rank:0] loss: 0.220, average forward (0.008876) and backward (0.020791) time: 0.035980, iotime: 0.006077 
2022-08-04 18:07:03,107 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052087, Speed: 1228.724807 images/s
2022-08-04 18:07:04,638 [dl_trainer.py:634] INFO train iter: 7154, num_batches_per_epoch: 98
2022-08-04 18:07:04,639 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 94.308036, lr: 0.100000, avg loss: 0.158941
2022-08-04 18:07:06,501 [dl_trainer.py:822] INFO Epoch 73, lr: 0.100000, val loss: 0.453530, val top-1 acc: 87.410430, top-5 acc: 99.611863
2022-08-04 18:07:06,783 [dl_trainer.py:731] WARNING [ 73][ 7160/   98][rank:0] loss: 0.112, average forward (0.011044) and backward (0.021892) time: 0.081652, iotime: 0.001817 
2022-08-04 18:07:06,902 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.094882, Speed: 674.519033 images/s
2022-08-04 18:07:07,682 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:07,683 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:08,887 [dl_trainer.py:731] WARNING [ 73][ 7200/   98][rank:0] loss: 0.301, average forward (0.010256) and backward (0.020696) time: 0.037527, iotime: 0.006323 
2022-08-04 18:07:09,821 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051178, Speed: 1250.544509 images/s
2022-08-04 18:07:10,818 [dl_trainer.py:731] WARNING [ 73][ 7240/   98][rank:0] loss: 0.154, average forward (0.010550) and backward (0.022484) time: 0.035055, iotime: 0.001753 
2022-08-04 18:07:11,425 [dl_trainer.py:634] INFO train iter: 7252, num_batches_per_epoch: 98
2022-08-04 18:07:11,425 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 94.355867, lr: 0.100000, avg loss: 0.161595
2022-08-04 18:07:13,106 [dl_trainer.py:822] INFO Epoch 74, lr: 0.100000, val loss: 0.537127, val top-1 acc: 85.519506, top-5 acc: 99.134156
2022-08-04 18:07:13,482 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091511, Speed: 699.366338 images/s
2022-08-04 18:07:14,263 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:14,263 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:14,650 [dl_trainer.py:731] WARNING [ 74][ 7280/   98][rank:0] loss: 0.111, average forward (0.010951) and backward (0.021440) time: 0.082138, iotime: 0.006420 
2022-08-04 18:07:16,441 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051883, Speed: 1233.551294 images/s
2022-08-04 18:07:16,618 [dl_trainer.py:731] WARNING [ 74][ 7320/   98][rank:0] loss: 0.081, average forward (0.008998) and backward (0.020380) time: 0.031187, iotime: 0.001572 
2022-08-04 18:07:18,107 [dl_trainer.py:634] INFO train iter: 7350, num_batches_per_epoch: 98
2022-08-04 18:07:18,107 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 94.531250, lr: 0.100000, avg loss: 0.149706
2022-08-04 18:07:19,721 [dl_trainer.py:822] INFO Epoch 75, lr: 0.100000, val loss: 0.509465, val top-1 acc: 86.405255, top-5 acc: 99.452627
2022-08-04 18:07:20,011 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089259, Speed: 717.011765 images/s
2022-08-04 18:07:20,202 [dl_trainer.py:731] WARNING [ 75][ 7360/   98][rank:0] loss: 0.200, average forward (0.011095) and backward (0.021367) time: 0.074993, iotime: 0.001840 
2022-08-04 18:07:20,812 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:20,812 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:22,297 [dl_trainer.py:731] WARNING [ 75][ 7400/   98][rank:0] loss: 0.042, average forward (0.010412) and backward (0.021192) time: 0.037614, iotime: 0.005748 
2022-08-04 18:07:22,961 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051724, Speed: 1237.335059 images/s
2022-08-04 18:07:24,258 [dl_trainer.py:731] WARNING [ 75][ 7440/   98][rank:0] loss: 0.068, average forward (0.008914) and backward (0.021406) time: 0.032059, iotime: 0.001506 
2022-08-04 18:07:24,668 [dl_trainer.py:634] INFO train iter: 7448, num_batches_per_epoch: 98
2022-08-04 18:07:24,668 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 94.276148, lr: 0.100000, avg loss: 0.155789
2022-08-04 18:07:26,281 [dl_trainer.py:822] INFO Epoch 76, lr: 0.100000, val loss: 0.407176, val top-1 acc: 88.355892, top-5 acc: 99.552150
2022-08-04 18:07:26,591 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090731, Speed: 705.379658 images/s
2022-08-04 18:07:27,378 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:27,379 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:28,050 [dl_trainer.py:731] WARNING [ 76][ 7480/   98][rank:0] loss: 0.123, average forward (0.009169) and backward (0.018177) time: 0.075016, iotime: 0.005789 
2022-08-04 18:07:29,534 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051611, Speed: 1240.057058 images/s
2022-08-04 18:07:29,981 [dl_trainer.py:731] WARNING [ 76][ 7520/   98][rank:0] loss: 0.233, average forward (0.009451) and backward (0.019007) time: 0.030400, iotime: 0.001692 
2022-08-04 18:07:31,293 [dl_trainer.py:634] INFO train iter: 7546, num_batches_per_epoch: 98
2022-08-04 18:07:31,294 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 94.595026, lr: 0.100000, avg loss: 0.147962
2022-08-04 18:07:32,908 [dl_trainer.py:822] INFO Epoch 77, lr: 0.100000, val loss: 0.491512, val top-1 acc: 86.614252, top-5 acc: 99.382962
2022-08-04 18:07:33,112 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089438, Speed: 715.581246 images/s
2022-08-04 18:07:33,590 [dl_trainer.py:731] WARNING [ 77][ 7560/   98][rank:0] loss: 0.110, average forward (0.011370) and backward (0.021853) time: 0.075836, iotime: 0.001881 
2022-08-04 18:07:33,907 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:33,907 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:35,724 [dl_trainer.py:731] WARNING [ 77][ 7600/   98][rank:0] loss: 0.182, average forward (0.009884) and backward (0.019300) time: 0.035897, iotime: 0.006432 
2022-08-04 18:07:36,095 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052311, Speed: 1223.440648 images/s
2022-08-04 18:07:37,684 [dl_trainer.py:731] WARNING [ 77][ 7640/   98][rank:0] loss: 0.125, average forward (0.009975) and backward (0.019489) time: 0.031432, iotime: 0.001715 
2022-08-04 18:07:37,892 [dl_trainer.py:634] INFO train iter: 7644, num_batches_per_epoch: 98
2022-08-04 18:07:37,892 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 94.690689, lr: 0.100000, avg loss: 0.146931
2022-08-04 18:07:39,443 [dl_trainer.py:822] INFO Epoch 78, lr: 0.100000, val loss: 0.487621, val top-1 acc: 86.355494, top-5 acc: 99.472532
2022-08-04 18:07:39,603 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.087678, Speed: 729.941496 images/s
2022-08-04 18:07:40,401 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:40,401 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:41,396 [dl_trainer.py:731] WARNING [ 78][ 7680/   98][rank:0] loss: 0.082, average forward (0.009726) and backward (0.020643) time: 0.075994, iotime: 0.005907 
2022-08-04 18:07:42,564 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051926, Speed: 1232.527523 images/s
2022-08-04 18:07:43,305 [dl_trainer.py:731] WARNING [ 78][ 7720/   98][rank:0] loss: 0.092, average forward (0.010775) and backward (0.021651) time: 0.034524, iotime: 0.001813 
2022-08-04 18:07:44,397 [dl_trainer.py:634] INFO train iter: 7742, num_batches_per_epoch: 98
2022-08-04 18:07:44,397 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 95.089286, lr: 0.100000, avg loss: 0.135046
2022-08-04 18:07:46,032 [dl_trainer.py:822] INFO Epoch 79, lr: 0.100000, val loss: 0.518582, val top-1 acc: 87.141720, top-5 acc: 99.353105
2022-08-04 18:07:46,134 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089231, Speed: 717.239122 images/s
2022-08-04 18:07:46,916 [dl_trainer.py:731] WARNING [ 79][ 7760/   98][rank:0] loss: 0.177, average forward (0.009545) and backward (0.019619) time: 0.072002, iotime: 0.001642 
2022-08-04 18:07:46,929 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:46,929 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:48,993 [dl_trainer.py:731] WARNING [ 79][ 7800/   98][rank:0] loss: 0.106, average forward (0.009001) and backward (0.020493) time: 0.035451, iotime: 0.005730 
2022-08-04 18:07:49,048 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051110, Speed: 1252.206158 images/s
2022-08-04 18:07:50,894 [dl_trainer.py:731] WARNING [ 79][ 7840/   98][rank:0] loss: 0.128, average forward (0.010610) and backward (0.022003) time: 0.034632, iotime: 0.001746 
2022-08-04 18:07:50,901 [dl_trainer.py:634] INFO train iter: 7840, num_batches_per_epoch: 98
2022-08-04 18:07:50,901 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 94.882015, lr: 0.100000, avg loss: 0.154387
2022-08-04 18:07:52,531 [dl_trainer.py:822] INFO Epoch 80, lr: 0.100000, val loss: 0.442688, val top-1 acc: 87.310908, top-5 acc: 99.482484
2022-08-04 18:07:52,645 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089908, Speed: 711.837417 images/s
2022-08-04 18:07:53,440 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:53,440 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:07:54,697 [dl_trainer.py:731] WARNING [ 80][ 7880/   98][rank:0] loss: 0.092, average forward (0.010369) and backward (0.022293) time: 0.081355, iotime: 0.006068 
2022-08-04 18:07:55,602 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051842, Speed: 1234.511922 images/s
2022-08-04 18:07:56,648 [dl_trainer.py:731] WARNING [ 80][ 7920/   98][rank:0] loss: 0.096, average forward (0.009792) and backward (0.021764) time: 0.033482, iotime: 0.001673 
2022-08-04 18:07:57,533 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048273, Speed: 1325.793292 images/s
2022-08-04 18:07:57,534 [dl_trainer.py:634] INFO train iter: 7938, num_batches_per_epoch: 98
2022-08-04 18:07:57,535 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 94.802296, lr: 0.100000, avg loss: 0.145224
2022-08-04 18:07:59,152 [dl_trainer.py:822] INFO Epoch 81, lr: 0.100000, val loss: 0.458272, val top-1 acc: 87.629379, top-5 acc: 99.522293
2022-08-04 18:07:59,894 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:07:59,894 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:00,341 [dl_trainer.py:731] WARNING [ 81][ 7960/   98][rank:0] loss: 0.072, average forward (0.010167) and backward (0.019918) time: 0.076712, iotime: 0.005844 
2022-08-04 18:08:02,044 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079092, Speed: 809.182351 images/s
2022-08-04 18:08:02,259 [dl_trainer.py:731] WARNING [ 81][ 8000/   98][rank:0] loss: 0.106, average forward (0.011862) and backward (0.020941) time: 0.035108, iotime: 0.001994 
2022-08-04 18:08:03,949 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047625, Speed: 1343.829182 images/s
2022-08-04 18:08:03,996 [dl_trainer.py:634] INFO train iter: 8036, num_batches_per_epoch: 98
2022-08-04 18:08:03,996 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 96.572066, lr: 0.010000, avg loss: 0.102697
2022-08-04 18:08:05,620 [dl_trainer.py:822] INFO Epoch 82, lr: 0.010000, val loss: 0.335117, val top-1 acc: 90.445860, top-5 acc: 99.681529
2022-08-04 18:08:05,869 [dl_trainer.py:731] WARNING [ 82][ 8040/   98][rank:0] loss: 0.127, average forward (0.011078) and backward (0.021530) time: 0.076732, iotime: 0.001804 
2022-08-04 18:08:06,412 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:06,413 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:07,942 [dl_trainer.py:731] WARNING [ 82][ 8080/   98][rank:0] loss: 0.101, average forward (0.011460) and backward (0.022115) time: 0.040108, iotime: 0.006247 
2022-08-04 18:08:08,533 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080396, Speed: 796.054787 images/s
2022-08-04 18:08:09,862 [dl_trainer.py:731] WARNING [ 82][ 8120/   98][rank:0] loss: 0.047, average forward (0.010615) and backward (0.020678) time: 0.033350, iotime: 0.001795 
2022-08-04 18:08:10,446 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047814, Speed: 1338.529286 images/s
2022-08-04 18:08:10,535 [dl_trainer.py:634] INFO train iter: 8134, num_batches_per_epoch: 98
2022-08-04 18:08:10,535 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 97.257653, lr: 0.010000, avg loss: 0.085370
2022-08-04 18:08:12,127 [dl_trainer.py:822] INFO Epoch 83, lr: 0.010000, val loss: 0.339842, val top-1 acc: 90.744427, top-5 acc: 99.691481
2022-08-04 18:08:12,806 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:12,806 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:13,530 [dl_trainer.py:731] WARNING [ 83][ 8160/   98][rank:0] loss: 0.024, average forward (0.010813) and backward (0.021719) time: 0.078714, iotime: 0.006072 
2022-08-04 18:08:14,914 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078367, Speed: 816.670554 images/s
2022-08-04 18:08:15,428 [dl_trainer.py:731] WARNING [ 83][ 8200/   98][rank:0] loss: 0.081, average forward (0.009713) and backward (0.020873) time: 0.032567, iotime: 0.001721 
2022-08-04 18:08:16,806 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047276, Speed: 1353.766366 images/s
2022-08-04 18:08:16,962 [dl_trainer.py:634] INFO train iter: 8232, num_batches_per_epoch: 98
2022-08-04 18:08:16,962 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 97.225765, lr: 0.010000, avg loss: 0.082165
2022-08-04 18:08:18,611 [dl_trainer.py:822] INFO Epoch 84, lr: 0.010000, val loss: 0.339511, val top-1 acc: 90.724522, top-5 acc: 99.711385
2022-08-04 18:08:19,210 [dl_trainer.py:731] WARNING [ 84][ 8240/   98][rank:0] loss: 0.027, average forward (0.010327) and backward (0.019909) time: 0.079520, iotime: 0.001786 
2022-08-04 18:08:19,488 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:19,488 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:21,312 [dl_trainer.py:731] WARNING [ 84][ 8280/   98][rank:0] loss: 0.091, average forward (0.010790) and backward (0.021597) time: 0.038670, iotime: 0.006008 
2022-08-04 18:08:21,626 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.084543, Speed: 757.013265 images/s
2022-08-04 18:08:23,215 [dl_trainer.py:731] WARNING [ 84][ 8320/   98][rank:0] loss: 0.036, average forward (0.009244) and backward (0.021503) time: 0.032627, iotime: 0.001636 
2022-08-04 18:08:23,507 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047016, Speed: 1361.233011 images/s
2022-08-04 18:08:23,708 [dl_trainer.py:634] INFO train iter: 8330, num_batches_per_epoch: 98
2022-08-04 18:08:23,708 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 97.943240, lr: 0.010000, avg loss: 0.063719
2022-08-04 18:08:25,343 [dl_trainer.py:822] INFO Epoch 85, lr: 0.010000, val loss: 0.342270, val top-1 acc: 90.873806, top-5 acc: 99.701433
2022-08-04 18:08:25,927 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:25,927 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:26,933 [dl_trainer.py:731] WARNING [ 85][ 8360/   98][rank:0] loss: 0.083, average forward (0.010343) and backward (0.020116) time: 0.077472, iotime: 0.005808 
2022-08-04 18:08:28,055 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079766, Speed: 802.347521 images/s
2022-08-04 18:08:28,862 [dl_trainer.py:731] WARNING [ 85][ 8400/   98][rank:0] loss: 0.063, average forward (0.010427) and backward (0.022529) time: 0.034967, iotime: 0.001743 
2022-08-04 18:08:29,987 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048270, Speed: 1325.868435 images/s
2022-08-04 18:08:30,233 [dl_trainer.py:634] INFO train iter: 8428, num_batches_per_epoch: 98
2022-08-04 18:08:30,234 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 97.767857, lr: 0.010000, avg loss: 0.069220
2022-08-04 18:08:32,002 [dl_trainer.py:822] INFO Epoch 86, lr: 0.010000, val loss: 0.341491, val top-1 acc: 90.893710, top-5 acc: 99.711385
2022-08-04 18:08:32,566 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:32,566 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:32,775 [dl_trainer.py:731] WARNING [ 86][ 8440/   98][rank:0] loss: 0.022, average forward (0.009797) and backward (0.021888) time: 0.082929, iotime: 0.006060 
2022-08-04 18:08:34,734 [dl_trainer.py:731] WARNING [ 86][ 8480/   98][rank:0] loss: 0.033, average forward (0.010580) and backward (0.020595) time: 0.033179, iotime: 0.001700 
2022-08-04 18:08:34,738 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083338, Speed: 767.954130 images/s
2022-08-04 18:08:36,657 [dl_trainer.py:731] WARNING [ 86][ 8520/   98][rank:0] loss: 0.107, average forward (0.009512) and backward (0.020078) time: 0.031456, iotime: 0.001606 
2022-08-04 18:08:36,669 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048256, Speed: 1326.260005 images/s
2022-08-04 18:08:36,969 [dl_trainer.py:634] INFO train iter: 8526, num_batches_per_epoch: 98
2022-08-04 18:08:36,969 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 97.831633, lr: 0.010000, avg loss: 0.066995
2022-08-04 18:08:38,651 [dl_trainer.py:822] INFO Epoch 87, lr: 0.010000, val loss: 0.347746, val top-1 acc: 90.943471, top-5 acc: 99.691481
2022-08-04 18:08:39,145 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:39,145 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:40,473 [dl_trainer.py:731] WARNING [ 87][ 8560/   98][rank:0] loss: 0.066, average forward (0.010468) and backward (0.021455) time: 0.080431, iotime: 0.006112 
2022-08-04 18:08:41,318 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081537, Speed: 784.920334 images/s
2022-08-04 18:08:42,354 [dl_trainer.py:731] WARNING [ 87][ 8600/   98][rank:0] loss: 0.127, average forward (0.008959) and backward (0.017033) time: 0.027770, iotime: 0.001555 
2022-08-04 18:08:43,173 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046343, Speed: 1381.001194 images/s
2022-08-04 18:08:43,484 [dl_trainer.py:634] INFO train iter: 8624, num_batches_per_epoch: 98
2022-08-04 18:08:43,484 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 98.086735, lr: 0.010000, avg loss: 0.062320
2022-08-04 18:08:45,055 [dl_trainer.py:822] INFO Epoch 88, lr: 0.010000, val loss: 0.349860, val top-1 acc: 90.833997, top-5 acc: 99.701433
2022-08-04 18:08:45,498 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:45,498 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:45,998 [dl_trainer.py:731] WARNING [ 88][ 8640/   98][rank:0] loss: 0.021, average forward (0.008872) and backward (0.020085) time: 0.075611, iotime: 0.006034 
2022-08-04 18:08:47,636 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078288, Speed: 817.497275 images/s
2022-08-04 18:08:47,919 [dl_trainer.py:731] WARNING [ 88][ 8680/   98][rank:0] loss: 0.024, average forward (0.009046) and backward (0.021508) time: 0.032377, iotime: 0.001594 
2022-08-04 18:08:49,567 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048256, Speed: 1326.267377 images/s
2022-08-04 18:08:49,845 [dl_trainer.py:731] WARNING [ 88][ 8720/   98][rank:0] loss: 0.024, average forward (0.010369) and backward (0.021508) time: 0.033876, iotime: 0.001742 
2022-08-04 18:08:49,951 [dl_trainer.py:634] INFO train iter: 8722, num_batches_per_epoch: 98
2022-08-04 18:08:49,952 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 97.895408, lr: 0.010000, avg loss: 0.062549
2022-08-04 18:08:51,517 [dl_trainer.py:822] INFO Epoch 89, lr: 0.010000, val loss: 0.347896, val top-1 acc: 90.873806, top-5 acc: 99.691481
2022-08-04 18:08:51,914 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:51,914 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:53,414 [dl_trainer.py:731] WARNING [ 89][ 8760/   98][rank:0] loss: 0.023, average forward (0.011553) and backward (0.019320) time: 0.076740, iotime: 0.006373 
2022-08-04 18:08:53,981 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.077424, Speed: 826.621098 images/s
2022-08-04 18:08:55,360 [dl_trainer.py:731] WARNING [ 89][ 8800/   98][rank:0] loss: 0.107, average forward (0.011934) and backward (0.022712) time: 0.036883, iotime: 0.001950 
2022-08-04 18:08:55,917 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048380, Speed: 1322.847502 images/s
2022-08-04 18:08:56,361 [dl_trainer.py:634] INFO train iter: 8820, num_batches_per_epoch: 98
2022-08-04 18:08:56,361 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 98.102679, lr: 0.010000, avg loss: 0.058546
2022-08-04 18:08:57,946 [dl_trainer.py:822] INFO Epoch 90, lr: 0.010000, val loss: 0.356900, val top-1 acc: 90.804140, top-5 acc: 99.681529
2022-08-04 18:08:58,331 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:08:58,332 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:08:59,115 [dl_trainer.py:731] WARNING [ 90][ 8840/   98][rank:0] loss: 0.073, average forward (0.010167) and backward (0.020810) time: 0.078133, iotime: 0.006080 
2022-08-04 18:09:00,475 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079937, Speed: 800.629135 images/s
2022-08-04 18:09:01,052 [dl_trainer.py:731] WARNING [ 90][ 8880/   98][rank:0] loss: 0.052, average forward (0.008803) and backward (0.019758) time: 0.030279, iotime: 0.001492 
2022-08-04 18:09:02,431 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048911, Speed: 1308.492432 images/s
2022-08-04 18:09:02,936 [dl_trainer.py:634] INFO train iter: 8918, num_batches_per_epoch: 98
2022-08-04 18:09:02,937 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 97.847577, lr: 0.010000, avg loss: 0.061025
2022-08-04 18:09:04,584 [dl_trainer.py:822] INFO Epoch 91, lr: 0.010000, val loss: 0.350868, val top-1 acc: 90.993232, top-5 acc: 99.711385
2022-08-04 18:09:04,688 [dl_trainer.py:731] WARNING [ 91][ 8920/   98][rank:0] loss: 0.042, average forward (0.009405) and backward (0.020977) time: 0.073412, iotime: 0.001546 
2022-08-04 18:09:04,894 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:04,894 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:06,810 [dl_trainer.py:731] WARNING [ 91][ 8960/   98][rank:0] loss: 0.061, average forward (0.010391) and backward (0.022526) time: 0.039149, iotime: 0.005983 
2022-08-04 18:09:07,087 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081661, Speed: 783.723248 images/s
2022-08-04 18:09:08,780 [dl_trainer.py:731] WARNING [ 91][ 9000/   98][rank:0] loss: 0.054, average forward (0.011081) and backward (0.023309) time: 0.036482, iotime: 0.001829 
2022-08-04 18:09:09,004 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047917, Speed: 1335.629855 images/s
2022-08-04 18:09:09,524 [dl_trainer.py:634] INFO train iter: 9016, num_batches_per_epoch: 98
2022-08-04 18:09:09,524 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 98.166454, lr: 0.010000, avg loss: 0.056694
2022-08-04 18:09:11,109 [dl_trainer.py:822] INFO Epoch 92, lr: 0.010000, val loss: 0.349275, val top-1 acc: 90.993232, top-5 acc: 99.691481
2022-08-04 18:09:11,393 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:11,393 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:12,440 [dl_trainer.py:731] WARNING [ 92][ 9040/   98][rank:0] loss: 0.087, average forward (0.011992) and backward (0.020425) time: 0.079852, iotime: 0.006294 
2022-08-04 18:09:13,557 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079846, Speed: 801.545768 images/s
2022-08-04 18:09:14,445 [dl_trainer.py:731] WARNING [ 92][ 9080/   98][rank:0] loss: 0.041, average forward (0.010733) and backward (0.021835) time: 0.034570, iotime: 0.001729 
2022-08-04 18:09:15,524 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049154, Speed: 1302.018473 images/s
2022-08-04 18:09:16,088 [dl_trainer.py:634] INFO train iter: 9114, num_batches_per_epoch: 98
2022-08-04 18:09:16,088 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 98.517219, lr: 0.010000, avg loss: 0.050047
2022-08-04 18:09:17,683 [dl_trainer.py:822] INFO Epoch 93, lr: 0.010000, val loss: 0.360292, val top-1 acc: 90.843949, top-5 acc: 99.721338
2022-08-04 18:09:17,896 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:17,896 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:18,165 [dl_trainer.py:731] WARNING [ 93][ 9120/   98][rank:0] loss: 0.039, average forward (0.009774) and backward (0.022530) time: 0.078203, iotime: 0.005721 
2022-08-04 18:09:20,076 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079840, Speed: 801.602878 images/s
2022-08-04 18:09:20,100 [dl_trainer.py:731] WARNING [ 93][ 9160/   98][rank:0] loss: 0.017, average forward (0.008543) and backward (0.019213) time: 0.029678, iotime: 0.001608 
2022-08-04 18:09:22,057 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049500, Speed: 1292.942203 images/s
2022-08-04 18:09:22,081 [dl_trainer.py:731] WARNING [ 93][ 9200/   98][rank:0] loss: 0.053, average forward (0.009567) and backward (0.019722) time: 0.031136, iotime: 0.001596 
2022-08-04 18:09:22,707 [dl_trainer.py:634] INFO train iter: 9212, num_batches_per_epoch: 98
2022-08-04 18:09:22,708 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 98.038903, lr: 0.010000, avg loss: 0.057459
2022-08-04 18:09:24,308 [dl_trainer.py:822] INFO Epoch 94, lr: 0.010000, val loss: 0.353736, val top-1 acc: 91.042994, top-5 acc: 99.701433
2022-08-04 18:09:24,505 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:24,505 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:25,870 [dl_trainer.py:731] WARNING [ 94][ 9240/   98][rank:0] loss: 0.045, average forward (0.009890) and backward (0.020638) time: 0.078073, iotime: 0.006025 
2022-08-04 18:09:26,673 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080965, Speed: 790.467627 images/s
2022-08-04 18:09:27,794 [dl_trainer.py:731] WARNING [ 94][ 9280/   98][rank:0] loss: 0.065, average forward (0.010515) and backward (0.022031) time: 0.034513, iotime: 0.001714 
2022-08-04 18:09:28,581 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047688, Speed: 1342.062366 images/s
2022-08-04 18:09:29,265 [dl_trainer.py:634] INFO train iter: 9310, num_batches_per_epoch: 98
2022-08-04 18:09:29,266 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 98.246173, lr: 0.010000, avg loss: 0.053964
2022-08-04 18:09:30,819 [dl_trainer.py:822] INFO Epoch 95, lr: 0.010000, val loss: 0.355866, val top-1 acc: 91.033041, top-5 acc: 99.691481
2022-08-04 18:09:30,927 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:30,928 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:31,474 [dl_trainer.py:731] WARNING [ 95][ 9320/   98][rank:0] loss: 0.028, average forward (0.011225) and backward (0.020820) time: 0.077325, iotime: 0.006100 
2022-08-04 18:09:33,085 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078992, Speed: 810.209843 images/s
2022-08-04 18:09:33,378 [dl_trainer.py:731] WARNING [ 95][ 9360/   98][rank:0] loss: 0.007, average forward (0.009223) and backward (0.020971) time: 0.032052, iotime: 0.001621 
2022-08-04 18:09:34,983 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047423, Speed: 1349.560218 images/s
2022-08-04 18:09:35,325 [dl_trainer.py:731] WARNING [ 95][ 9400/   98][rank:0] loss: 0.131, average forward (0.009753) and backward (0.020437) time: 0.032036, iotime: 0.001592 
2022-08-04 18:09:35,728 [dl_trainer.py:634] INFO train iter: 9408, num_batches_per_epoch: 98
2022-08-04 18:09:35,729 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 98.469388, lr: 0.010000, avg loss: 0.049806
2022-08-04 18:09:37,342 [dl_trainer.py:822] INFO Epoch 96, lr: 0.010000, val loss: 0.365050, val top-1 acc: 90.983280, top-5 acc: 99.751194
2022-08-04 18:09:37,427 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:37,428 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:39,112 [dl_trainer.py:731] WARNING [ 96][ 9440/   98][rank:0] loss: 0.041, average forward (0.010200) and backward (0.019487) time: 0.076882, iotime: 0.005722 
2022-08-04 18:09:39,624 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081400, Speed: 786.244976 images/s
2022-08-04 18:09:41,054 [dl_trainer.py:731] WARNING [ 96][ 9480/   98][rank:0] loss: 0.114, average forward (0.009493) and backward (0.019079) time: 0.030351, iotime: 0.001545 
2022-08-04 18:09:41,556 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048286, Speed: 1325.448464 images/s
2022-08-04 18:09:42,344 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:42,344 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:42,345 [dl_trainer.py:634] INFO train iter: 9506, num_batches_per_epoch: 98
2022-08-04 18:09:42,345 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 98.437500, lr: 0.010000, avg loss: 0.048537
2022-08-04 18:09:44,163 [dl_trainer.py:822] INFO Epoch 97, lr: 0.010000, val loss: 0.360901, val top-1 acc: 90.893710, top-5 acc: 99.721338
2022-08-04 18:09:44,996 [dl_trainer.py:731] WARNING [ 97][ 9520/   98][rank:0] loss: 0.045, average forward (0.011033) and backward (0.021776) time: 0.084497, iotime: 0.005883 
2022-08-04 18:09:46,277 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082807, Speed: 772.884318 images/s
2022-08-04 18:09:46,884 [dl_trainer.py:731] WARNING [ 97][ 9560/   98][rank:0] loss: 0.042, average forward (0.010574) and backward (0.020710) time: 0.033302, iotime: 0.001751 
2022-08-04 18:09:48,211 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048332, Speed: 1324.185081 images/s
2022-08-04 18:09:48,803 [dl_trainer.py:731] WARNING [ 97][ 9600/   98][rank:0] loss: 0.047, average forward (0.010068) and backward (0.023185) time: 0.035240, iotime: 0.001740 
2022-08-04 18:09:48,966 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:48,966 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:49,189 [dl_trainer.py:634] INFO train iter: 9604, num_batches_per_epoch: 98
2022-08-04 18:09:49,189 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 98.565051, lr: 0.010000, avg loss: 0.044883
2022-08-04 18:09:50,750 [dl_trainer.py:822] INFO Epoch 98, lr: 0.010000, val loss: 0.368020, val top-1 acc: 90.923567, top-5 acc: 99.691481
2022-08-04 18:09:52,480 [dl_trainer.py:731] WARNING [ 98][ 9640/   98][rank:0] loss: 0.103, average forward (0.009382) and backward (0.022011) time: 0.077109, iotime: 0.005768 
2022-08-04 18:09:52,696 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078670, Speed: 813.526540 images/s
2022-08-04 18:09:54,431 [dl_trainer.py:731] WARNING [ 98][ 9680/   98][rank:0] loss: 0.018, average forward (0.009659) and backward (0.020836) time: 0.032418, iotime: 0.001669 
2022-08-04 18:09:54,657 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049019, Speed: 1305.624050 images/s
2022-08-04 18:09:55,450 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:09:55,451 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:09:55,732 [dl_trainer.py:634] INFO train iter: 9702, num_batches_per_epoch: 98
2022-08-04 18:09:55,733 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 98.517219, lr: 0.010000, avg loss: 0.043796
2022-08-04 18:09:57,311 [dl_trainer.py:822] INFO Epoch 99, lr: 0.010000, val loss: 0.365530, val top-1 acc: 91.013137, top-5 acc: 99.731290
2022-08-04 18:09:58,189 [dl_trainer.py:731] WARNING [ 99][ 9720/   98][rank:0] loss: 0.052, average forward (0.010123) and backward (0.022002) time: 0.078222, iotime: 0.006292 
2022-08-04 18:09:59,186 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079430, Speed: 805.736165 images/s
2022-08-04 18:10:00,071 [dl_trainer.py:731] WARNING [ 99][ 9760/   98][rank:0] loss: 0.035, average forward (0.011165) and backward (0.021152) time: 0.034398, iotime: 0.001812 
2022-08-04 18:10:01,084 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047427, Speed: 1349.440814 images/s
2022-08-04 18:10:01,815 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:01,815 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:02,134 [dl_trainer.py:731] WARNING [ 99][ 9800/   98][rank:0] loss: 0.047, average forward (0.010393) and backward (0.018997) time: 0.035558, iotime: 0.005902 
2022-08-04 18:10:02,141 [dl_trainer.py:634] INFO train iter: 9800, num_batches_per_epoch: 98
2022-08-04 18:10:02,142 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 98.357781, lr: 0.010000, avg loss: 0.051359
2022-08-04 18:10:03,857 [dl_trainer.py:822] INFO Epoch 100, lr: 0.010000, val loss: 0.365623, val top-1 acc: 90.853901, top-5 acc: 99.711385
2022-08-04 18:10:05,674 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080512, Speed: 794.913336 images/s
2022-08-04 18:10:05,766 [dl_trainer.py:731] WARNING [100][ 9840/   98][rank:0] loss: 0.116, average forward (0.009254) and backward (0.021633) time: 0.076784, iotime: 0.001622 
2022-08-04 18:10:07,612 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048420, Speed: 1321.770303 images/s
2022-08-04 18:10:07,704 [dl_trainer.py:731] WARNING [100][ 9880/   98][rank:0] loss: 0.037, average forward (0.012371) and backward (0.023596) time: 0.038281, iotime: 0.001986 
2022-08-04 18:10:08,396 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:08,396 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:08,751 [dl_trainer.py:634] INFO train iter: 9898, num_batches_per_epoch: 98
2022-08-04 18:10:08,751 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 98.262117, lr: 0.010000, avg loss: 0.052616
2022-08-04 18:10:10,321 [dl_trainer.py:822] INFO Epoch 101, lr: 0.010000, val loss: 0.378784, val top-1 acc: 90.843949, top-5 acc: 99.761146
2022-08-04 18:10:11,350 [dl_trainer.py:731] WARNING [101][ 9920/   98][rank:0] loss: 0.052, average forward (0.009888) and backward (0.020163) time: 0.075845, iotime: 0.006239 
2022-08-04 18:10:12,107 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078834, Speed: 811.837171 images/s
2022-08-04 18:10:13,268 [dl_trainer.py:731] WARNING [101][ 9960/   98][rank:0] loss: 0.029, average forward (0.010216) and backward (0.022115) time: 0.034337, iotime: 0.001753 
2022-08-04 18:10:14,000 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047326, Speed: 1352.330578 images/s
2022-08-04 18:10:14,788 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:14,789 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:15,191 [dl_trainer.py:634] INFO train iter: 9996, num_batches_per_epoch: 98
2022-08-04 18:10:15,192 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 98.373724, lr: 0.010000, avg loss: 0.049558
2022-08-04 18:10:16,780 [dl_trainer.py:822] INFO Epoch 102, lr: 0.010000, val loss: 0.370863, val top-1 acc: 90.843949, top-5 acc: 99.711385
2022-08-04 18:10:16,979 [dl_trainer.py:731] WARNING [102][10000/   98][rank:0] loss: 0.010, average forward (0.011056) and backward (0.021650) time: 0.079519, iotime: 0.006064 
2022-08-04 18:10:18,541 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079639, Speed: 803.628181 images/s
2022-08-04 18:10:18,929 [dl_trainer.py:731] WARNING [102][10040/   98][rank:0] loss: 0.018, average forward (0.009168) and backward (0.022310) time: 0.033293, iotime: 0.001584 
2022-08-04 18:10:20,513 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049303, Speed: 1298.100285 images/s
2022-08-04 18:10:20,899 [dl_trainer.py:731] WARNING [102][10080/   98][rank:0] loss: 0.151, average forward (0.009213) and backward (0.022160) time: 0.033227, iotime: 0.001621 
2022-08-04 18:10:21,308 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:21,309 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:21,775 [dl_trainer.py:634] INFO train iter: 10094, num_batches_per_epoch: 98
2022-08-04 18:10:21,776 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 98.421556, lr: 0.010000, avg loss: 0.046373
2022-08-04 18:10:23,345 [dl_trainer.py:822] INFO Epoch 103, lr: 0.010000, val loss: 0.374408, val top-1 acc: 90.814092, top-5 acc: 99.681529
2022-08-04 18:10:24,615 [dl_trainer.py:731] WARNING [103][10120/   98][rank:0] loss: 0.029, average forward (0.009660) and backward (0.020041) time: 0.075098, iotime: 0.005849 
2022-08-04 18:10:25,092 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080305, Speed: 796.966284 images/s
2022-08-04 18:10:26,571 [dl_trainer.py:731] WARNING [103][10160/   98][rank:0] loss: 0.008, average forward (0.008787) and backward (0.018802) time: 0.029404, iotime: 0.001586 
2022-08-04 18:10:26,997 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047625, Speed: 1343.821109 images/s
2022-08-04 18:10:27,793 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:27,793 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:28,301 [dl_trainer.py:634] INFO train iter: 10192, num_batches_per_epoch: 98
2022-08-04 18:10:28,301 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 98.724490, lr: 0.010000, avg loss: 0.042498
2022-08-04 18:10:29,884 [dl_trainer.py:822] INFO Epoch 104, lr: 0.010000, val loss: 0.372303, val top-1 acc: 90.863854, top-5 acc: 99.711385
2022-08-04 18:10:30,458 [dl_trainer.py:731] WARNING [104][10200/   98][rank:0] loss: 0.015, average forward (0.009981) and backward (0.019363) time: 0.080142, iotime: 0.005626 
2022-08-04 18:10:31,740 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083179, Speed: 769.425931 images/s
2022-08-04 18:10:32,429 [dl_trainer.py:731] WARNING [104][10240/   98][rank:0] loss: 0.040, average forward (0.009450) and backward (0.022348) time: 0.033663, iotime: 0.001617 
2022-08-04 18:10:33,692 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048771, Speed: 1312.245774 images/s
2022-08-04 18:10:34,357 [dl_trainer.py:731] WARNING [104][10280/   98][rank:0] loss: 0.043, average forward (0.010236) and backward (0.019404) time: 0.031622, iotime: 0.001719 
2022-08-04 18:10:34,469 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:34,469 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:35,044 [dl_trainer.py:634] INFO train iter: 10290, num_batches_per_epoch: 98
2022-08-04 18:10:35,045 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 98.389668, lr: 0.010000, avg loss: 0.045414
2022-08-04 18:10:36,658 [dl_trainer.py:822] INFO Epoch 105, lr: 0.010000, val loss: 0.375428, val top-1 acc: 90.724522, top-5 acc: 99.711385
2022-08-04 18:10:38,098 [dl_trainer.py:731] WARNING [105][10320/   98][rank:0] loss: 0.033, average forward (0.009912) and backward (0.021872) time: 0.078395, iotime: 0.005940 
2022-08-04 18:10:38,246 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079890, Speed: 801.103652 images/s
2022-08-04 18:10:39,994 [dl_trainer.py:731] WARNING [105][10360/   98][rank:0] loss: 0.051, average forward (0.009255) and backward (0.020829) time: 0.031866, iotime: 0.001552 
2022-08-04 18:10:40,157 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047760, Speed: 1340.021353 images/s
2022-08-04 18:10:40,943 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:40,944 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:41,547 [dl_trainer.py:634] INFO train iter: 10388, num_batches_per_epoch: 98
2022-08-04 18:10:41,548 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 98.804209, lr: 0.010000, avg loss: 0.041262
2022-08-04 18:10:43,156 [dl_trainer.py:822] INFO Epoch 106, lr: 0.010000, val loss: 0.377132, val top-1 acc: 90.734475, top-5 acc: 99.651672
2022-08-04 18:10:43,776 [dl_trainer.py:731] WARNING [106][10400/   98][rank:0] loss: 0.024, average forward (0.010790) and backward (0.021445) time: 0.079676, iotime: 0.006124 
2022-08-04 18:10:44,698 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079636, Speed: 803.657432 images/s
2022-08-04 18:10:45,649 [dl_trainer.py:731] WARNING [106][10440/   98][rank:0] loss: 0.084, average forward (0.010092) and backward (0.019407) time: 0.031474, iotime: 0.001730 
2022-08-04 18:10:46,603 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047604, Speed: 1344.437111 images/s
2022-08-04 18:10:47,353 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:47,353 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:47,710 [dl_trainer.py:731] WARNING [106][10480/   98][rank:0] loss: 0.084, average forward (0.010425) and backward (0.020520) time: 0.037681, iotime: 0.006476 
2022-08-04 18:10:48,039 [dl_trainer.py:634] INFO train iter: 10486, num_batches_per_epoch: 98
2022-08-04 18:10:48,040 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 98.325893, lr: 0.010000, avg loss: 0.046610
2022-08-04 18:10:49,613 [dl_trainer.py:822] INFO Epoch 107, lr: 0.010000, val loss: 0.376230, val top-1 acc: 91.102707, top-5 acc: 99.691481
2022-08-04 18:10:51,135 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079493, Speed: 805.098144 images/s
2022-08-04 18:10:51,256 [dl_trainer.py:731] WARNING [107][10520/   98][rank:0] loss: 0.047, average forward (0.009621) and backward (0.022854) time: 0.073913, iotime: 0.001642 
2022-08-04 18:10:53,061 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048133, Speed: 1329.638128 images/s
2022-08-04 18:10:53,209 [dl_trainer.py:731] WARNING [107][10560/   98][rank:0] loss: 0.055, average forward (0.010028) and backward (0.020363) time: 0.032354, iotime: 0.001690 
2022-08-04 18:10:53,842 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:10:53,842 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:10:54,513 [dl_trainer.py:634] INFO train iter: 10584, num_batches_per_epoch: 98
2022-08-04 18:10:54,514 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 98.724490, lr: 0.010000, avg loss: 0.041065
2022-08-04 18:10:56,183 [dl_trainer.py:822] INFO Epoch 108, lr: 0.010000, val loss: 0.378627, val top-1 acc: 90.913615, top-5 acc: 99.691481
2022-08-04 18:10:56,941 [dl_trainer.py:731] WARNING [108][10600/   98][rank:0] loss: 0.082, average forward (0.010972) and backward (0.020070) time: 0.079769, iotime: 0.005924 
2022-08-04 18:10:57,635 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080211, Speed: 797.895847 images/s
2022-08-04 18:10:58,869 [dl_trainer.py:731] WARNING [108][10640/   98][rank:0] loss: 0.014, average forward (0.009615) and backward (0.022273) time: 0.033831, iotime: 0.001689 
2022-08-04 18:10:59,557 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048038, Speed: 1332.287708 images/s
2022-08-04 18:11:00,319 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:00,320 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:00,966 [dl_trainer.py:731] WARNING [108][10680/   98][rank:0] loss: 0.031, average forward (0.009205) and backward (0.021640) time: 0.036934, iotime: 0.005839 
2022-08-04 18:11:01,093 [dl_trainer.py:634] INFO train iter: 10682, num_batches_per_epoch: 98
2022-08-04 18:11:01,094 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 98.580995, lr: 0.010000, avg loss: 0.047726
2022-08-04 18:11:02,711 [dl_trainer.py:822] INFO Epoch 109, lr: 0.010000, val loss: 0.385640, val top-1 acc: 90.804140, top-5 acc: 99.681529
2022-08-04 18:11:04,137 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080330, Speed: 796.712358 images/s
2022-08-04 18:11:04,572 [dl_trainer.py:731] WARNING [109][10720/   98][rank:0] loss: 0.026, average forward (0.010524) and backward (0.021803) time: 0.074931, iotime: 0.001805 
2022-08-04 18:11:06,073 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048401, Speed: 1322.277174 images/s
2022-08-04 18:11:06,505 [dl_trainer.py:731] WARNING [109][10760/   98][rank:0] loss: 0.062, average forward (0.009780) and backward (0.019120) time: 0.030871, iotime: 0.001713 
2022-08-04 18:11:06,873 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:06,873 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:07,646 [dl_trainer.py:634] INFO train iter: 10780, num_batches_per_epoch: 98
2022-08-04 18:11:07,647 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 98.485332, lr: 0.010000, avg loss: 0.044436
2022-08-04 18:11:09,291 [dl_trainer.py:822] INFO Epoch 110, lr: 0.010000, val loss: 0.382169, val top-1 acc: 90.853901, top-5 acc: 99.701433
2022-08-04 18:11:10,272 [dl_trainer.py:731] WARNING [110][10800/   98][rank:0] loss: 0.013, average forward (0.010302) and backward (0.021061) time: 0.079341, iotime: 0.005860 
2022-08-04 18:11:10,660 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080442, Speed: 795.599923 images/s
2022-08-04 18:11:12,166 [dl_trainer.py:731] WARNING [110][10840/   98][rank:0] loss: 0.088, average forward (0.009528) and backward (0.020007) time: 0.031552, iotime: 0.001753 
2022-08-04 18:11:12,543 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047061, Speed: 1359.950300 images/s
2022-08-04 18:11:13,324 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:13,324 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:14,153 [dl_trainer.py:634] INFO train iter: 10878, num_batches_per_epoch: 98
2022-08-04 18:11:14,154 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 98.772321, lr: 0.010000, avg loss: 0.040979
2022-08-04 18:11:15,843 [dl_trainer.py:822] INFO Epoch 111, lr: 0.010000, val loss: 0.393595, val top-1 acc: 90.734475, top-5 acc: 99.701433
2022-08-04 18:11:15,933 [dl_trainer.py:731] WARNING [111][10880/   98][rank:0] loss: 0.037, average forward (0.010917) and backward (0.021177) time: 0.080691, iotime: 0.005992 
2022-08-04 18:11:17,152 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080835, Speed: 791.731834 images/s
2022-08-04 18:11:17,836 [dl_trainer.py:731] WARNING [111][10920/   98][rank:0] loss: 0.008, average forward (0.011022) and backward (0.021865) time: 0.035018, iotime: 0.001857 
2022-08-04 18:11:19,117 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049124, Speed: 1302.837768 images/s
2022-08-04 18:11:19,880 [dl_trainer.py:731] WARNING [111][10960/   98][rank:0] loss: 0.088, average forward (0.011386) and backward (0.024850) time: 0.038440, iotime: 0.001896 
2022-08-04 18:11:19,936 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:19,936 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:20,812 [dl_trainer.py:634] INFO train iter: 10976, num_batches_per_epoch: 98
2022-08-04 18:11:20,813 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 98.692602, lr: 0.010000, avg loss: 0.041676
2022-08-04 18:11:22,417 [dl_trainer.py:822] INFO Epoch 112, lr: 0.010000, val loss: 0.387231, val top-1 acc: 91.042994, top-5 acc: 99.661624
2022-08-04 18:11:23,751 [dl_trainer.py:731] WARNING [112][11000/   98][rank:0] loss: 0.030, average forward (0.010057) and backward (0.019476) time: 0.080524, iotime: 0.006234 
2022-08-04 18:11:23,853 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083047, Speed: 770.652013 images/s
2022-08-04 18:11:25,684 [dl_trainer.py:731] WARNING [112][11040/   98][rank:0] loss: 0.079, average forward (0.009729) and backward (0.022340) time: 0.034034, iotime: 0.001668 
2022-08-04 18:11:25,801 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048693, Speed: 1314.345360 images/s
2022-08-04 18:11:26,544 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:26,544 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:27,495 [dl_trainer.py:634] INFO train iter: 11074, num_batches_per_epoch: 98
2022-08-04 18:11:27,496 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 98.485332, lr: 0.010000, avg loss: 0.048083
2022-08-04 18:11:29,076 [dl_trainer.py:822] INFO Epoch 113, lr: 0.010000, val loss: 0.392213, val top-1 acc: 91.023089, top-5 acc: 99.701433
2022-08-04 18:11:29,354 [dl_trainer.py:731] WARNING [113][11080/   98][rank:0] loss: 0.035, average forward (0.010316) and backward (0.020426) time: 0.076452, iotime: 0.005860 
2022-08-04 18:11:30,280 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078559, Speed: 814.678559 images/s
2022-08-04 18:11:31,273 [dl_trainer.py:731] WARNING [113][11120/   98][rank:0] loss: 0.051, average forward (0.009431) and backward (0.021323) time: 0.032696, iotime: 0.001698 
2022-08-04 18:11:32,227 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048676, Speed: 1314.823688 images/s
2022-08-04 18:11:32,984 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:32,985 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:33,383 [dl_trainer.py:731] WARNING [113][11160/   98][rank:0] loss: 0.076, average forward (0.010197) and backward (0.022486) time: 0.038960, iotime: 0.005999 
2022-08-04 18:11:33,966 [dl_trainer.py:634] INFO train iter: 11172, num_batches_per_epoch: 98
2022-08-04 18:11:33,966 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 98.644770, lr: 0.010000, avg loss: 0.042996
2022-08-04 18:11:35,547 [dl_trainer.py:822] INFO Epoch 114, lr: 0.010000, val loss: 0.390953, val top-1 acc: 90.873806, top-5 acc: 99.691481
2022-08-04 18:11:36,739 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079127, Speed: 808.824798 images/s
2022-08-04 18:11:36,919 [dl_trainer.py:731] WARNING [114][11200/   98][rank:0] loss: 0.007, average forward (0.009799) and backward (0.018917) time: 0.071057, iotime: 0.001702 
2022-08-04 18:11:38,688 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048730, Speed: 1313.359866 images/s
2022-08-04 18:11:38,859 [dl_trainer.py:731] WARNING [114][11240/   98][rank:0] loss: 0.033, average forward (0.010081) and backward (0.021382) time: 0.033442, iotime: 0.001728 
2022-08-04 18:11:39,452 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:39,453 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:40,435 [dl_trainer.py:634] INFO train iter: 11270, num_batches_per_epoch: 98
2022-08-04 18:11:40,435 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 98.565051, lr: 0.010000, avg loss: 0.043158
2022-08-04 18:11:42,067 [dl_trainer.py:822] INFO Epoch 115, lr: 0.010000, val loss: 0.399016, val top-1 acc: 90.734475, top-5 acc: 99.681529
2022-08-04 18:11:42,539 [dl_trainer.py:731] WARNING [115][11280/   98][rank:0] loss: 0.021, average forward (0.010523) and backward (0.020695) time: 0.078397, iotime: 0.006039 
2022-08-04 18:11:43,186 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078890, Speed: 811.255604 images/s
2022-08-04 18:11:44,492 [dl_trainer.py:731] WARNING [115][11320/   98][rank:0] loss: 0.019, average forward (0.010333) and backward (0.022470) time: 0.034806, iotime: 0.001732 
2022-08-04 18:11:45,123 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048397, Speed: 1322.385304 images/s
2022-08-04 18:11:45,888 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:45,889 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:46,568 [dl_trainer.py:731] WARNING [115][11360/   98][rank:0] loss: 0.019, average forward (0.009386) and backward (0.022063) time: 0.037571, iotime: 0.005888 
2022-08-04 18:11:46,957 [dl_trainer.py:634] INFO train iter: 11368, num_batches_per_epoch: 98
2022-08-04 18:11:46,958 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 98.836097, lr: 0.010000, avg loss: 0.039121
2022-08-04 18:11:48,637 [dl_trainer.py:822] INFO Epoch 116, lr: 0.010000, val loss: 0.401345, val top-1 acc: 90.654857, top-5 acc: 99.721338
2022-08-04 18:11:49,753 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081216, Speed: 788.025792 images/s
2022-08-04 18:11:50,247 [dl_trainer.py:731] WARNING [116][11400/   98][rank:0] loss: 0.016, average forward (0.008379) and backward (0.020649) time: 0.073615, iotime: 0.001567 
2022-08-04 18:11:51,742 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049703, Speed: 1287.656148 images/s
2022-08-04 18:11:52,181 [dl_trainer.py:731] WARNING [116][11440/   98][rank:0] loss: 0.175, average forward (0.012459) and backward (0.023694) time: 0.038461, iotime: 0.002019 
2022-08-04 18:11:52,487 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:52,487 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:53,632 [dl_trainer.py:634] INFO train iter: 11466, num_batches_per_epoch: 98
2022-08-04 18:11:53,633 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 98.804209, lr: 0.010000, avg loss: 0.038390
2022-08-04 18:11:55,209 [dl_trainer.py:822] INFO Epoch 117, lr: 0.010000, val loss: 0.395701, val top-1 acc: 90.744427, top-5 acc: 99.701433
2022-08-04 18:11:55,865 [dl_trainer.py:731] WARNING [117][11480/   98][rank:0] loss: 0.005, average forward (0.010185) and backward (0.020251) time: 0.076037, iotime: 0.005854 
2022-08-04 18:11:56,224 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078623, Speed: 814.015130 images/s
2022-08-04 18:11:57,781 [dl_trainer.py:731] WARNING [117][11520/   98][rank:0] loss: 0.033, average forward (0.010171) and backward (0.022153) time: 0.034347, iotime: 0.001758 
2022-08-04 18:11:58,140 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047886, Speed: 1336.521123 images/s
2022-08-04 18:11:58,921 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:11:58,922 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:11:59,889 [dl_trainer.py:731] WARNING [117][11560/   98][rank:0] loss: 0.020, average forward (0.010512) and backward (0.021720) time: 0.038573, iotime: 0.006064 
2022-08-04 18:12:00,101 [dl_trainer.py:634] INFO train iter: 11564, num_batches_per_epoch: 98
2022-08-04 18:12:00,102 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 98.772321, lr: 0.010000, avg loss: 0.039302
2022-08-04 18:12:01,671 [dl_trainer.py:822] INFO Epoch 118, lr: 0.010000, val loss: 0.393805, val top-1 acc: 90.694666, top-5 acc: 99.681529
2022-08-04 18:12:02,654 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079168, Speed: 808.408914 images/s
2022-08-04 18:12:03,405 [dl_trainer.py:731] WARNING [118][11600/   98][rank:0] loss: 0.029, average forward (0.010220) and backward (0.022012) time: 0.073932, iotime: 0.001727 
2022-08-04 18:12:04,576 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048027, Speed: 1332.579873 images/s
2022-08-04 18:12:05,321 [dl_trainer.py:731] WARNING [118][11640/   98][rank:0] loss: 0.079, average forward (0.011529) and backward (0.021939) time: 0.035617, iotime: 0.001870 
2022-08-04 18:12:05,332 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:05,333 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:06,591 [dl_trainer.py:634] INFO train iter: 11662, num_batches_per_epoch: 98
2022-08-04 18:12:06,591 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 98.772321, lr: 0.010000, avg loss: 0.037042
2022-08-04 18:12:08,244 [dl_trainer.py:822] INFO Epoch 119, lr: 0.010000, val loss: 0.400468, val top-1 acc: 90.644904, top-5 acc: 99.671576
2022-08-04 18:12:09,099 [dl_trainer.py:731] WARNING [119][11680/   98][rank:0] loss: 0.011, average forward (0.009863) and backward (0.022288) time: 0.079566, iotime: 0.005752 
2022-08-04 18:12:09,156 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080326, Speed: 796.754218 images/s
2022-08-04 18:12:11,001 [dl_trainer.py:731] WARNING [119][11720/   98][rank:0] loss: 0.035, average forward (0.011767) and backward (0.021743) time: 0.035691, iotime: 0.001921 
2022-08-04 18:12:11,074 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047936, Speed: 1335.110706 images/s
2022-08-04 18:12:11,862 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:11,863 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:13,096 [dl_trainer.py:731] WARNING [119][11760/   98][rank:0] loss: 0.007, average forward (0.009834) and backward (0.020192) time: 0.036360, iotime: 0.006063 
2022-08-04 18:12:13,110 [dl_trainer.py:634] INFO train iter: 11760, num_batches_per_epoch: 98
2022-08-04 18:12:13,110 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 98.644770, lr: 0.010000, avg loss: 0.039446
2022-08-04 18:12:14,789 [dl_trainer.py:822] INFO Epoch 120, lr: 0.010000, val loss: 0.393164, val top-1 acc: 90.903662, top-5 acc: 99.701433
2022-08-04 18:12:15,717 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081454, Speed: 785.722645 images/s
2022-08-04 18:12:16,785 [dl_trainer.py:731] WARNING [120][11800/   98][rank:0] loss: 0.039, average forward (0.009121) and backward (0.019509) time: 0.074106, iotime: 0.001551 
2022-08-04 18:12:17,677 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048968, Speed: 1306.978232 images/s
2022-08-04 18:12:18,453 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:18,453 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:18,894 [dl_trainer.py:731] WARNING [120][11840/   98][rank:0] loss: 0.009, average forward (0.008683) and backward (0.020854) time: 0.035557, iotime: 0.005792 
2022-08-04 18:12:19,716 [dl_trainer.py:634] INFO train iter: 11858, num_batches_per_epoch: 98
2022-08-04 18:12:19,717 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 98.724490, lr: 0.010000, avg loss: 0.038832
2022-08-04 18:12:21,425 [dl_trainer.py:822] INFO Epoch 121, lr: 0.010000, val loss: 0.395317, val top-1 acc: 90.933519, top-5 acc: 99.661624
2022-08-04 18:12:22,204 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079408, Speed: 805.968238 images/s
2022-08-04 18:12:22,428 [dl_trainer.py:731] WARNING [121][11880/   98][rank:0] loss: 0.026, average forward (0.009493) and backward (0.020684) time: 0.074903, iotime: 0.001696 
2022-08-04 18:12:24,108 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047603, Speed: 1344.466234 images/s
2022-08-04 18:12:24,326 [dl_trainer.py:731] WARNING [121][11920/   98][rank:0] loss: 0.086, average forward (0.010225) and backward (0.021351) time: 0.033643, iotime: 0.001810 
2022-08-04 18:12:24,877 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:24,877 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:26,229 [dl_trainer.py:634] INFO train iter: 11956, num_batches_per_epoch: 98
2022-08-04 18:12:26,229 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 98.692602, lr: 0.010000, avg loss: 0.038621
2022-08-04 18:12:27,823 [dl_trainer.py:822] INFO Epoch 122, lr: 0.010000, val loss: 0.398650, val top-1 acc: 90.824045, top-5 acc: 99.661624
2022-08-04 18:12:28,055 [dl_trainer.py:731] WARNING [122][11960/   98][rank:0] loss: 0.138, average forward (0.009788) and backward (0.021696) time: 0.078422, iotime: 0.005876 
2022-08-04 18:12:28,637 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079423, Speed: 805.814837 images/s
2022-08-04 18:12:29,910 [dl_trainer.py:731] WARNING [122][12000/   98][rank:0] loss: 0.037, average forward (0.010065) and backward (0.019787) time: 0.031790, iotime: 0.001695 
2022-08-04 18:12:30,528 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047266, Speed: 1354.051465 images/s
2022-08-04 18:12:31,305 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:31,305 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:32,053 [dl_trainer.py:731] WARNING [122][12040/   98][rank:0] loss: 0.021, average forward (0.009280) and backward (0.018583) time: 0.034272, iotime: 0.006159 
2022-08-04 18:12:32,752 [dl_trainer.py:634] INFO train iter: 12054, num_batches_per_epoch: 98
2022-08-04 18:12:32,752 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 98.820153, lr: 0.001000, avg loss: 0.037957
2022-08-04 18:12:34,451 [dl_trainer.py:822] INFO Epoch 123, lr: 0.001000, val loss: 0.394269, val top-1 acc: 90.883758, top-5 acc: 99.681529
2022-08-04 18:12:35,194 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081854, Speed: 781.878131 images/s
2022-08-04 18:12:35,714 [dl_trainer.py:731] WARNING [123][12080/   98][rank:0] loss: 0.018, average forward (0.010368) and backward (0.020811) time: 0.075766, iotime: 0.001771 
2022-08-04 18:12:37,151 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048903, Speed: 1308.722090 images/s
2022-08-04 18:12:37,654 [dl_trainer.py:731] WARNING [123][12120/   98][rank:0] loss: 0.022, average forward (0.010116) and backward (0.021202) time: 0.033319, iotime: 0.001739 
2022-08-04 18:12:37,922 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:37,922 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:39,423 [dl_trainer.py:634] INFO train iter: 12152, num_batches_per_epoch: 98
2022-08-04 18:12:39,424 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 98.947704, lr: 0.001000, avg loss: 0.030365
2022-08-04 18:12:40,999 [dl_trainer.py:822] INFO Epoch 124, lr: 0.001000, val loss: 0.393102, val top-1 acc: 90.963376, top-5 acc: 99.711385
2022-08-04 18:12:41,430 [dl_trainer.py:731] WARNING [124][12160/   98][rank:0] loss: 0.036, average forward (0.009650) and backward (0.022092) time: 0.078452, iotime: 0.005821 
2022-08-04 18:12:41,732 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080340, Speed: 796.618572 images/s
2022-08-04 18:12:43,378 [dl_trainer.py:731] WARNING [124][12200/   98][rank:0] loss: 0.012, average forward (0.010119) and backward (0.021173) time: 0.033191, iotime: 0.001643 
2022-08-04 18:12:43,673 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048533, Speed: 1318.681184 images/s
2022-08-04 18:12:44,452 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:44,452 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:45,474 [dl_trainer.py:731] WARNING [124][12240/   98][rank:0] loss: 0.038, average forward (0.010406) and backward (0.021618) time: 0.038322, iotime: 0.006031 
2022-08-04 18:12:45,963 [dl_trainer.py:634] INFO train iter: 12250, num_batches_per_epoch: 98
2022-08-04 18:12:45,963 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 99.011480, lr: 0.001000, avg loss: 0.033691
2022-08-04 18:12:47,609 [dl_trainer.py:822] INFO Epoch 125, lr: 0.001000, val loss: 0.393705, val top-1 acc: 90.824045, top-5 acc: 99.701433
2022-08-04 18:12:48,238 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080071, Speed: 799.285852 images/s
2022-08-04 18:12:49,052 [dl_trainer.py:731] WARNING [125][12280/   98][rank:0] loss: 0.033, average forward (0.010339) and backward (0.022036) time: 0.075571, iotime: 0.001720 
2022-08-04 18:12:50,177 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048449, Speed: 1320.985047 images/s
2022-08-04 18:12:50,958 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:50,959 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:51,168 [dl_trainer.py:731] WARNING [125][12320/   98][rank:0] loss: 0.075, average forward (0.010325) and backward (0.020050) time: 0.036731, iotime: 0.006088 
2022-08-04 18:12:52,504 [dl_trainer.py:634] INFO train iter: 12348, num_batches_per_epoch: 98
2022-08-04 18:12:52,505 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 98.740434, lr: 0.001000, avg loss: 0.037377
2022-08-04 18:12:54,098 [dl_trainer.py:822] INFO Epoch 126, lr: 0.001000, val loss: 0.395484, val top-1 acc: 90.923567, top-5 acc: 99.711385
2022-08-04 18:12:54,812 [dl_trainer.py:731] WARNING [126][12360/   98][rank:0] loss: 0.030, average forward (0.009671) and backward (0.019832) time: 0.075849, iotime: 0.001691 
2022-08-04 18:12:54,832 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081652, Speed: 783.811212 images/s
2022-08-04 18:12:56,766 [dl_trainer.py:731] WARNING [126][12400/   98][rank:0] loss: 0.033, average forward (0.009293) and backward (0.022465) time: 0.033581, iotime: 0.001583 
2022-08-04 18:12:56,778 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048646, Speed: 1315.619688 images/s
2022-08-04 18:12:57,522 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:12:57,523 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:12:58,827 [dl_trainer.py:731] WARNING [126][12440/   98][rank:0] loss: 0.041, average forward (0.010318) and backward (0.019650) time: 0.036354, iotime: 0.006117 
2022-08-04 18:12:59,129 [dl_trainer.py:634] INFO train iter: 12446, num_batches_per_epoch: 98
2022-08-04 18:12:59,129 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 99.011480, lr: 0.001000, avg loss: 0.032777
2022-08-04 18:13:00,754 [dl_trainer.py:822] INFO Epoch 127, lr: 0.001000, val loss: 0.395548, val top-1 acc: 90.863854, top-5 acc: 99.671576
2022-08-04 18:13:01,290 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079128, Speed: 808.812570 images/s
2022-08-04 18:13:02,396 [dl_trainer.py:731] WARNING [127][12480/   98][rank:0] loss: 0.015, average forward (0.010037) and backward (0.023160) time: 0.075880, iotime: 0.001733 
2022-08-04 18:13:03,235 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048631, Speed: 1316.029582 images/s
2022-08-04 18:13:04,010 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:04,010 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:04,514 [dl_trainer.py:731] WARNING [127][12520/   98][rank:0] loss: 0.022, average forward (0.010058) and backward (0.020599) time: 0.037144, iotime: 0.006218 
2022-08-04 18:13:05,712 [dl_trainer.py:634] INFO train iter: 12544, num_batches_per_epoch: 98
2022-08-04 18:13:05,712 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 98.947704, lr: 0.001000, avg loss: 0.033676
2022-08-04 18:13:07,326 [dl_trainer.py:822] INFO Epoch 128, lr: 0.001000, val loss: 0.394452, val top-1 acc: 90.883758, top-5 acc: 99.681529
2022-08-04 18:13:07,968 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083008, Speed: 771.009201 images/s
2022-08-04 18:13:08,204 [dl_trainer.py:731] WARNING [128][12560/   98][rank:0] loss: 0.020, average forward (0.011209) and backward (0.023354) time: 0.080995, iotime: 0.001829 
2022-08-04 18:13:09,886 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047948, Speed: 1334.765826 images/s
2022-08-04 18:13:10,163 [dl_trainer.py:731] WARNING [128][12600/   98][rank:0] loss: 0.027, average forward (0.010735) and backward (0.021938) time: 0.034770, iotime: 0.001834 
2022-08-04 18:13:10,638 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:10,638 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:12,268 [dl_trainer.py:731] WARNING [128][12640/   98][rank:0] loss: 0.022, average forward (0.010708) and backward (0.020223) time: 0.037261, iotime: 0.006051 
2022-08-04 18:13:12,382 [dl_trainer.py:634] INFO train iter: 12642, num_batches_per_epoch: 98
2022-08-04 18:13:12,383 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 98.947704, lr: 0.001000, avg loss: 0.032867
2022-08-04 18:13:14,006 [dl_trainer.py:822] INFO Epoch 129, lr: 0.001000, val loss: 0.394572, val top-1 acc: 90.963376, top-5 acc: 99.681529
2022-08-04 18:13:14,442 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079895, Speed: 801.050136 images/s
2022-08-04 18:13:15,788 [dl_trainer.py:731] WARNING [129][12680/   98][rank:0] loss: 0.023, average forward (0.011374) and backward (0.021654) time: 0.075854, iotime: 0.001885 
2022-08-04 18:13:16,306 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046581, Speed: 1373.937388 images/s
2022-08-04 18:13:17,081 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:17,081 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:17,849 [dl_trainer.py:731] WARNING [129][12720/   98][rank:0] loss: 0.022, average forward (0.011132) and backward (0.020333) time: 0.037758, iotime: 0.006012 
2022-08-04 18:13:18,827 [dl_trainer.py:634] INFO train iter: 12740, num_batches_per_epoch: 98
2022-08-04 18:13:18,828 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 98.788265, lr: 0.001000, avg loss: 0.036809
2022-08-04 18:13:20,457 [dl_trainer.py:822] INFO Epoch 130, lr: 0.001000, val loss: 0.395535, val top-1 acc: 90.953424, top-5 acc: 99.671576
2022-08-04 18:13:20,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081134, Speed: 788.815800 images/s
2022-08-04 18:13:21,471 [dl_trainer.py:731] WARNING [130][12760/   98][rank:0] loss: 0.051, average forward (0.011337) and backward (0.020788) time: 0.077807, iotime: 0.001868 
2022-08-04 18:13:22,853 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048009, Speed: 1333.082160 images/s
2022-08-04 18:13:23,429 [dl_trainer.py:731] WARNING [130][12800/   98][rank:0] loss: 0.016, average forward (0.009175) and backward (0.020530) time: 0.031568, iotime: 0.001623 
2022-08-04 18:13:23,633 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:23,633 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:25,465 [dl_trainer.py:634] INFO train iter: 12838, num_batches_per_epoch: 98
2022-08-04 18:13:25,465 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 99.059311, lr: 0.001000, avg loss: 0.030796
2022-08-04 18:13:27,095 [dl_trainer.py:822] INFO Epoch 131, lr: 0.001000, val loss: 0.393558, val top-1 acc: 90.963376, top-5 acc: 99.671576
2022-08-04 18:13:27,184 [dl_trainer.py:731] WARNING [131][12840/   98][rank:0] loss: 0.034, average forward (0.011531) and backward (0.020497) time: 0.079659, iotime: 0.006513 
2022-08-04 18:13:27,453 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080676, Speed: 793.296938 images/s
2022-08-04 18:13:29,088 [dl_trainer.py:731] WARNING [131][12880/   98][rank:0] loss: 0.008, average forward (0.009587) and backward (0.019273) time: 0.030769, iotime: 0.001658 
2022-08-04 18:13:29,347 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047339, Speed: 1351.946615 images/s
2022-08-04 18:13:30,143 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:30,144 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:31,233 [dl_trainer.py:731] WARNING [131][12920/   98][rank:0] loss: 0.008, average forward (0.009568) and backward (0.021178) time: 0.036893, iotime: 0.005892 
2022-08-04 18:13:32,024 [dl_trainer.py:634] INFO train iter: 12936, num_batches_per_epoch: 98
2022-08-04 18:13:32,025 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 99.139031, lr: 0.001000, avg loss: 0.030893
2022-08-04 18:13:33,670 [dl_trainer.py:822] INFO Epoch 132, lr: 0.001000, val loss: 0.398338, val top-1 acc: 90.983280, top-5 acc: 99.701433
2022-08-04 18:13:34,005 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081690, Speed: 783.446118 images/s
2022-08-04 18:13:34,863 [dl_trainer.py:731] WARNING [132][12960/   98][rank:0] loss: 0.025, average forward (0.011187) and backward (0.021545) time: 0.077153, iotime: 0.001953 
2022-08-04 18:13:35,898 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047321, Speed: 1352.472129 images/s
2022-08-04 18:13:36,660 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:36,660 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:36,933 [dl_trainer.py:731] WARNING [132][13000/   98][rank:0] loss: 0.008, average forward (0.010676) and backward (0.019884) time: 0.037237, iotime: 0.006399 
2022-08-04 18:13:38,571 [dl_trainer.py:634] INFO train iter: 13034, num_batches_per_epoch: 98
2022-08-04 18:13:38,571 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 98.931760, lr: 0.001000, avg loss: 0.035573
2022-08-04 18:13:40,141 [dl_trainer.py:822] INFO Epoch 133, lr: 0.001000, val loss: 0.392826, val top-1 acc: 90.903662, top-5 acc: 99.691481
2022-08-04 18:13:40,382 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078636, Speed: 813.880946 images/s
2022-08-04 18:13:40,421 [dl_trainer.py:731] WARNING [133][13040/   98][rank:0] loss: 0.026, average forward (0.011099) and backward (0.021999) time: 0.074476, iotime: 0.001779 
2022-08-04 18:13:42,323 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048513, Speed: 1319.231556 images/s
2022-08-04 18:13:42,353 [dl_trainer.py:731] WARNING [133][13080/   98][rank:0] loss: 0.068, average forward (0.010301) and backward (0.022576) time: 0.034882, iotime: 0.001734 
2022-08-04 18:13:43,119 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:43,119 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:44,508 [dl_trainer.py:731] WARNING [133][13120/   98][rank:0] loss: 0.028, average forward (0.009788) and backward (0.022525) time: 0.038541, iotime: 0.005964 
2022-08-04 18:13:45,090 [dl_trainer.py:634] INFO train iter: 13132, num_batches_per_epoch: 98
2022-08-04 18:13:45,091 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 99.091199, lr: 0.001000, avg loss: 0.032411
2022-08-04 18:13:46,762 [dl_trainer.py:822] INFO Epoch 134, lr: 0.001000, val loss: 0.394654, val top-1 acc: 91.092755, top-5 acc: 99.681529
2022-08-04 18:13:46,993 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081906, Speed: 781.378781 images/s
2022-08-04 18:13:48,077 [dl_trainer.py:731] WARNING [134][13160/   98][rank:0] loss: 0.024, average forward (0.010169) and backward (0.020760) time: 0.075729, iotime: 0.001802 
2022-08-04 18:13:48,835 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046052, Speed: 1389.726242 images/s
2022-08-04 18:13:49,589 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:49,589 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:50,133 [dl_trainer.py:731] WARNING [134][13200/   98][rank:0] loss: 0.026, average forward (0.010176) and backward (0.020114) time: 0.036653, iotime: 0.006097 
2022-08-04 18:13:51,575 [dl_trainer.py:634] INFO train iter: 13230, num_batches_per_epoch: 98
2022-08-04 18:13:51,576 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 99.011480, lr: 0.001000, avg loss: 0.031747
2022-08-04 18:13:53,163 [dl_trainer.py:822] INFO Epoch 135, lr: 0.001000, val loss: 0.394996, val top-1 acc: 91.003185, top-5 acc: 99.681529
2022-08-04 18:13:53,323 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.078702, Speed: 813.190982 images/s
2022-08-04 18:13:53,652 [dl_trainer.py:731] WARNING [135][13240/   98][rank:0] loss: 0.010, average forward (0.009998) and backward (0.020872) time: 0.072626, iotime: 0.001747 
2022-08-04 18:13:55,211 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047188, Speed: 1356.264631 images/s
2022-08-04 18:13:55,556 [dl_trainer.py:731] WARNING [135][13280/   98][rank:0] loss: 0.050, average forward (0.011484) and backward (0.020304) time: 0.034006, iotime: 0.001924 
2022-08-04 18:13:56,012 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:13:56,012 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:13:57,639 [dl_trainer.py:731] WARNING [135][13320/   98][rank:0] loss: 0.027, average forward (0.012000) and backward (0.021008) time: 0.039429, iotime: 0.006148 
2022-08-04 18:13:58,031 [dl_trainer.py:634] INFO train iter: 13328, num_batches_per_epoch: 98
2022-08-04 18:13:58,031 [dl_trainer.py:635] INFO Epoch 136, avg train acc: 99.123087, lr: 0.001000, avg loss: 0.030491
2022-08-04 18:13:59,646 [dl_trainer.py:822] INFO Epoch 136, lr: 0.001000, val loss: 0.397159, val top-1 acc: 90.933519, top-5 acc: 99.691481
2022-08-04 18:13:59,760 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079786, Speed: 802.144441 images/s
2022-08-04 18:14:01,159 [dl_trainer.py:731] WARNING [136][13360/   98][rank:0] loss: 0.056, average forward (0.011241) and backward (0.021607) time: 0.075783, iotime: 0.001801 
2022-08-04 18:14:01,648 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047191, Speed: 1356.202619 images/s
2022-08-04 18:14:02,405 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:14:02,405 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:14:03,243 [dl_trainer.py:731] WARNING [136][13400/   98][rank:0] loss: 0.050, average forward (0.009177) and backward (0.020547) time: 0.036146, iotime: 0.006183 
2022-08-04 18:14:04,533 [dl_trainer.py:634] INFO train iter: 13426, num_batches_per_epoch: 98
2022-08-04 18:14:04,533 [dl_trainer.py:635] INFO Epoch 137, avg train acc: 99.123087, lr: 0.001000, avg loss: 0.030928
2022-08-04 18:14:06,196 [dl_trainer.py:822] INFO Epoch 137, lr: 0.001000, val loss: 0.395259, val top-1 acc: 90.943471, top-5 acc: 99.711385
2022-08-04 18:14:06,258 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080859, Speed: 791.502891 images/s
2022-08-04 18:14:06,855 [dl_trainer.py:731] WARNING [137][13440/   98][rank:0] loss: 0.045, average forward (0.009698) and backward (0.019313) time: 0.072600, iotime: 0.001678 
2022-08-04 18:14:08,175 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047904, Speed: 1336.003441 images/s
2022-08-04 18:14:08,789 [dl_trainer.py:731] WARNING [137][13480/   98][rank:0] loss: 0.015, average forward (0.010520) and backward (0.023483) time: 0.036065, iotime: 0.001787 
2022-08-04 18:14:08,928 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:14:08,928 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:14:10,829 [dl_trainer.py:731] WARNING [137][13520/   98][rank:0] loss: 0.024, average forward (0.009595) and backward (0.020517) time: 0.036441, iotime: 0.006081 
2022-08-04 18:14:11,038 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050210, Speed: 1274.634084 images/s
2022-08-04 18:14:11,039 [dl_trainer.py:634] INFO train iter: 13524, num_batches_per_epoch: 98
2022-08-04 18:14:11,039 [dl_trainer.py:635] INFO Epoch 138, avg train acc: 98.963648, lr: 0.001000, avg loss: 0.034829
2022-08-04 18:14:12,604 [dl_trainer.py:822] INFO Epoch 138, lr: 0.001000, val loss: 0.396439, val top-1 acc: 91.072850, top-5 acc: 99.711385
2022-08-04 18:14:14,352 [dl_trainer.py:731] WARNING [138][13560/   98][rank:0] loss: 0.029, average forward (0.009637) and backward (0.018722) time: 0.070345, iotime: 0.001671 
2022-08-04 18:14:14,559 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088025, Speed: 727.069689 images/s
2022-08-04 18:14:15,315 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:14:15,315 [distributed_optimizer.py:143] INFO The number of selected gradients: []
2022-08-04 18:14:16,442 [dl_trainer.py:731] WARNING [138][13600/   98][rank:0] loss: 0.007, average forward (0.010317) and backward (0.019816) time: 0.036174, iotime: 0.005762 
2022-08-04 18:14:17,478 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051187, Speed: 1250.308045 images/s
2022-08-04 18:14:17,528 [dl_trainer.py:634] INFO train iter: 13622, num_batches_per_epoch: 98
2022-08-04 18:14:17,528 [dl_trainer.py:635] INFO Epoch 139, avg train acc: 99.091199, lr: 0.001000, avg loss: 0.031342
2022-08-04 18:14:19,114 [dl_trainer.py:822] INFO Epoch 139, lr: 0.001000, val loss: 0.393188, val top-1 acc: 91.023089, top-5 acc: 99.701433
2022-08-04 18:14:19,979 [dl_trainer.py:731] WARNING [139][13640/   98][rank:0] loss: 0.022, average forward (0.010160) and backward (0.021182) time: 0.073111, iotime: 0.001799 
2022-08-04 18:14:21,031 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088792, Speed: 720.785251 images/s
2022-08-04 18:14:21,814 [distributed_optimizer.py:142] INFO Average number of selected gradients: nan, exact k: 269
2022-08-04 18:14:21,815 [distributed_optimizer.py:143] INFO The number of selected gradients: []

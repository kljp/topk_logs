2022-08-04 05:25:03,762 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=256, compressor='topk', data_dir='./data', dataset='cifar10', density=0.001, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 05:25:32,677 [dl_trainer.py:254] INFO num_batches_per_epoch: 25
2022-08-04 05:25:33,405 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 05:25:33,407 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 05:25:33,407 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 05:25:33,407 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 05:25:34,188 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 05:25:37,796 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.150322, Speed: 1703.015032 images/s
2022-08-04 05:25:37,797 [distributed_optimizer.py:142] INFO Average number of selected gradients: 4214.000000, exact k: 1078
2022-08-04 05:25:37,797 [distributed_optimizer.py:143] INFO The number of selected gradients: [4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214]
2022-08-04 05:25:38,078 [dl_trainer.py:634] INFO train iter: 25, num_batches_per_epoch: 25
2022-08-04 05:25:38,078 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 16.609375, lr: 0.020640, avg loss: 2.658201
2022-08-04 05:25:39,558 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020640, val loss: 2.692566, val top-1 acc: 14.482422, top-5 acc: 59.707031
2022-08-04 05:25:40,349 [dl_trainer.py:731] WARNING [  1][   40/   25][rank:0] loss: 2.070, average forward (0.064015) and backward (0.030513) time: 0.140524, iotime: 0.008388 
2022-08-04 05:25:40,782 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124364, Speed: 2058.470577 images/s
2022-08-04 05:25:40,783 [distributed_optimizer.py:142] INFO Average number of selected gradients: 1078.000000, exact k: 269
2022-08-04 05:25:40,783 [distributed_optimizer.py:143] INFO The number of selected gradients: [1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]
2022-08-04 05:25:41,136 [dl_trainer.py:634] INFO train iter: 50, num_batches_per_epoch: 25
2022-08-04 05:25:41,137 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 25.359375, lr: 0.040480, avg loss: 2.051829
2022-08-04 05:25:42,683 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040480, val loss: 3.502450, val top-1 acc: 17.509766, top-5 acc: 71.503906
2022-08-04 05:25:43,820 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126502, Speed: 2023.680603 images/s
2022-08-04 05:25:43,820 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:43,820 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:44,223 [dl_trainer.py:634] INFO train iter: 75, num_batches_per_epoch: 25
2022-08-04 05:25:44,223 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 27.093750, lr: 0.060320, avg loss: 1.931756
2022-08-04 05:25:45,702 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060320, val loss: 2.136756, val top-1 acc: 23.447266, top-5 acc: 77.480469
2022-08-04 05:25:45,969 [dl_trainer.py:731] WARNING [  3][   80/   25][rank:0] loss: 1.944, average forward (0.010010) and backward (0.025612) time: 0.126378, iotime: 0.014000 
2022-08-04 05:25:46,803 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124265, Speed: 2060.118651 images/s
2022-08-04 05:25:46,803 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:46,803 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:47,249 [dl_trainer.py:634] INFO train iter: 100, num_batches_per_epoch: 25
2022-08-04 05:25:47,250 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 27.531250, lr: 0.080160, avg loss: 1.963171
2022-08-04 05:25:48,838 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080160, val loss: 3.116102, val top-1 acc: 18.017578, top-5 acc: 70.117188
2022-08-04 05:25:49,922 [dl_trainer.py:731] WARNING [  4][  120/   25][rank:0] loss: 2.063, average forward (0.009269) and backward (0.026440) time: 0.084789, iotime: 0.008367 
2022-08-04 05:25:49,931 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.130290, Speed: 1964.840881 images/s
2022-08-04 05:25:49,931 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:49,931 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:50,453 [dl_trainer.py:634] INFO train iter: 125, num_batches_per_epoch: 25
2022-08-04 05:25:50,454 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 26.734375, lr: 0.100000, avg loss: 1.983328
2022-08-04 05:25:51,945 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 3.522312, val top-1 acc: 19.560547, top-5 acc: 75.820312
2022-08-04 05:25:52,890 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123276, Speed: 2076.635423 images/s
2022-08-04 05:25:52,891 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:52,891 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:53,432 [dl_trainer.py:634] INFO train iter: 150, num_batches_per_epoch: 25
2022-08-04 05:25:53,433 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 27.546875, lr: 0.100000, avg loss: 1.917916
2022-08-04 05:25:54,907 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 3.753260, val top-1 acc: 21.621094, top-5 acc: 74.638672
2022-08-04 05:25:55,407 [dl_trainer.py:731] WARNING [  6][  160/   25][rank:0] loss: 1.971, average forward (0.009722) and backward (0.026104) time: 0.125942, iotime: 0.014299 
2022-08-04 05:25:55,817 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121911, Speed: 2099.888451 images/s
2022-08-04 05:25:55,817 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:55,818 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:56,400 [dl_trainer.py:634] INFO train iter: 175, num_batches_per_epoch: 25
2022-08-04 05:25:56,401 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 29.171875, lr: 0.100000, avg loss: 1.878307
2022-08-04 05:25:57,867 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 2.110803, val top-1 acc: 30.126953, top-5 acc: 85.302734
2022-08-04 05:25:58,722 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121013, Speed: 2115.469542 images/s
2022-08-04 05:25:58,723 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:25:58,723 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:25:59,361 [dl_trainer.py:731] WARNING [  7][  200/   25][rank:0] loss: 1.850, average forward (0.009300) and backward (0.025467) time: 0.086132, iotime: 0.014354 
2022-08-04 05:25:59,373 [dl_trainer.py:634] INFO train iter: 200, num_batches_per_epoch: 25
2022-08-04 05:25:59,373 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 31.125000, lr: 0.100000, avg loss: 1.825897
2022-08-04 05:26:00,856 [dl_trainer.py:822] INFO Epoch 8, lr: 0.100000, val loss: 1.764928, val top-1 acc: 33.935547, top-5 acc: 87.802734
2022-08-04 05:26:01,658 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122236, Speed: 2094.304033 images/s
2022-08-04 05:26:01,659 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:01,659 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:02,354 [dl_trainer.py:634] INFO train iter: 225, num_batches_per_epoch: 25
2022-08-04 05:26:02,355 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 34.046875, lr: 0.100000, avg loss: 1.734441
2022-08-04 05:26:03,825 [dl_trainer.py:822] INFO Epoch 9, lr: 0.100000, val loss: 1.786182, val top-1 acc: 36.279297, top-5 acc: 88.535156
2022-08-04 05:26:04,589 [dl_trainer.py:731] WARNING [  9][  240/   25][rank:0] loss: 1.683, average forward (0.009562) and backward (0.026580) time: 0.119216, iotime: 0.008364 
2022-08-04 05:26:04,603 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122635, Speed: 2087.489715 images/s
2022-08-04 05:26:04,603 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:04,603 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:05,344 [dl_trainer.py:634] INFO train iter: 250, num_batches_per_epoch: 25
2022-08-04 05:26:05,344 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 36.328125, lr: 0.100000, avg loss: 1.712669
2022-08-04 05:26:06,857 [dl_trainer.py:822] INFO Epoch 10, lr: 0.100000, val loss: 1.795591, val top-1 acc: 35.458984, top-5 acc: 86.914062
2022-08-04 05:26:07,569 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123592, Speed: 2071.323333 images/s
2022-08-04 05:26:07,570 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:07,570 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:08,344 [dl_trainer.py:634] INFO train iter: 275, num_batches_per_epoch: 25
2022-08-04 05:26:08,345 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 37.546875, lr: 0.100000, avg loss: 1.679506
2022-08-04 05:26:09,805 [dl_trainer.py:822] INFO Epoch 11, lr: 0.100000, val loss: 1.876353, val top-1 acc: 34.316406, top-5 acc: 86.660156
2022-08-04 05:26:10,068 [dl_trainer.py:731] WARNING [ 11][  280/   25][rank:0] loss: 1.632, average forward (0.010129) and backward (0.026651) time: 0.125946, iotime: 0.013941 
2022-08-04 05:26:10,458 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.120312, Speed: 2127.803768 images/s
2022-08-04 05:26:10,458 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:10,458 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:11,280 [dl_trainer.py:634] INFO train iter: 300, num_batches_per_epoch: 25
2022-08-04 05:26:11,280 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 39.546875, lr: 0.100000, avg loss: 1.628559
2022-08-04 05:26:12,768 [dl_trainer.py:822] INFO Epoch 12, lr: 0.100000, val loss: 1.631745, val top-1 acc: 39.160156, top-5 acc: 89.892578
2022-08-04 05:26:13,415 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123180, Speed: 2078.264453 images/s
2022-08-04 05:26:13,415 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:13,415 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:14,062 [dl_trainer.py:731] WARNING [ 12][  320/   25][rank:0] loss: 1.530, average forward (0.009942) and backward (0.025767) time: 0.087474, iotime: 0.013759 
2022-08-04 05:26:14,342 [dl_trainer.py:634] INFO train iter: 325, num_batches_per_epoch: 25
2022-08-04 05:26:14,342 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 41.625000, lr: 0.100000, avg loss: 1.586444
2022-08-04 05:26:15,818 [dl_trainer.py:822] INFO Epoch 13, lr: 0.100000, val loss: 1.637241, val top-1 acc: 39.316406, top-5 acc: 90.146484
2022-08-04 05:26:16,419 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125133, Speed: 2045.822630 images/s
2022-08-04 05:26:16,419 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:16,419 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:17,343 [dl_trainer.py:634] INFO train iter: 350, num_batches_per_epoch: 25
2022-08-04 05:26:17,343 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 42.046875, lr: 0.100000, avg loss: 1.548398
2022-08-04 05:26:18,799 [dl_trainer.py:822] INFO Epoch 14, lr: 0.100000, val loss: 1.620472, val top-1 acc: 40.156250, top-5 acc: 90.869141
2022-08-04 05:26:19,344 [dl_trainer.py:731] WARNING [ 14][  360/   25][rank:0] loss: 1.566, average forward (0.009735) and backward (0.027303) time: 0.119741, iotime: 0.008529 
2022-08-04 05:26:19,354 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122289, Speed: 2093.396221 images/s
2022-08-04 05:26:19,355 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:19,355 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:20,294 [dl_trainer.py:634] INFO train iter: 375, num_batches_per_epoch: 25
2022-08-04 05:26:20,294 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 43.531250, lr: 0.100000, avg loss: 1.507786
2022-08-04 05:26:21,779 [dl_trainer.py:822] INFO Epoch 15, lr: 0.100000, val loss: 1.490672, val top-1 acc: 44.521484, top-5 acc: 92.099609
2022-08-04 05:26:22,242 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.120272, Speed: 2128.516790 images/s
2022-08-04 05:26:22,242 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:22,242 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:23,237 [dl_trainer.py:731] WARNING [ 15][  400/   25][rank:0] loss: 1.430, average forward (0.009252) and backward (0.026211) time: 0.087270, iotime: 0.014352 
2022-08-04 05:26:23,241 [dl_trainer.py:634] INFO train iter: 400, num_batches_per_epoch: 25
2022-08-04 05:26:23,242 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 43.984375, lr: 0.100000, avg loss: 1.513421
2022-08-04 05:26:24,726 [dl_trainer.py:822] INFO Epoch 16, lr: 0.100000, val loss: 1.571175, val top-1 acc: 42.802734, top-5 acc: 91.728516
2022-08-04 05:26:25,175 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122208, Speed: 2094.788714 images/s
2022-08-04 05:26:25,176 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:25,176 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:26,307 [dl_trainer.py:634] INFO train iter: 425, num_batches_per_epoch: 25
2022-08-04 05:26:26,308 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 46.593750, lr: 0.100000, avg loss: 1.486862
2022-08-04 05:26:27,784 [dl_trainer.py:822] INFO Epoch 17, lr: 0.100000, val loss: 1.469349, val top-1 acc: 45.937500, top-5 acc: 92.480469
2022-08-04 05:26:28,148 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123819, Speed: 2067.531681 images/s
2022-08-04 05:26:28,148 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:28,148 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:28,763 [dl_trainer.py:731] WARNING [ 17][  440/   25][rank:0] loss: 1.494, average forward (0.009668) and backward (0.026353) time: 0.125068, iotime: 0.013833 
2022-08-04 05:26:29,274 [dl_trainer.py:634] INFO train iter: 450, num_batches_per_epoch: 25
2022-08-04 05:26:29,274 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 46.000000, lr: 0.100000, avg loss: 1.469226
2022-08-04 05:26:30,745 [dl_trainer.py:822] INFO Epoch 18, lr: 0.100000, val loss: 1.510519, val top-1 acc: 44.296875, top-5 acc: 91.962891
2022-08-04 05:26:31,092 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122657, Speed: 2087.124359 images/s
2022-08-04 05:26:31,093 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:31,093 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:32,245 [dl_trainer.py:634] INFO train iter: 475, num_batches_per_epoch: 25
2022-08-04 05:26:32,246 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 48.093750, lr: 0.100000, avg loss: 1.425778
2022-08-04 05:26:33,751 [dl_trainer.py:822] INFO Epoch 19, lr: 0.100000, val loss: 1.555519, val top-1 acc: 45.390625, top-5 acc: 91.494141
2022-08-04 05:26:34,032 [dl_trainer.py:731] WARNING [ 19][  480/   25][rank:0] loss: 1.385, average forward (0.009004) and backward (0.026418) time: 0.119324, iotime: 0.008541 
2022-08-04 05:26:34,046 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123005, Speed: 2081.215492 images/s
2022-08-04 05:26:34,046 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:34,046 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:35,304 [dl_trainer.py:634] INFO train iter: 500, num_batches_per_epoch: 25
2022-08-04 05:26:35,305 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 48.343750, lr: 0.100000, avg loss: 1.414194
2022-08-04 05:26:36,773 [dl_trainer.py:822] INFO Epoch 20, lr: 0.100000, val loss: 1.462364, val top-1 acc: 45.615234, top-5 acc: 93.203125
2022-08-04 05:26:37,026 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124154, Speed: 2061.961394 images/s
2022-08-04 05:26:37,027 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:37,027 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:38,060 [dl_trainer.py:731] WARNING [ 20][  520/   25][rank:0] loss: 1.312, average forward (0.010464) and backward (0.025765) time: 0.089764, iotime: 0.015185 
2022-08-04 05:26:38,323 [dl_trainer.py:634] INFO train iter: 525, num_batches_per_epoch: 25
2022-08-04 05:26:38,323 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 49.890625, lr: 0.100000, avg loss: 1.381713
2022-08-04 05:26:39,831 [dl_trainer.py:822] INFO Epoch 21, lr: 0.100000, val loss: 1.389251, val top-1 acc: 49.589844, top-5 acc: 93.222656
2022-08-04 05:26:40,011 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124329, Speed: 2059.057591 images/s
2022-08-04 05:26:40,011 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:40,012 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:41,414 [dl_trainer.py:634] INFO train iter: 550, num_batches_per_epoch: 25
2022-08-04 05:26:41,414 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 50.953125, lr: 0.100000, avg loss: 1.348116
2022-08-04 05:26:42,893 [dl_trainer.py:822] INFO Epoch 22, lr: 0.100000, val loss: 1.341324, val top-1 acc: 51.347656, top-5 acc: 93.925781
2022-08-04 05:26:43,013 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125047, Speed: 2047.236125 images/s
2022-08-04 05:26:43,013 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:43,013 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:43,648 [dl_trainer.py:731] WARNING [ 22][  560/   25][rank:0] loss: 1.301, average forward (0.009796) and backward (0.027140) time: 0.126524, iotime: 0.013863 
2022-08-04 05:26:44,411 [dl_trainer.py:634] INFO train iter: 575, num_batches_per_epoch: 25
2022-08-04 05:26:44,411 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 51.718750, lr: 0.100000, avg loss: 1.304885
2022-08-04 05:26:45,947 [dl_trainer.py:822] INFO Epoch 23, lr: 0.100000, val loss: 1.316392, val top-1 acc: 52.626953, top-5 acc: 94.335938
2022-08-04 05:26:46,006 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124682, Speed: 2053.225048 images/s
2022-08-04 05:26:46,007 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:46,007 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:47,424 [dl_trainer.py:731] WARNING [ 23][  600/   25][rank:0] loss: 1.323, average forward (0.009445) and backward (0.025744) time: 0.083091, iotime: 0.008923 
2022-08-04 05:26:47,434 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.059464, Speed: 4305.159043 images/s
2022-08-04 05:26:47,435 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:47,435 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:47,436 [dl_trainer.py:634] INFO train iter: 600, num_batches_per_epoch: 25
2022-08-04 05:26:47,436 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 52.062500, lr: 0.100000, avg loss: 1.310182
2022-08-04 05:26:48,968 [dl_trainer.py:822] INFO Epoch 24, lr: 0.100000, val loss: 1.437182, val top-1 acc: 49.033203, top-5 acc: 93.662109
2022-08-04 05:26:50,446 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125443, Speed: 2040.765716 images/s
2022-08-04 05:26:50,447 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:50,447 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:50,751 [dl_trainer.py:634] INFO train iter: 625, num_batches_per_epoch: 25
2022-08-04 05:26:50,752 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 51.312500, lr: 0.100000, avg loss: 1.304890
2022-08-04 05:26:52,245 [dl_trainer.py:822] INFO Epoch 25, lr: 0.100000, val loss: 1.422836, val top-1 acc: 50.888672, top-5 acc: 93.427734
2022-08-04 05:26:53,003 [dl_trainer.py:731] WARNING [ 25][  640/   25][rank:0] loss: 1.344, average forward (0.009667) and backward (0.026980) time: 0.128031, iotime: 0.014356 
2022-08-04 05:26:53,407 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123314, Speed: 2075.995029 images/s
2022-08-04 05:26:53,407 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:53,407 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:53,772 [dl_trainer.py:634] INFO train iter: 650, num_batches_per_epoch: 25
2022-08-04 05:26:53,773 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 52.578125, lr: 0.100000, avg loss: 1.296259
2022-08-04 05:26:55,263 [dl_trainer.py:822] INFO Epoch 26, lr: 0.100000, val loss: 1.436998, val top-1 acc: 50.615234, top-5 acc: 93.359375
2022-08-04 05:26:56,401 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124735, Speed: 2052.348569 images/s
2022-08-04 05:26:56,401 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:56,402 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:56,833 [dl_trainer.py:634] INFO train iter: 675, num_batches_per_epoch: 25
2022-08-04 05:26:56,834 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 53.687500, lr: 0.100000, avg loss: 1.265567
2022-08-04 05:26:58,312 [dl_trainer.py:822] INFO Epoch 27, lr: 0.100000, val loss: 1.247869, val top-1 acc: 54.990234, top-5 acc: 94.931641
2022-08-04 05:26:58,560 [dl_trainer.py:731] WARNING [ 27][  680/   25][rank:0] loss: 1.286, average forward (0.009757) and backward (0.026326) time: 0.125721, iotime: 0.014323 
2022-08-04 05:26:59,333 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122125, Speed: 2096.214265 images/s
2022-08-04 05:26:59,333 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:26:59,333 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:26:59,830 [dl_trainer.py:634] INFO train iter: 700, num_batches_per_epoch: 25
2022-08-04 05:26:59,831 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 54.296875, lr: 0.100000, avg loss: 1.244013
2022-08-04 05:27:01,322 [dl_trainer.py:822] INFO Epoch 28, lr: 0.100000, val loss: 1.399092, val top-1 acc: 51.904297, top-5 acc: 94.160156
2022-08-04 05:27:02,319 [dl_trainer.py:731] WARNING [ 28][  720/   25][rank:0] loss: 1.324, average forward (0.009568) and backward (0.025664) time: 0.082744, iotime: 0.009171 
2022-08-04 05:27:02,334 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125005, Speed: 2047.923017 images/s
2022-08-04 05:27:02,334 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:02,334 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:02,870 [dl_trainer.py:634] INFO train iter: 725, num_batches_per_epoch: 25
2022-08-04 05:27:02,871 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 54.234375, lr: 0.100000, avg loss: 1.261889
2022-08-04 05:27:04,343 [dl_trainer.py:822] INFO Epoch 29, lr: 0.100000, val loss: 1.378942, val top-1 acc: 50.546875, top-5 acc: 93.935547
2022-08-04 05:27:05,309 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123947, Speed: 2065.391728 images/s
2022-08-04 05:27:05,310 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:05,310 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:05,853 [dl_trainer.py:634] INFO train iter: 750, num_batches_per_epoch: 25
2022-08-04 05:27:05,854 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 55.296875, lr: 0.100000, avg loss: 1.219809
2022-08-04 05:27:07,318 [dl_trainer.py:822] INFO Epoch 30, lr: 0.100000, val loss: 1.247151, val top-1 acc: 55.058594, top-5 acc: 95.029297
2022-08-04 05:27:07,851 [dl_trainer.py:731] WARNING [ 30][  760/   25][rank:0] loss: 1.225, average forward (0.009214) and backward (0.026448) time: 0.124345, iotime: 0.014072 
2022-08-04 05:27:08,326 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125658, Speed: 2037.277946 images/s
2022-08-04 05:27:08,326 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:08,327 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:08,943 [dl_trainer.py:634] INFO train iter: 775, num_batches_per_epoch: 25
2022-08-04 05:27:08,943 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 57.140625, lr: 0.100000, avg loss: 1.184794
2022-08-04 05:27:10,434 [dl_trainer.py:822] INFO Epoch 31, lr: 0.100000, val loss: 1.256110, val top-1 acc: 53.789062, top-5 acc: 95.107422
2022-08-04 05:27:11,325 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124911, Speed: 2049.455298 images/s
2022-08-04 05:27:11,325 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:11,325 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:11,975 [dl_trainer.py:731] WARNING [ 31][  800/   25][rank:0] loss: 1.205, average forward (0.009552) and backward (0.026781) time: 0.087604, iotime: 0.013630 
2022-08-04 05:27:11,994 [dl_trainer.py:634] INFO train iter: 800, num_batches_per_epoch: 25
2022-08-04 05:27:11,995 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 57.937500, lr: 0.100000, avg loss: 1.157631
2022-08-04 05:27:13,497 [dl_trainer.py:822] INFO Epoch 32, lr: 0.100000, val loss: 1.303065, val top-1 acc: 53.974609, top-5 acc: 95.029297
2022-08-04 05:27:14,389 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.127665, Speed: 2005.246969 images/s
2022-08-04 05:27:14,390 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:14,390 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:15,115 [dl_trainer.py:634] INFO train iter: 825, num_batches_per_epoch: 25
2022-08-04 05:27:15,115 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 58.656250, lr: 0.100000, avg loss: 1.156775
2022-08-04 05:27:16,599 [dl_trainer.py:822] INFO Epoch 33, lr: 0.100000, val loss: 1.189797, val top-1 acc: 57.666016, top-5 acc: 96.201172
2022-08-04 05:27:17,377 [dl_trainer.py:731] WARNING [ 33][  840/   25][rank:0] loss: 1.090, average forward (0.009893) and backward (0.026475) time: 0.120387, iotime: 0.008456 
2022-08-04 05:27:17,386 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124838, Speed: 2050.661168 images/s
2022-08-04 05:27:17,387 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:17,387 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:18,151 [dl_trainer.py:634] INFO train iter: 850, num_batches_per_epoch: 25
2022-08-04 05:27:18,152 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 59.843750, lr: 0.100000, avg loss: 1.114594
2022-08-04 05:27:19,652 [dl_trainer.py:822] INFO Epoch 34, lr: 0.100000, val loss: 1.245955, val top-1 acc: 57.294922, top-5 acc: 95.830078
2022-08-04 05:27:20,371 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124327, Speed: 2059.079309 images/s
2022-08-04 05:27:20,372 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:20,372 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:21,152 [dl_trainer.py:634] INFO train iter: 875, num_batches_per_epoch: 25
2022-08-04 05:27:21,153 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 60.828125, lr: 0.100000, avg loss: 1.081359
2022-08-04 05:27:22,617 [dl_trainer.py:822] INFO Epoch 35, lr: 0.100000, val loss: 1.211359, val top-1 acc: 57.861328, top-5 acc: 94.804688
2022-08-04 05:27:22,884 [dl_trainer.py:731] WARNING [ 35][  880/   25][rank:0] loss: 1.157, average forward (0.009963) and backward (0.026015) time: 0.125261, iotime: 0.013921 
2022-08-04 05:27:23,296 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121818, Speed: 2101.500540 images/s
2022-08-04 05:27:23,296 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:23,296 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:24,167 [dl_trainer.py:634] INFO train iter: 900, num_batches_per_epoch: 25
2022-08-04 05:27:24,167 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 59.406250, lr: 0.100000, avg loss: 1.118660
2022-08-04 05:27:25,646 [dl_trainer.py:822] INFO Epoch 36, lr: 0.100000, val loss: 1.358954, val top-1 acc: 53.320312, top-5 acc: 94.990234
2022-08-04 05:27:26,258 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123392, Speed: 2074.696884 images/s
2022-08-04 05:27:26,258 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:26,258 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:26,883 [dl_trainer.py:731] WARNING [ 36][  920/   25][rank:0] loss: 1.131, average forward (0.009751) and backward (0.025510) time: 0.088528, iotime: 0.015428 
2022-08-04 05:27:27,146 [dl_trainer.py:634] INFO train iter: 925, num_batches_per_epoch: 25
2022-08-04 05:27:27,146 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 59.421875, lr: 0.100000, avg loss: 1.135792
2022-08-04 05:27:28,629 [dl_trainer.py:822] INFO Epoch 37, lr: 0.100000, val loss: 1.286033, val top-1 acc: 54.833984, top-5 acc: 94.794922
2022-08-04 05:27:29,207 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122857, Speed: 2083.721762 images/s
2022-08-04 05:27:29,208 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:29,208 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:30,167 [dl_trainer.py:634] INFO train iter: 950, num_batches_per_epoch: 25
2022-08-04 05:27:30,167 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 61.484375, lr: 0.100000, avg loss: 1.085170
2022-08-04 05:27:31,640 [dl_trainer.py:822] INFO Epoch 38, lr: 0.100000, val loss: 1.072780, val top-1 acc: 61.982422, top-5 acc: 96.337891
2022-08-04 05:27:32,168 [dl_trainer.py:731] WARNING [ 38][  960/   25][rank:0] loss: 0.933, average forward (0.008859) and backward (0.027492) time: 0.119263, iotime: 0.008099 
2022-08-04 05:27:32,176 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123684, Speed: 2069.790774 images/s
2022-08-04 05:27:32,177 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:32,177 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:33,127 [dl_trainer.py:634] INFO train iter: 975, num_batches_per_epoch: 25
2022-08-04 05:27:33,127 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 61.734375, lr: 0.100000, avg loss: 1.066757
2022-08-04 05:27:34,610 [dl_trainer.py:822] INFO Epoch 39, lr: 0.100000, val loss: 1.115684, val top-1 acc: 61.220703, top-5 acc: 96.162109
2022-08-04 05:27:35,098 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121703, Speed: 2103.480244 images/s
2022-08-04 05:27:35,099 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:35,099 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:36,149 [dl_trainer.py:731] WARNING [ 39][ 1000/   25][rank:0] loss: 1.124, average forward (0.010057) and backward (0.025662) time: 0.087043, iotime: 0.013896 
2022-08-04 05:27:36,156 [dl_trainer.py:634] INFO train iter: 1000, num_batches_per_epoch: 25
2022-08-04 05:27:36,156 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 62.515625, lr: 0.100000, avg loss: 1.052156
2022-08-04 05:27:37,598 [dl_trainer.py:822] INFO Epoch 40, lr: 0.100000, val loss: 1.117559, val top-1 acc: 60.888672, top-5 acc: 96.425781
2022-08-04 05:27:38,039 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122500, Speed: 2089.790963 images/s
2022-08-04 05:27:38,039 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:38,039 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:39,171 [dl_trainer.py:634] INFO train iter: 1025, num_batches_per_epoch: 25
2022-08-04 05:27:39,172 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 63.031250, lr: 0.100000, avg loss: 1.017220
2022-08-04 05:27:40,671 [dl_trainer.py:822] INFO Epoch 41, lr: 0.100000, val loss: 1.062156, val top-1 acc: 62.324219, top-5 acc: 96.406250
2022-08-04 05:27:41,055 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125636, Speed: 2037.638947 images/s
2022-08-04 05:27:41,056 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:41,056 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:41,684 [dl_trainer.py:731] WARNING [ 41][ 1040/   25][rank:0] loss: 1.067, average forward (0.009538) and backward (0.026679) time: 0.125185, iotime: 0.014057 
2022-08-04 05:27:42,170 [dl_trainer.py:634] INFO train iter: 1050, num_batches_per_epoch: 25
2022-08-04 05:27:42,171 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 65.125000, lr: 0.100000, avg loss: 0.983815
2022-08-04 05:27:43,670 [dl_trainer.py:822] INFO Epoch 42, lr: 0.100000, val loss: 1.047495, val top-1 acc: 62.919922, top-5 acc: 96.757812
2022-08-04 05:27:44,038 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124252, Speed: 2060.330467 images/s
2022-08-04 05:27:44,039 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:44,039 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:45,210 [dl_trainer.py:634] INFO train iter: 1075, num_batches_per_epoch: 25
2022-08-04 05:27:45,211 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 64.671875, lr: 0.100000, avg loss: 0.985923
2022-08-04 05:27:46,727 [dl_trainer.py:822] INFO Epoch 43, lr: 0.100000, val loss: 1.061803, val top-1 acc: 63.271484, top-5 acc: 96.865234
2022-08-04 05:27:47,004 [dl_trainer.py:731] WARNING [ 43][ 1080/   25][rank:0] loss: 0.949, average forward (0.009894) and backward (0.026491) time: 0.121089, iotime: 0.008474 
2022-08-04 05:27:47,012 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123873, Speed: 2066.639136 images/s
2022-08-04 05:27:47,012 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:47,013 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:48,261 [dl_trainer.py:634] INFO train iter: 1100, num_batches_per_epoch: 25
2022-08-04 05:27:48,262 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 65.203125, lr: 0.100000, avg loss: 0.976426
2022-08-04 05:27:49,801 [dl_trainer.py:822] INFO Epoch 44, lr: 0.100000, val loss: 1.103899, val top-1 acc: 61.191406, top-5 acc: 96.904297
2022-08-04 05:27:50,045 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126322, Speed: 2026.562386 images/s
2022-08-04 05:27:50,045 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:50,045 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:51,031 [dl_trainer.py:731] WARNING [ 44][ 1120/   25][rank:0] loss: 1.099, average forward (0.009957) and backward (0.024873) time: 0.088585, iotime: 0.014209 
2022-08-04 05:27:51,279 [dl_trainer.py:634] INFO train iter: 1125, num_batches_per_epoch: 25
2022-08-04 05:27:51,280 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 65.171875, lr: 0.100000, avg loss: 0.980681
2022-08-04 05:27:52,839 [dl_trainer.py:822] INFO Epoch 45, lr: 0.100000, val loss: 1.003602, val top-1 acc: 65.253906, top-5 acc: 96.992188
2022-08-04 05:27:53,004 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123289, Speed: 2076.423588 images/s
2022-08-04 05:27:53,005 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:53,005 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:54,383 [dl_trainer.py:634] INFO train iter: 1150, num_batches_per_epoch: 25
2022-08-04 05:27:54,383 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 65.125000, lr: 0.100000, avg loss: 0.973178
2022-08-04 05:27:56,155 [dl_trainer.py:822] INFO Epoch 46, lr: 0.100000, val loss: 1.313107, val top-1 acc: 58.886719, top-5 acc: 95.585938
2022-08-04 05:27:56,292 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.136934, Speed: 1869.507782 images/s
2022-08-04 05:27:56,292 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:56,292 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:27:56,999 [dl_trainer.py:731] WARNING [ 46][ 1160/   25][rank:0] loss: 0.865, average forward (0.010192) and backward (0.026215) time: 0.136613, iotime: 0.015784 
2022-08-04 05:27:57,699 [dl_trainer.py:634] INFO train iter: 1175, num_batches_per_epoch: 25
2022-08-04 05:27:57,699 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 66.390625, lr: 0.100000, avg loss: 0.957788
2022-08-04 05:27:59,242 [dl_trainer.py:822] INFO Epoch 47, lr: 0.100000, val loss: 1.018046, val top-1 acc: 64.150391, top-5 acc: 97.128906
2022-08-04 05:27:59,310 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125736, Speed: 2036.005557 images/s
2022-08-04 05:27:59,311 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:27:59,311 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:00,814 [dl_trainer.py:731] WARNING [ 47][ 1200/   25][rank:0] loss: 0.864, average forward (0.009032) and backward (0.025537) time: 0.084345, iotime: 0.010777 
2022-08-04 05:28:00,832 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.063364, Speed: 4040.180707 images/s
2022-08-04 05:28:00,832 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:00,832 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:00,833 [dl_trainer.py:634] INFO train iter: 1200, num_batches_per_epoch: 25
2022-08-04 05:28:00,833 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 66.500000, lr: 0.100000, avg loss: 0.929292
2022-08-04 05:28:02,452 [dl_trainer.py:822] INFO Epoch 48, lr: 0.100000, val loss: 0.957577, val top-1 acc: 65.908203, top-5 acc: 97.275391
2022-08-04 05:28:04,011 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.132423, Speed: 1933.204230 images/s
2022-08-04 05:28:04,012 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:04,012 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:04,326 [dl_trainer.py:634] INFO train iter: 1225, num_batches_per_epoch: 25
2022-08-04 05:28:04,326 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 67.218750, lr: 0.100000, avg loss: 0.922364
2022-08-04 05:28:05,830 [dl_trainer.py:822] INFO Epoch 49, lr: 0.100000, val loss: 0.984629, val top-1 acc: 65.117188, top-5 acc: 97.500000
2022-08-04 05:28:06,591 [dl_trainer.py:731] WARNING [ 49][ 1240/   25][rank:0] loss: 0.892, average forward (0.008963) and backward (0.025393) time: 0.130415, iotime: 0.017113 
2022-08-04 05:28:06,973 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123357, Speed: 2075.280991 images/s
2022-08-04 05:28:06,973 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:06,973 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:07,366 [dl_trainer.py:634] INFO train iter: 1250, num_batches_per_epoch: 25
2022-08-04 05:28:07,366 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 67.156250, lr: 0.100000, avg loss: 0.901753
2022-08-04 05:28:08,893 [dl_trainer.py:822] INFO Epoch 50, lr: 0.100000, val loss: 1.014159, val top-1 acc: 64.765625, top-5 acc: 96.699219
2022-08-04 05:28:10,087 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.129736, Speed: 1973.241945 images/s
2022-08-04 05:28:10,088 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:10,088 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:10,557 [dl_trainer.py:634] INFO train iter: 1275, num_batches_per_epoch: 25
2022-08-04 05:28:10,558 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 69.281250, lr: 0.100000, avg loss: 0.860619
2022-08-04 05:28:12,083 [dl_trainer.py:822] INFO Epoch 51, lr: 0.100000, val loss: 0.997002, val top-1 acc: 64.726562, top-5 acc: 97.246094
2022-08-04 05:28:12,367 [dl_trainer.py:731] WARNING [ 51][ 1280/   25][rank:0] loss: 0.830, average forward (0.010183) and backward (0.026567) time: 0.131790, iotime: 0.017697 
2022-08-04 05:28:13,241 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.131357, Speed: 1948.891447 images/s
2022-08-04 05:28:13,241 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:13,242 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:13,779 [dl_trainer.py:634] INFO train iter: 1300, num_batches_per_epoch: 25
2022-08-04 05:28:13,779 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 67.968750, lr: 0.100000, avg loss: 0.876391
2022-08-04 05:28:15,297 [dl_trainer.py:822] INFO Epoch 52, lr: 0.100000, val loss: 1.137073, val top-1 acc: 63.583984, top-5 acc: 96.601562
2022-08-04 05:28:16,329 [dl_trainer.py:731] WARNING [ 52][ 1320/   25][rank:0] loss: 0.841, average forward (0.009351) and backward (0.026483) time: 0.085085, iotime: 0.010519 
2022-08-04 05:28:16,337 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.128971, Speed: 1984.942496 images/s
2022-08-04 05:28:16,337 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:16,338 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:16,868 [dl_trainer.py:634] INFO train iter: 1325, num_batches_per_epoch: 25
2022-08-04 05:28:16,869 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 68.968750, lr: 0.100000, avg loss: 0.852094
2022-08-04 05:28:18,385 [dl_trainer.py:822] INFO Epoch 53, lr: 0.100000, val loss: 1.002418, val top-1 acc: 65.449219, top-5 acc: 96.953125
2022-08-04 05:28:19,347 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125378, Speed: 2041.828226 images/s
2022-08-04 05:28:19,347 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:19,347 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:19,958 [dl_trainer.py:634] INFO train iter: 1350, num_batches_per_epoch: 25
2022-08-04 05:28:19,958 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 69.093750, lr: 0.100000, avg loss: 0.857535
2022-08-04 05:28:21,459 [dl_trainer.py:822] INFO Epoch 54, lr: 0.100000, val loss: 1.040322, val top-1 acc: 65.332031, top-5 acc: 97.177734
2022-08-04 05:28:21,988 [dl_trainer.py:731] WARNING [ 54][ 1360/   25][rank:0] loss: 0.829, average forward (0.009529) and backward (0.025945) time: 0.128976, iotime: 0.017040 
2022-08-04 05:28:22,422 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.128108, Speed: 1998.309665 images/s
2022-08-04 05:28:22,422 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:22,423 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:23,111 [dl_trainer.py:634] INFO train iter: 1375, num_batches_per_epoch: 25
2022-08-04 05:28:23,112 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 69.703125, lr: 0.100000, avg loss: 0.839352
2022-08-04 05:28:24,625 [dl_trainer.py:822] INFO Epoch 55, lr: 0.100000, val loss: 0.980713, val top-1 acc: 66.074219, top-5 acc: 97.246094
2022-08-04 05:28:25,461 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126608, Speed: 2021.987161 images/s
2022-08-04 05:28:25,462 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:25,462 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:26,128 [dl_trainer.py:731] WARNING [ 55][ 1400/   25][rank:0] loss: 0.897, average forward (0.009399) and backward (0.026317) time: 0.090842, iotime: 0.016596 
2022-08-04 05:28:26,138 [dl_trainer.py:634] INFO train iter: 1400, num_batches_per_epoch: 25
2022-08-04 05:28:26,138 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 70.968750, lr: 0.100000, avg loss: 0.819580
2022-08-04 05:28:27,639 [dl_trainer.py:822] INFO Epoch 56, lr: 0.100000, val loss: 1.000151, val top-1 acc: 65.312500, top-5 acc: 96.718750
2022-08-04 05:28:28,486 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125982, Speed: 2032.039745 images/s
2022-08-04 05:28:28,486 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:28,486 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:29,206 [dl_trainer.py:634] INFO train iter: 1425, num_batches_per_epoch: 25
2022-08-04 05:28:29,206 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 70.718750, lr: 0.100000, avg loss: 0.817332
2022-08-04 05:28:30,719 [dl_trainer.py:822] INFO Epoch 57, lr: 0.100000, val loss: 0.964631, val top-1 acc: 67.968750, top-5 acc: 97.304688
2022-08-04 05:28:31,458 [dl_trainer.py:731] WARNING [ 57][ 1440/   25][rank:0] loss: 1.023, average forward (0.009242) and backward (0.026414) time: 0.121251, iotime: 0.009350 
2022-08-04 05:28:31,471 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124362, Speed: 2058.511521 images/s
2022-08-04 05:28:31,472 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:31,472 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:32,273 [dl_trainer.py:634] INFO train iter: 1450, num_batches_per_epoch: 25
2022-08-04 05:28:32,273 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 70.687500, lr: 0.100000, avg loss: 0.828540
2022-08-04 05:28:33,778 [dl_trainer.py:822] INFO Epoch 58, lr: 0.100000, val loss: 0.899664, val top-1 acc: 68.671875, top-5 acc: 98.125000
2022-08-04 05:28:34,488 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125654, Speed: 2037.332547 images/s
2022-08-04 05:28:34,488 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:34,488 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:35,383 [dl_trainer.py:634] INFO train iter: 1475, num_batches_per_epoch: 25
2022-08-04 05:28:35,384 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 72.031250, lr: 0.100000, avg loss: 0.783661
2022-08-04 05:28:36,935 [dl_trainer.py:822] INFO Epoch 59, lr: 0.100000, val loss: 0.869257, val top-1 acc: 69.785156, top-5 acc: 97.841797
2022-08-04 05:28:37,202 [dl_trainer.py:731] WARNING [ 59][ 1480/   25][rank:0] loss: 0.841, average forward (0.010041) and backward (0.026700) time: 0.131759, iotime: 0.017236 
2022-08-04 05:28:37,631 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.130943, Speed: 1955.055517 images/s
2022-08-04 05:28:37,632 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:37,632 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:38,569 [dl_trainer.py:634] INFO train iter: 1500, num_batches_per_epoch: 25
2022-08-04 05:28:38,569 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 72.375000, lr: 0.100000, avg loss: 0.769225
2022-08-04 05:28:40,080 [dl_trainer.py:822] INFO Epoch 60, lr: 0.100000, val loss: 1.043882, val top-1 acc: 65.537109, top-5 acc: 97.431641
2022-08-04 05:28:40,731 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.129135, Speed: 1982.419320 images/s
2022-08-04 05:28:40,732 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:40,732 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:41,479 [dl_trainer.py:731] WARNING [ 60][ 1520/   25][rank:0] loss: 0.792, average forward (0.009406) and backward (0.026554) time: 0.092275, iotime: 0.016796 
2022-08-04 05:28:41,748 [dl_trainer.py:634] INFO train iter: 1525, num_batches_per_epoch: 25
2022-08-04 05:28:41,748 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 71.359375, lr: 0.100000, avg loss: 0.793420
2022-08-04 05:28:43,244 [dl_trainer.py:822] INFO Epoch 61, lr: 0.100000, val loss: 0.918083, val top-1 acc: 68.115234, top-5 acc: 97.978516
2022-08-04 05:28:43,804 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.127998, Speed: 2000.031493 images/s
2022-08-04 05:28:43,805 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:43,805 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:44,802 [dl_trainer.py:634] INFO train iter: 1550, num_batches_per_epoch: 25
2022-08-04 05:28:44,803 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 73.312500, lr: 0.100000, avg loss: 0.759995
2022-08-04 05:28:46,362 [dl_trainer.py:822] INFO Epoch 62, lr: 0.100000, val loss: 0.905451, val top-1 acc: 69.111328, top-5 acc: 98.037109
2022-08-04 05:28:46,868 [dl_trainer.py:731] WARNING [ 62][ 1560/   25][rank:0] loss: 0.826, average forward (0.009701) and backward (0.026194) time: 0.124151, iotime: 0.010357 
2022-08-04 05:28:46,893 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.128658, Speed: 1989.769693 images/s
2022-08-04 05:28:46,893 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:46,894 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:47,952 [dl_trainer.py:634] INFO train iter: 1575, num_batches_per_epoch: 25
2022-08-04 05:28:47,953 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 73.625000, lr: 0.100000, avg loss: 0.742682
2022-08-04 05:28:49,475 [dl_trainer.py:822] INFO Epoch 63, lr: 0.100000, val loss: 0.869772, val top-1 acc: 69.931641, top-5 acc: 98.125000
2022-08-04 05:28:49,971 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.128201, Speed: 1996.863094 images/s
2022-08-04 05:28:49,971 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:49,971 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:51,089 [dl_trainer.py:731] WARNING [ 63][ 1600/   25][rank:0] loss: 0.773, average forward (0.011042) and backward (0.026139) time: 0.093126, iotime: 0.017152 
2022-08-04 05:28:51,097 [dl_trainer.py:634] INFO train iter: 1600, num_batches_per_epoch: 25
2022-08-04 05:28:51,097 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 73.937500, lr: 0.100000, avg loss: 0.733262
2022-08-04 05:28:52,606 [dl_trainer.py:822] INFO Epoch 64, lr: 0.100000, val loss: 0.863986, val top-1 acc: 70.888672, top-5 acc: 98.183594
2022-08-04 05:28:53,097 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.130242, Speed: 1965.569383 images/s
2022-08-04 05:28:53,097 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:53,097 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:54,154 [dl_trainer.py:634] INFO train iter: 1625, num_batches_per_epoch: 25
2022-08-04 05:28:54,154 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 73.671875, lr: 0.100000, avg loss: 0.734058
2022-08-04 05:28:55,633 [dl_trainer.py:822] INFO Epoch 65, lr: 0.100000, val loss: 1.020240, val top-1 acc: 65.898438, top-5 acc: 97.216797
2022-08-04 05:28:56,016 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121590, Speed: 2105.430481 images/s
2022-08-04 05:28:56,016 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:56,016 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:28:56,656 [dl_trainer.py:731] WARNING [ 65][ 1640/   25][rank:0] loss: 0.690, average forward (0.008954) and backward (0.026872) time: 0.127187, iotime: 0.014029 
2022-08-04 05:28:57,128 [dl_trainer.py:634] INFO train iter: 1650, num_batches_per_epoch: 25
2022-08-04 05:28:57,129 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 73.406250, lr: 0.100000, avg loss: 0.738864
2022-08-04 05:28:58,615 [dl_trainer.py:822] INFO Epoch 66, lr: 0.100000, val loss: 0.797296, val top-1 acc: 72.617188, top-5 acc: 98.222656
2022-08-04 05:28:59,001 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124370, Speed: 2058.374062 images/s
2022-08-04 05:28:59,002 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:28:59,002 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:00,165 [dl_trainer.py:634] INFO train iter: 1675, num_batches_per_epoch: 25
2022-08-04 05:29:00,165 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 74.687500, lr: 0.100000, avg loss: 0.718979
2022-08-04 05:29:01,651 [dl_trainer.py:822] INFO Epoch 67, lr: 0.100000, val loss: 0.842753, val top-1 acc: 71.425781, top-5 acc: 98.183594
2022-08-04 05:29:01,916 [dl_trainer.py:731] WARNING [ 67][ 1680/   25][rank:0] loss: 0.604, average forward (0.009710) and backward (0.027160) time: 0.120492, iotime: 0.008366 
2022-08-04 05:29:01,926 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121837, Speed: 2101.162299 images/s
2022-08-04 05:29:01,927 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:01,927 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:03,154 [dl_trainer.py:634] INFO train iter: 1700, num_batches_per_epoch: 25
2022-08-04 05:29:03,155 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 75.375000, lr: 0.100000, avg loss: 0.700682
2022-08-04 05:29:04,630 [dl_trainer.py:822] INFO Epoch 68, lr: 0.100000, val loss: 0.889471, val top-1 acc: 70.664062, top-5 acc: 97.548828
2022-08-04 05:29:04,882 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123100, Speed: 2079.615399 images/s
2022-08-04 05:29:04,882 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:04,882 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:05,947 [dl_trainer.py:731] WARNING [ 68][ 1720/   25][rank:0] loss: 0.528, average forward (0.009352) and backward (0.025672) time: 0.086962, iotime: 0.013959 
2022-08-04 05:29:06,200 [dl_trainer.py:634] INFO train iter: 1725, num_batches_per_epoch: 25
2022-08-04 05:29:06,200 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 75.421875, lr: 0.100000, avg loss: 0.697898
2022-08-04 05:29:07,672 [dl_trainer.py:822] INFO Epoch 69, lr: 0.100000, val loss: 0.884009, val top-1 acc: 70.595703, top-5 acc: 98.281250
2022-08-04 05:29:07,850 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123654, Speed: 2070.286129 images/s
2022-08-04 05:29:07,851 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:07,851 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:09,179 [dl_trainer.py:634] INFO train iter: 1750, num_batches_per_epoch: 25
2022-08-04 05:29:09,180 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 76.984375, lr: 0.100000, avg loss: 0.660980
2022-08-04 05:29:10,663 [dl_trainer.py:822] INFO Epoch 70, lr: 0.100000, val loss: 0.942673, val top-1 acc: 68.642578, top-5 acc: 97.236328
2022-08-04 05:29:10,803 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122979, Speed: 2081.664370 images/s
2022-08-04 05:29:10,803 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:10,803 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:11,445 [dl_trainer.py:731] WARNING [ 70][ 1760/   25][rank:0] loss: 0.598, average forward (0.009668) and backward (0.027137) time: 0.125519, iotime: 0.013843 
2022-08-04 05:29:12,187 [dl_trainer.py:634] INFO train iter: 1775, num_batches_per_epoch: 25
2022-08-04 05:29:12,187 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 77.984375, lr: 0.100000, avg loss: 0.642504
2022-08-04 05:29:13,665 [dl_trainer.py:822] INFO Epoch 71, lr: 0.100000, val loss: 0.780067, val top-1 acc: 74.003906, top-5 acc: 98.574219
2022-08-04 05:29:13,722 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121570, Speed: 2105.785928 images/s
2022-08-04 05:29:13,722 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:13,722 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:15,182 [dl_trainer.py:731] WARNING [ 71][ 1800/   25][rank:0] loss: 0.669, average forward (0.009519) and backward (0.024922) time: 0.080332, iotime: 0.008609 
2022-08-04 05:29:15,201 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.061603, Speed: 4155.646477 images/s
2022-08-04 05:29:15,201 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:15,202 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:15,202 [dl_trainer.py:634] INFO train iter: 1800, num_batches_per_epoch: 25
2022-08-04 05:29:15,202 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 76.093750, lr: 0.100000, avg loss: 0.675947
2022-08-04 05:29:16,802 [dl_trainer.py:822] INFO Epoch 72, lr: 0.100000, val loss: 0.811164, val top-1 acc: 72.753906, top-5 acc: 98.300781
2022-08-04 05:29:18,244 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126759, Speed: 2019.584537 images/s
2022-08-04 05:29:18,245 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:18,245 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:18,540 [dl_trainer.py:634] INFO train iter: 1825, num_batches_per_epoch: 25
2022-08-04 05:29:18,540 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 77.078125, lr: 0.100000, avg loss: 0.642240
2022-08-04 05:29:20,050 [dl_trainer.py:822] INFO Epoch 73, lr: 0.100000, val loss: 0.848761, val top-1 acc: 72.441406, top-5 acc: 98.359375
2022-08-04 05:29:20,923 [dl_trainer.py:731] WARNING [ 73][ 1840/   25][rank:0] loss: 0.634, average forward (0.009977) and backward (0.025927) time: 0.129619, iotime: 0.014773 
2022-08-04 05:29:21,338 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.128856, Speed: 1986.710125 images/s
2022-08-04 05:29:21,338 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:21,338 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:21,701 [dl_trainer.py:634] INFO train iter: 1850, num_batches_per_epoch: 25
2022-08-04 05:29:21,701 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 77.750000, lr: 0.100000, avg loss: 0.634819
2022-08-04 05:29:23,242 [dl_trainer.py:822] INFO Epoch 74, lr: 0.100000, val loss: 0.736302, val top-1 acc: 75.322266, top-5 acc: 98.369141
2022-08-04 05:29:24,365 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126108, Speed: 2030.011773 images/s
2022-08-04 05:29:24,366 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:24,366 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:24,776 [dl_trainer.py:634] INFO train iter: 1875, num_batches_per_epoch: 25
2022-08-04 05:29:24,777 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 78.234375, lr: 0.100000, avg loss: 0.617317
2022-08-04 05:29:26,279 [dl_trainer.py:822] INFO Epoch 75, lr: 0.100000, val loss: 0.718345, val top-1 acc: 75.732422, top-5 acc: 98.642578
2022-08-04 05:29:26,523 [dl_trainer.py:731] WARNING [ 75][ 1880/   25][rank:0] loss: 0.492, average forward (0.010434) and backward (0.026751) time: 0.128343, iotime: 0.014105 
2022-08-04 05:29:27,326 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123317, Speed: 2075.946698 images/s
2022-08-04 05:29:27,326 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:27,326 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:27,807 [dl_trainer.py:634] INFO train iter: 1900, num_batches_per_epoch: 25
2022-08-04 05:29:27,808 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 79.140625, lr: 0.100000, avg loss: 0.586593
2022-08-04 05:29:29,327 [dl_trainer.py:822] INFO Epoch 76, lr: 0.100000, val loss: 0.831080, val top-1 acc: 72.763672, top-5 acc: 98.437500
2022-08-04 05:29:30,347 [dl_trainer.py:731] WARNING [ 76][ 1920/   25][rank:0] loss: 0.783, average forward (0.009503) and backward (0.026111) time: 0.083901, iotime: 0.008783 
2022-08-04 05:29:30,364 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126587, Speed: 2022.321655 images/s
2022-08-04 05:29:30,365 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:30,365 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:30,879 [dl_trainer.py:634] INFO train iter: 1925, num_batches_per_epoch: 25
2022-08-04 05:29:30,880 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 78.515625, lr: 0.100000, avg loss: 0.610164
2022-08-04 05:29:32,351 [dl_trainer.py:822] INFO Epoch 77, lr: 0.100000, val loss: 0.720391, val top-1 acc: 76.787109, top-5 acc: 98.085938
2022-08-04 05:29:33,287 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121766, Speed: 2102.398072 images/s
2022-08-04 05:29:33,288 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:33,288 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:33,829 [dl_trainer.py:634] INFO train iter: 1950, num_batches_per_epoch: 25
2022-08-04 05:29:33,829 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 79.718750, lr: 0.100000, avg loss: 0.593868
2022-08-04 05:29:35,333 [dl_trainer.py:822] INFO Epoch 78, lr: 0.100000, val loss: 0.757477, val top-1 acc: 74.384766, top-5 acc: 98.720703
2022-08-04 05:29:35,842 [dl_trainer.py:731] WARNING [ 78][ 1960/   25][rank:0] loss: 0.589, average forward (0.009203) and backward (0.025794) time: 0.124658, iotime: 0.013917 
2022-08-04 05:29:36,258 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123726, Speed: 2069.085980 images/s
2022-08-04 05:29:36,258 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:36,258 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:36,848 [dl_trainer.py:634] INFO train iter: 1975, num_batches_per_epoch: 25
2022-08-04 05:29:36,849 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 79.875000, lr: 0.100000, avg loss: 0.576933
2022-08-04 05:29:38,461 [dl_trainer.py:822] INFO Epoch 79, lr: 0.100000, val loss: 0.805978, val top-1 acc: 73.378906, top-5 acc: 98.642578
2022-08-04 05:29:39,356 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.129064, Speed: 1983.518871 images/s
2022-08-04 05:29:39,356 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:39,356 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:40,014 [dl_trainer.py:731] WARNING [ 79][ 2000/   25][rank:0] loss: 0.587, average forward (0.009248) and backward (0.026195) time: 0.090009, iotime: 0.013898 
2022-08-04 05:29:40,032 [dl_trainer.py:634] INFO train iter: 2000, num_batches_per_epoch: 25
2022-08-04 05:29:40,032 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 79.718750, lr: 0.100000, avg loss: 0.572086
2022-08-04 05:29:41,509 [dl_trainer.py:822] INFO Epoch 80, lr: 0.100000, val loss: 0.761173, val top-1 acc: 75.380859, top-5 acc: 98.437500
2022-08-04 05:29:42,356 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124966, Speed: 2048.550278 images/s
2022-08-04 05:29:42,357 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:42,357 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:43,056 [dl_trainer.py:634] INFO train iter: 2025, num_batches_per_epoch: 25
2022-08-04 05:29:43,057 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 79.000000, lr: 0.100000, avg loss: 0.592068
2022-08-04 05:29:44,553 [dl_trainer.py:822] INFO Epoch 81, lr: 0.100000, val loss: 0.687862, val top-1 acc: 77.275391, top-5 acc: 98.652344
2022-08-04 05:29:45,303 [dl_trainer.py:731] WARNING [ 81][ 2040/   25][rank:0] loss: 0.389, average forward (0.009178) and backward (0.026652) time: 0.119732, iotime: 0.008680 
2022-08-04 05:29:45,310 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123051, Speed: 2080.439744 images/s
2022-08-04 05:29:45,311 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:45,311 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:46,036 [dl_trainer.py:634] INFO train iter: 2050, num_batches_per_epoch: 25
2022-08-04 05:29:46,036 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 81.437500, lr: 0.010000, avg loss: 0.538785
2022-08-04 05:29:47,541 [dl_trainer.py:822] INFO Epoch 82, lr: 0.010000, val loss: 0.582354, val top-1 acc: 80.546875, top-5 acc: 98.984375
2022-08-04 05:29:48,290 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124112, Speed: 2062.647638 images/s
2022-08-04 05:29:48,290 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:48,291 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:49,082 [dl_trainer.py:634] INFO train iter: 2075, num_batches_per_epoch: 25
2022-08-04 05:29:49,083 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 82.390625, lr: 0.010000, avg loss: 0.509917
2022-08-04 05:29:50,578 [dl_trainer.py:822] INFO Epoch 83, lr: 0.010000, val loss: 0.553525, val top-1 acc: 80.986328, top-5 acc: 99.072266
2022-08-04 05:29:50,842 [dl_trainer.py:731] WARNING [ 83][ 2080/   25][rank:0] loss: 0.581, average forward (0.009594) and backward (0.025995) time: 0.125994, iotime: 0.014244 
2022-08-04 05:29:51,261 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123752, Speed: 2068.660943 images/s
2022-08-04 05:29:51,262 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:51,262 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:52,112 [dl_trainer.py:634] INFO train iter: 2100, num_batches_per_epoch: 25
2022-08-04 05:29:52,113 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 82.765625, lr: 0.010000, avg loss: 0.508321
2022-08-04 05:29:53,614 [dl_trainer.py:822] INFO Epoch 84, lr: 0.010000, val loss: 0.547429, val top-1 acc: 81.347656, top-5 acc: 99.160156
2022-08-04 05:29:54,239 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124059, Speed: 2063.526824 images/s
2022-08-04 05:29:54,240 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:54,240 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:54,855 [dl_trainer.py:731] WARNING [ 84][ 2120/   25][rank:0] loss: 0.478, average forward (0.009669) and backward (0.025579) time: 0.088723, iotime: 0.015147 
2022-08-04 05:29:55,117 [dl_trainer.py:634] INFO train iter: 2125, num_batches_per_epoch: 25
2022-08-04 05:29:55,117 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 83.265625, lr: 0.010000, avg loss: 0.481652
2022-08-04 05:29:56,597 [dl_trainer.py:822] INFO Epoch 85, lr: 0.010000, val loss: 0.544924, val top-1 acc: 81.591797, top-5 acc: 99.179688
2022-08-04 05:29:57,181 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122543, Speed: 2089.067235 images/s
2022-08-04 05:29:57,182 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:29:57,182 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:29:58,166 [dl_trainer.py:634] INFO train iter: 2150, num_batches_per_epoch: 25
2022-08-04 05:29:58,166 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 82.812500, lr: 0.010000, avg loss: 0.495082
2022-08-04 05:29:59,619 [dl_trainer.py:822] INFO Epoch 86, lr: 0.010000, val loss: 0.541850, val top-1 acc: 81.601562, top-5 acc: 99.130859
2022-08-04 05:30:00,168 [dl_trainer.py:731] WARNING [ 86][ 2160/   25][rank:0] loss: 0.462, average forward (0.009205) and backward (0.027020) time: 0.119340, iotime: 0.008470 
2022-08-04 05:30:00,176 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124726, Speed: 2052.504188 images/s
2022-08-04 05:30:00,176 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:00,177 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:01,214 [dl_trainer.py:634] INFO train iter: 2175, num_batches_per_epoch: 25
2022-08-04 05:30:01,215 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 82.781250, lr: 0.010000, avg loss: 0.496880
2022-08-04 05:30:02,719 [dl_trainer.py:822] INFO Epoch 87, lr: 0.010000, val loss: 0.541292, val top-1 acc: 81.523438, top-5 acc: 99.121094
2022-08-04 05:30:03,182 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125228, Speed: 2044.270800 images/s
2022-08-04 05:30:03,183 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:03,183 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:04,247 [dl_trainer.py:731] WARNING [ 87][ 2200/   25][rank:0] loss: 0.499, average forward (0.010001) and backward (0.025551) time: 0.088599, iotime: 0.014659 
2022-08-04 05:30:04,275 [dl_trainer.py:634] INFO train iter: 2200, num_batches_per_epoch: 25
2022-08-04 05:30:04,275 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 83.796875, lr: 0.010000, avg loss: 0.476918
2022-08-04 05:30:05,786 [dl_trainer.py:822] INFO Epoch 88, lr: 0.010000, val loss: 0.538655, val top-1 acc: 81.669922, top-5 acc: 99.150391
2022-08-04 05:30:06,220 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126517, Speed: 2023.435899 images/s
2022-08-04 05:30:06,220 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:06,220 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:07,287 [dl_trainer.py:634] INFO train iter: 2225, num_batches_per_epoch: 25
2022-08-04 05:30:07,288 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 83.203125, lr: 0.010000, avg loss: 0.475613
2022-08-04 05:30:08,748 [dl_trainer.py:822] INFO Epoch 89, lr: 0.010000, val loss: 0.536475, val top-1 acc: 81.943359, top-5 acc: 99.121094
2022-08-04 05:30:09,120 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.120834, Speed: 2118.614872 images/s
2022-08-04 05:30:09,121 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:09,121 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:09,786 [dl_trainer.py:731] WARNING [ 89][ 2240/   25][rank:0] loss: 0.424, average forward (0.009515) and backward (0.026160) time: 0.124890, iotime: 0.013906 
2022-08-04 05:30:10,296 [dl_trainer.py:634] INFO train iter: 2250, num_batches_per_epoch: 25
2022-08-04 05:30:10,296 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 84.015625, lr: 0.010000, avg loss: 0.468686
2022-08-04 05:30:11,770 [dl_trainer.py:822] INFO Epoch 90, lr: 0.010000, val loss: 0.535935, val top-1 acc: 81.884766, top-5 acc: 99.189453
2022-08-04 05:30:12,117 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124839, Speed: 2050.639465 images/s
2022-08-04 05:30:12,118 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:12,118 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:13,306 [dl_trainer.py:634] INFO train iter: 2275, num_batches_per_epoch: 25
2022-08-04 05:30:13,307 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 83.062500, lr: 0.010000, avg loss: 0.490037
2022-08-04 05:30:14,789 [dl_trainer.py:822] INFO Epoch 91, lr: 0.010000, val loss: 0.535887, val top-1 acc: 81.894531, top-5 acc: 99.140625
2022-08-04 05:30:15,084 [dl_trainer.py:731] WARNING [ 91][ 2280/   25][rank:0] loss: 0.465, average forward (0.009533) and backward (0.026136) time: 0.119168, iotime: 0.008417 
2022-08-04 05:30:15,097 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124144, Speed: 2062.122104 images/s
2022-08-04 05:30:15,098 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:15,098 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:16,327 [dl_trainer.py:634] INFO train iter: 2300, num_batches_per_epoch: 25
2022-08-04 05:30:16,328 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 83.046875, lr: 0.010000, avg loss: 0.486092
2022-08-04 05:30:17,832 [dl_trainer.py:822] INFO Epoch 92, lr: 0.010000, val loss: 0.535546, val top-1 acc: 81.904297, top-5 acc: 99.091797
2022-08-04 05:30:18,079 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124209, Speed: 2061.045298 images/s
2022-08-04 05:30:18,080 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:18,080 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:19,091 [dl_trainer.py:731] WARNING [ 92][ 2320/   25][rank:0] loss: 0.445, average forward (0.009499) and backward (0.026181) time: 0.089219, iotime: 0.015007 
2022-08-04 05:30:19,352 [dl_trainer.py:634] INFO train iter: 2325, num_batches_per_epoch: 25
2022-08-04 05:30:19,352 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 83.625000, lr: 0.010000, avg loss: 0.477402
2022-08-04 05:30:20,826 [dl_trainer.py:822] INFO Epoch 93, lr: 0.010000, val loss: 0.535468, val top-1 acc: 82.070312, top-5 acc: 99.111328
2022-08-04 05:30:21,009 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122063, Speed: 2097.281203 images/s
2022-08-04 05:30:21,010 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:21,010 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:22,309 [dl_trainer.py:634] INFO train iter: 2350, num_batches_per_epoch: 25
2022-08-04 05:30:22,310 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 83.953125, lr: 0.010000, avg loss: 0.462374
2022-08-04 05:30:23,805 [dl_trainer.py:822] INFO Epoch 94, lr: 0.010000, val loss: 0.535732, val top-1 acc: 81.992188, top-5 acc: 99.121094
2022-08-04 05:30:23,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121799, Speed: 2101.823289 images/s
2022-08-04 05:30:23,934 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:23,934 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:24,599 [dl_trainer.py:731] WARNING [ 94][ 2360/   25][rank:0] loss: 0.477, average forward (0.009336) and backward (0.025819) time: 0.124527, iotime: 0.014262 
2022-08-04 05:30:25,317 [dl_trainer.py:634] INFO train iter: 2375, num_batches_per_epoch: 25
2022-08-04 05:30:25,318 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 83.421875, lr: 0.010000, avg loss: 0.480780
2022-08-04 05:30:26,838 [dl_trainer.py:822] INFO Epoch 95, lr: 0.010000, val loss: 0.534272, val top-1 acc: 82.119141, top-5 acc: 99.091797
2022-08-04 05:30:26,898 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123488, Speed: 2073.072440 images/s
2022-08-04 05:30:26,898 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:26,899 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:28,349 [dl_trainer.py:731] WARNING [ 95][ 2400/   25][rank:0] loss: 0.430, average forward (0.009574) and backward (0.026223) time: 0.082601, iotime: 0.008413 
2022-08-04 05:30:28,357 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.060747, Speed: 4214.178681 images/s
2022-08-04 05:30:28,357 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:28,358 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:28,358 [dl_trainer.py:634] INFO train iter: 2400, num_batches_per_epoch: 25
2022-08-04 05:30:28,359 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 84.328125, lr: 0.010000, avg loss: 0.455870
2022-08-04 05:30:29,908 [dl_trainer.py:822] INFO Epoch 96, lr: 0.010000, val loss: 0.533640, val top-1 acc: 82.080078, top-5 acc: 99.140625
2022-08-04 05:30:31,357 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124934, Speed: 2049.085699 images/s
2022-08-04 05:30:31,357 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:31,357 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:31,670 [dl_trainer.py:634] INFO train iter: 2425, num_batches_per_epoch: 25
2022-08-04 05:30:31,671 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 83.984375, lr: 0.010000, avg loss: 0.464335
2022-08-04 05:30:33,199 [dl_trainer.py:822] INFO Epoch 97, lr: 0.010000, val loss: 0.533400, val top-1 acc: 82.343750, top-5 acc: 99.140625
2022-08-04 05:30:33,942 [dl_trainer.py:731] WARNING [ 97][ 2440/   25][rank:0] loss: 0.440, average forward (0.010148) and backward (0.026605) time: 0.129259, iotime: 0.014433 
2022-08-04 05:30:34,394 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126527, Speed: 2023.278461 images/s
2022-08-04 05:30:34,395 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:34,395 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:34,761 [dl_trainer.py:634] INFO train iter: 2450, num_batches_per_epoch: 25
2022-08-04 05:30:34,762 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 83.609375, lr: 0.010000, avg loss: 0.460329
2022-08-04 05:30:36,341 [dl_trainer.py:822] INFO Epoch 98, lr: 0.010000, val loss: 0.532797, val top-1 acc: 82.060547, top-5 acc: 99.101562
2022-08-04 05:30:37,536 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.130849, Speed: 1956.459210 images/s
2022-08-04 05:30:37,536 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:37,536 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:37,942 [dl_trainer.py:634] INFO train iter: 2475, num_batches_per_epoch: 25
2022-08-04 05:30:37,943 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 83.937500, lr: 0.010000, avg loss: 0.459218
2022-08-04 05:30:39,665 [dl_trainer.py:822] INFO Epoch 99, lr: 0.010000, val loss: 0.530917, val top-1 acc: 82.324219, top-5 acc: 99.121094
2022-08-04 05:30:39,917 [dl_trainer.py:731] WARNING [ 99][ 2480/   25][rank:0] loss: 0.480, average forward (0.009510) and backward (0.026462) time: 0.133645, iotime: 0.013972 
2022-08-04 05:30:40,748 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.133790, Speed: 1913.448488 images/s
2022-08-04 05:30:40,748 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:40,748 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:41,197 [dl_trainer.py:634] INFO train iter: 2500, num_batches_per_epoch: 25
2022-08-04 05:30:41,198 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 83.906250, lr: 0.010000, avg loss: 0.457352
2022-08-04 05:30:42,687 [dl_trainer.py:822] INFO Epoch 100, lr: 0.010000, val loss: 0.530444, val top-1 acc: 82.509766, top-5 acc: 99.121094
2022-08-04 05:30:43,718 [dl_trainer.py:731] WARNING [100][ 2520/   25][rank:0] loss: 0.578, average forward (0.009426) and backward (0.026510) time: 0.082560, iotime: 0.008571 
2022-08-04 05:30:43,726 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124061, Speed: 2063.506004 images/s
2022-08-04 05:30:43,727 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:43,727 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:44,216 [dl_trainer.py:634] INFO train iter: 2525, num_batches_per_epoch: 25
2022-08-04 05:30:44,217 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 83.687500, lr: 0.010000, avg loss: 0.467320
2022-08-04 05:30:45,677 [dl_trainer.py:822] INFO Epoch 101, lr: 0.010000, val loss: 0.530331, val top-1 acc: 82.500000, top-5 acc: 99.150391
2022-08-04 05:30:46,661 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122242, Speed: 2094.206000 images/s
2022-08-04 05:30:46,662 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:46,662 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:47,202 [dl_trainer.py:634] INFO train iter: 2550, num_batches_per_epoch: 25
2022-08-04 05:30:47,203 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 84.906250, lr: 0.010000, avg loss: 0.442669
2022-08-04 05:30:48,712 [dl_trainer.py:822] INFO Epoch 102, lr: 0.010000, val loss: 0.530494, val top-1 acc: 82.255859, top-5 acc: 99.130859
2022-08-04 05:30:49,250 [dl_trainer.py:731] WARNING [102][ 2560/   25][rank:0] loss: 0.431, average forward (0.009636) and backward (0.025467) time: 0.124626, iotime: 0.014400 
2022-08-04 05:30:49,678 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125684, Speed: 2036.858148 images/s
2022-08-04 05:30:49,679 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:49,679 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:50,268 [dl_trainer.py:634] INFO train iter: 2575, num_batches_per_epoch: 25
2022-08-04 05:30:50,269 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 84.531250, lr: 0.010000, avg loss: 0.451227
2022-08-04 05:30:51,757 [dl_trainer.py:822] INFO Epoch 103, lr: 0.010000, val loss: 0.529793, val top-1 acc: 82.441406, top-5 acc: 99.140625
2022-08-04 05:30:52,604 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121869, Speed: 2100.618670 images/s
2022-08-04 05:30:52,604 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:52,605 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:53,226 [dl_trainer.py:731] WARNING [103][ 2600/   25][rank:0] loss: 0.468, average forward (0.009863) and backward (0.026736) time: 0.087884, iotime: 0.013671 
2022-08-04 05:30:53,254 [dl_trainer.py:634] INFO train iter: 2600, num_batches_per_epoch: 25
2022-08-04 05:30:53,255 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 84.609375, lr: 0.010000, avg loss: 0.446745
2022-08-04 05:30:54,739 [dl_trainer.py:822] INFO Epoch 104, lr: 0.010000, val loss: 0.530938, val top-1 acc: 82.294922, top-5 acc: 99.218750
2022-08-04 05:30:55,544 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122477, Speed: 2090.187769 images/s
2022-08-04 05:30:55,545 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:55,545 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:56,234 [dl_trainer.py:634] INFO train iter: 2625, num_batches_per_epoch: 25
2022-08-04 05:30:56,235 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 84.203125, lr: 0.010000, avg loss: 0.448554
2022-08-04 05:30:57,718 [dl_trainer.py:822] INFO Epoch 105, lr: 0.010000, val loss: 0.528110, val top-1 acc: 82.382812, top-5 acc: 99.140625
2022-08-04 05:30:58,466 [dl_trainer.py:731] WARNING [105][ 2640/   25][rank:0] loss: 0.555, average forward (0.009671) and backward (0.026323) time: 0.119528, iotime: 0.008471 
2022-08-04 05:30:58,475 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122065, Speed: 2097.241775 images/s
2022-08-04 05:30:58,475 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:30:58,475 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:30:59,185 [dl_trainer.py:634] INFO train iter: 2650, num_batches_per_epoch: 25
2022-08-04 05:30:59,185 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 83.671875, lr: 0.010000, avg loss: 0.461343
2022-08-04 05:31:00,697 [dl_trainer.py:822] INFO Epoch 106, lr: 0.010000, val loss: 0.527923, val top-1 acc: 82.617188, top-5 acc: 99.150391
2022-08-04 05:31:01,412 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122351, Speed: 2092.348861 images/s
2022-08-04 05:31:01,413 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:01,413 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:02,194 [dl_trainer.py:634] INFO train iter: 2675, num_batches_per_epoch: 25
2022-08-04 05:31:02,195 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 84.515625, lr: 0.010000, avg loss: 0.456993
2022-08-04 05:31:03,680 [dl_trainer.py:822] INFO Epoch 107, lr: 0.010000, val loss: 0.529029, val top-1 acc: 82.490234, top-5 acc: 99.111328
2022-08-04 05:31:03,931 [dl_trainer.py:731] WARNING [107][ 2680/   25][rank:0] loss: 0.503, average forward (0.009883) and backward (0.026283) time: 0.125887, iotime: 0.013829 
2022-08-04 05:31:04,326 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121372, Speed: 2109.213032 images/s
2022-08-04 05:31:04,326 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:04,326 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:05,184 [dl_trainer.py:634] INFO train iter: 2700, num_batches_per_epoch: 25
2022-08-04 05:31:05,184 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 84.328125, lr: 0.010000, avg loss: 0.444833
2022-08-04 05:31:06,674 [dl_trainer.py:822] INFO Epoch 108, lr: 0.010000, val loss: 0.528192, val top-1 acc: 82.626953, top-5 acc: 99.140625
2022-08-04 05:31:07,335 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125356, Speed: 2042.185824 images/s
2022-08-04 05:31:07,336 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:07,336 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:07,967 [dl_trainer.py:731] WARNING [108][ 2720/   25][rank:0] loss: 0.455, average forward (0.009804) and backward (0.026892) time: 0.089083, iotime: 0.013973 
2022-08-04 05:31:08,222 [dl_trainer.py:634] INFO train iter: 2725, num_batches_per_epoch: 25
2022-08-04 05:31:08,223 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 83.937500, lr: 0.010000, avg loss: 0.466591
2022-08-04 05:31:09,715 [dl_trainer.py:822] INFO Epoch 109, lr: 0.010000, val loss: 0.528198, val top-1 acc: 82.714844, top-5 acc: 99.150391
2022-08-04 05:31:10,301 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123563, Speed: 2071.815091 images/s
2022-08-04 05:31:10,302 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:10,302 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:11,236 [dl_trainer.py:634] INFO train iter: 2750, num_batches_per_epoch: 25
2022-08-04 05:31:11,236 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 84.281250, lr: 0.010000, avg loss: 0.451965
2022-08-04 05:31:12,733 [dl_trainer.py:822] INFO Epoch 110, lr: 0.010000, val loss: 0.527217, val top-1 acc: 82.617188, top-5 acc: 99.228516
2022-08-04 05:31:13,256 [dl_trainer.py:731] WARNING [110][ 2760/   25][rank:0] loss: 0.469, average forward (0.009882) and backward (0.026483) time: 0.120281, iotime: 0.008492 
2022-08-04 05:31:13,268 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123574, Speed: 2071.630717 images/s
2022-08-04 05:31:13,269 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:13,269 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:14,252 [dl_trainer.py:634] INFO train iter: 2775, num_batches_per_epoch: 25
2022-08-04 05:31:14,252 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 84.343750, lr: 0.010000, avg loss: 0.446559
2022-08-04 05:31:15,785 [dl_trainer.py:822] INFO Epoch 111, lr: 0.010000, val loss: 0.525739, val top-1 acc: 82.646484, top-5 acc: 99.150391
2022-08-04 05:31:16,250 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124202, Speed: 2061.154263 images/s
2022-08-04 05:31:16,250 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:16,250 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:17,250 [dl_trainer.py:731] WARNING [111][ 2800/   25][rank:0] loss: 0.475, average forward (0.010173) and backward (0.026114) time: 0.089817, iotime: 0.014373 
2022-08-04 05:31:17,276 [dl_trainer.py:634] INFO train iter: 2800, num_batches_per_epoch: 25
2022-08-04 05:31:17,276 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 84.265625, lr: 0.010000, avg loss: 0.459323
2022-08-04 05:31:18,722 [dl_trainer.py:822] INFO Epoch 112, lr: 0.010000, val loss: 0.525118, val top-1 acc: 82.675781, top-5 acc: 99.169922
2022-08-04 05:31:19,145 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.120594, Speed: 2122.824564 images/s
2022-08-04 05:31:19,145 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:19,145 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:20,239 [dl_trainer.py:634] INFO train iter: 2825, num_batches_per_epoch: 25
2022-08-04 05:31:20,239 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 84.828125, lr: 0.010000, avg loss: 0.446526
2022-08-04 05:31:21,733 [dl_trainer.py:822] INFO Epoch 113, lr: 0.010000, val loss: 0.525379, val top-1 acc: 82.714844, top-5 acc: 99.228516
2022-08-04 05:31:22,099 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123055, Speed: 2080.370379 images/s
2022-08-04 05:31:22,099 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:22,099 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:22,751 [dl_trainer.py:731] WARNING [113][ 2840/   25][rank:0] loss: 0.442, average forward (0.010158) and backward (0.025872) time: 0.125373, iotime: 0.014788 
2022-08-04 05:31:23,217 [dl_trainer.py:634] INFO train iter: 2850, num_batches_per_epoch: 25
2022-08-04 05:31:23,217 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 83.828125, lr: 0.010000, avg loss: 0.457327
2022-08-04 05:31:24,682 [dl_trainer.py:822] INFO Epoch 114, lr: 0.010000, val loss: 0.523134, val top-1 acc: 82.851562, top-5 acc: 99.228516
2022-08-04 05:31:25,028 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122019, Speed: 2098.031985 images/s
2022-08-04 05:31:25,029 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:25,029 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:26,187 [dl_trainer.py:634] INFO train iter: 2875, num_batches_per_epoch: 25
2022-08-04 05:31:26,188 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 84.859375, lr: 0.010000, avg loss: 0.439278
2022-08-04 05:31:27,643 [dl_trainer.py:822] INFO Epoch 115, lr: 0.010000, val loss: 0.524943, val top-1 acc: 82.695312, top-5 acc: 99.218750
2022-08-04 05:31:27,922 [dl_trainer.py:731] WARNING [115][ 2880/   25][rank:0] loss: 0.482, average forward (0.009857) and backward (0.026908) time: 0.118769, iotime: 0.008137 
2022-08-04 05:31:27,933 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121004, Speed: 2115.633665 images/s
2022-08-04 05:31:27,933 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:27,934 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:29,122 [dl_trainer.py:634] INFO train iter: 2900, num_batches_per_epoch: 25
2022-08-04 05:31:29,123 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 84.687500, lr: 0.010000, avg loss: 0.441226
2022-08-04 05:31:30,685 [dl_trainer.py:822] INFO Epoch 116, lr: 0.010000, val loss: 0.525923, val top-1 acc: 82.568359, top-5 acc: 99.306641
2022-08-04 05:31:30,930 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124850, Speed: 2050.462267 images/s
2022-08-04 05:31:30,931 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:30,931 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:31,960 [dl_trainer.py:731] WARNING [116][ 2920/   25][rank:0] loss: 0.318, average forward (0.010180) and backward (0.025473) time: 0.089721, iotime: 0.014179 
2022-08-04 05:31:32,201 [dl_trainer.py:634] INFO train iter: 2925, num_batches_per_epoch: 25
2022-08-04 05:31:32,202 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 84.843750, lr: 0.010000, avg loss: 0.431696
2022-08-04 05:31:33,690 [dl_trainer.py:822] INFO Epoch 117, lr: 0.010000, val loss: 0.527405, val top-1 acc: 82.529297, top-5 acc: 99.238281
2022-08-04 05:31:33,873 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122575, Speed: 2088.513596 images/s
2022-08-04 05:31:33,873 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:33,873 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:35,210 [dl_trainer.py:634] INFO train iter: 2950, num_batches_per_epoch: 25
2022-08-04 05:31:35,211 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 84.671875, lr: 0.010000, avg loss: 0.438926
2022-08-04 05:31:36,706 [dl_trainer.py:822] INFO Epoch 118, lr: 0.010000, val loss: 0.524573, val top-1 acc: 82.734375, top-5 acc: 99.218750
2022-08-04 05:31:36,842 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123699, Speed: 2069.539612 images/s
2022-08-04 05:31:36,843 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:36,843 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:37,468 [dl_trainer.py:731] WARNING [118][ 2960/   25][rank:0] loss: 0.479, average forward (0.009628) and backward (0.025856) time: 0.125409, iotime: 0.014279 
2022-08-04 05:31:38,306 [dl_trainer.py:634] INFO train iter: 2975, num_batches_per_epoch: 25
2022-08-04 05:31:38,306 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 83.781250, lr: 0.010000, avg loss: 0.468504
2022-08-04 05:31:39,790 [dl_trainer.py:822] INFO Epoch 119, lr: 0.010000, val loss: 0.523417, val top-1 acc: 82.832031, top-5 acc: 99.218750
2022-08-04 05:31:39,860 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125685, Speed: 2036.844141 images/s
2022-08-04 05:31:39,861 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:39,861 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:41,272 [dl_trainer.py:731] WARNING [119][ 3000/   25][rank:0] loss: 0.469, average forward (0.009600) and backward (0.026297) time: 0.082014, iotime: 0.008475 
2022-08-04 05:31:41,284 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.059264, Speed: 4319.663532 images/s
2022-08-04 05:31:41,284 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:41,284 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:41,285 [dl_trainer.py:634] INFO train iter: 3000, num_batches_per_epoch: 25
2022-08-04 05:31:41,285 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 84.875000, lr: 0.010000, avg loss: 0.445510
2022-08-04 05:31:42,838 [dl_trainer.py:822] INFO Epoch 120, lr: 0.010000, val loss: 0.522768, val top-1 acc: 82.880859, top-5 acc: 99.199219
2022-08-04 05:31:44,286 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125051, Speed: 2047.163590 images/s
2022-08-04 05:31:44,287 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:44,287 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:44,597 [dl_trainer.py:634] INFO train iter: 3025, num_batches_per_epoch: 25
2022-08-04 05:31:44,598 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 84.703125, lr: 0.010000, avg loss: 0.432410
2022-08-04 05:31:46,104 [dl_trainer.py:822] INFO Epoch 121, lr: 0.010000, val loss: 0.521039, val top-1 acc: 82.851562, top-5 acc: 99.248047
2022-08-04 05:31:46,833 [dl_trainer.py:731] WARNING [121][ 3040/   25][rank:0] loss: 0.451, average forward (0.009736) and backward (0.026543) time: 0.127169, iotime: 0.013483 
2022-08-04 05:31:47,253 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123554, Speed: 2071.963680 images/s
2022-08-04 05:31:47,254 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:47,254 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:47,610 [dl_trainer.py:634] INFO train iter: 3050, num_batches_per_epoch: 25
2022-08-04 05:31:47,611 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 85.187500, lr: 0.010000, avg loss: 0.429778
2022-08-04 05:31:49,072 [dl_trainer.py:822] INFO Epoch 122, lr: 0.010000, val loss: 0.522328, val top-1 acc: 82.744141, top-5 acc: 99.326172
2022-08-04 05:31:50,163 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121192, Speed: 2112.348948 images/s
2022-08-04 05:31:50,163 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:50,163 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:50,568 [dl_trainer.py:634] INFO train iter: 3075, num_batches_per_epoch: 25
2022-08-04 05:31:50,568 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 85.187500, lr: 0.001000, avg loss: 0.413748
2022-08-04 05:31:52,063 [dl_trainer.py:822] INFO Epoch 123, lr: 0.001000, val loss: 0.522576, val top-1 acc: 82.841797, top-5 acc: 99.208984
2022-08-04 05:31:52,320 [dl_trainer.py:731] WARNING [123][ 3080/   25][rank:0] loss: 0.434, average forward (0.009548) and backward (0.026989) time: 0.125653, iotime: 0.014060 
2022-08-04 05:31:53,124 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123360, Speed: 2075.235199 images/s
2022-08-04 05:31:53,125 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:53,125 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:53,571 [dl_trainer.py:634] INFO train iter: 3100, num_batches_per_epoch: 25
2022-08-04 05:31:53,572 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 84.890625, lr: 0.001000, avg loss: 0.437486
2022-08-04 05:31:55,075 [dl_trainer.py:822] INFO Epoch 124, lr: 0.001000, val loss: 0.522930, val top-1 acc: 82.832031, top-5 acc: 99.257812
2022-08-04 05:31:56,072 [dl_trainer.py:731] WARNING [124][ 3120/   25][rank:0] loss: 0.527, average forward (0.009471) and backward (0.025852) time: 0.082285, iotime: 0.008473 
2022-08-04 05:31:56,082 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123225, Speed: 2077.502291 images/s
2022-08-04 05:31:56,083 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:56,083 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:56,596 [dl_trainer.py:634] INFO train iter: 3125, num_batches_per_epoch: 25
2022-08-04 05:31:56,596 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 84.906250, lr: 0.001000, avg loss: 0.428999
2022-08-04 05:31:58,066 [dl_trainer.py:822] INFO Epoch 125, lr: 0.001000, val loss: 0.521817, val top-1 acc: 82.763672, top-5 acc: 99.277344
2022-08-04 05:31:59,071 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124490, Speed: 2056.392815 images/s
2022-08-04 05:31:59,072 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:31:59,072 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:31:59,616 [dl_trainer.py:634] INFO train iter: 3150, num_batches_per_epoch: 25
2022-08-04 05:31:59,617 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 84.859375, lr: 0.001000, avg loss: 0.430587
2022-08-04 05:32:01,122 [dl_trainer.py:822] INFO Epoch 126, lr: 0.001000, val loss: 0.521632, val top-1 acc: 82.832031, top-5 acc: 99.228516
2022-08-04 05:32:01,633 [dl_trainer.py:731] WARNING [126][ 3160/   25][rank:0] loss: 0.491, average forward (0.010048) and backward (0.026139) time: 0.125737, iotime: 0.014298 
2022-08-04 05:32:02,052 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124122, Speed: 2062.485361 images/s
2022-08-04 05:32:02,053 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:02,053 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:02,645 [dl_trainer.py:634] INFO train iter: 3175, num_batches_per_epoch: 25
2022-08-04 05:32:02,646 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 85.421875, lr: 0.001000, avg loss: 0.422815
2022-08-04 05:32:04,164 [dl_trainer.py:822] INFO Epoch 127, lr: 0.001000, val loss: 0.521279, val top-1 acc: 82.792969, top-5 acc: 99.267578
2022-08-04 05:32:05,051 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.124926, Speed: 2049.204973 images/s
2022-08-04 05:32:05,052 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:05,052 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:05,698 [dl_trainer.py:731] WARNING [127][ 3200/   25][rank:0] loss: 0.465, average forward (0.009783) and backward (0.026252) time: 0.089514, iotime: 0.014991 
2022-08-04 05:32:05,711 [dl_trainer.py:634] INFO train iter: 3200, num_batches_per_epoch: 25
2022-08-04 05:32:05,711 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 85.031250, lr: 0.001000, avg loss: 0.430765
2022-08-04 05:32:07,229 [dl_trainer.py:822] INFO Epoch 128, lr: 0.001000, val loss: 0.522036, val top-1 acc: 82.871094, top-5 acc: 99.257812
2022-08-04 05:32:08,088 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.126497, Speed: 2023.755615 images/s
2022-08-04 05:32:08,089 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:08,089 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:08,752 [dl_trainer.py:634] INFO train iter: 3225, num_batches_per_epoch: 25
2022-08-04 05:32:08,753 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 85.250000, lr: 0.001000, avg loss: 0.419482
2022-08-04 05:32:10,239 [dl_trainer.py:822] INFO Epoch 129, lr: 0.001000, val loss: 0.521302, val top-1 acc: 82.822266, top-5 acc: 99.248047
2022-08-04 05:32:11,007 [dl_trainer.py:731] WARNING [129][ 3240/   25][rank:0] loss: 0.342, average forward (0.009756) and backward (0.027154) time: 0.121225, iotime: 0.008297 
2022-08-04 05:32:11,019 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122083, Speed: 2096.939542 images/s
2022-08-04 05:32:11,019 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:11,019 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:11,759 [dl_trainer.py:634] INFO train iter: 3250, num_batches_per_epoch: 25
2022-08-04 05:32:11,760 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 84.265625, lr: 0.001000, avg loss: 0.446283
2022-08-04 05:32:13,212 [dl_trainer.py:822] INFO Epoch 130, lr: 0.001000, val loss: 0.522234, val top-1 acc: 82.900391, top-5 acc: 99.248047
2022-08-04 05:32:13,932 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121362, Speed: 2109.389308 images/s
2022-08-04 05:32:13,933 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:13,933 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:14,719 [dl_trainer.py:634] INFO train iter: 3275, num_batches_per_epoch: 25
2022-08-04 05:32:14,720 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 85.250000, lr: 0.001000, avg loss: 0.426441
2022-08-04 05:32:16,205 [dl_trainer.py:822] INFO Epoch 131, lr: 0.001000, val loss: 0.520914, val top-1 acc: 82.880859, top-5 acc: 99.306641
2022-08-04 05:32:16,455 [dl_trainer.py:731] WARNING [131][ 3280/   25][rank:0] loss: 0.418, average forward (0.009728) and backward (0.026591) time: 0.124294, iotime: 0.013626 
2022-08-04 05:32:16,886 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123035, Speed: 2080.704814 images/s
2022-08-04 05:32:16,886 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:16,886 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:17,748 [dl_trainer.py:634] INFO train iter: 3300, num_batches_per_epoch: 25
2022-08-04 05:32:17,748 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 85.171875, lr: 0.001000, avg loss: 0.424368
2022-08-04 05:32:19,271 [dl_trainer.py:822] INFO Epoch 132, lr: 0.001000, val loss: 0.520622, val top-1 acc: 82.851562, top-5 acc: 99.267578
2022-08-04 05:32:19,906 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.125820, Speed: 2034.645755 images/s
2022-08-04 05:32:19,907 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:19,907 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:20,540 [dl_trainer.py:731] WARNING [132][ 3320/   25][rank:0] loss: 0.415, average forward (0.009386) and backward (0.026578) time: 0.089419, iotime: 0.014459 
2022-08-04 05:32:20,795 [dl_trainer.py:634] INFO train iter: 3325, num_batches_per_epoch: 25
2022-08-04 05:32:20,796 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 85.015625, lr: 0.001000, avg loss: 0.427748
2022-08-04 05:32:22,286 [dl_trainer.py:822] INFO Epoch 133, lr: 0.001000, val loss: 0.520775, val top-1 acc: 82.910156, top-5 acc: 99.296875
2022-08-04 05:32:22,862 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.123123, Speed: 2079.226455 images/s
2022-08-04 05:32:22,863 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:22,863 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:23,787 [dl_trainer.py:634] INFO train iter: 3350, num_batches_per_epoch: 25
2022-08-04 05:32:23,787 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 85.359375, lr: 0.001000, avg loss: 0.433266
2022-08-04 05:32:25,269 [dl_trainer.py:822] INFO Epoch 134, lr: 0.001000, val loss: 0.521209, val top-1 acc: 82.890625, top-5 acc: 99.238281
2022-08-04 05:32:25,793 [dl_trainer.py:731] WARNING [134][ 3360/   25][rank:0] loss: 0.403, average forward (0.009230) and backward (0.026990) time: 0.119629, iotime: 0.008249 
2022-08-04 05:32:25,799 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.122347, Speed: 2092.410871 images/s
2022-08-04 05:32:25,800 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:25,800 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:32:26,760 [dl_trainer.py:634] INFO train iter: 3375, num_batches_per_epoch: 25
2022-08-04 05:32:26,761 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 84.718750, lr: 0.001000, avg loss: 0.432517
2022-08-04 05:32:28,232 [dl_trainer.py:822] INFO Epoch 135, lr: 0.001000, val loss: 0.520615, val top-1 acc: 82.958984, top-5 acc: 99.228516
2022-08-04 05:32:28,707 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.121100, Speed: 2113.961850 images/s
2022-08-04 05:32:28,707 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:32:28,707 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]

2022-08-04 04:43:13,055 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=64, compressor='topk', data_dir='./data', dataset='cifar10', density=0.001, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 04:43:33,887 [dl_trainer.py:254] INFO num_batches_per_epoch: 98
2022-08-04 04:43:34,488 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 04:43:34,489 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 04:43:34,490 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 04:43:34,490 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 04:43:35,226 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 04:43:38,686 [dl_trainer.py:731] WARNING [  0][   40/   98][rank:0] loss: 2.224, average forward (0.040321) and backward (0.029174) time: 0.071936, iotime: 0.001965 
2022-08-04 04:43:38,744 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.085750, Speed: 746.351954 images/s
2022-08-04 04:43:40,771 [dl_trainer.py:731] WARNING [  0][   80/   98][rank:0] loss: 1.956, average forward (0.010106) and backward (0.021510) time: 0.033625, iotime: 0.001729 
2022-08-04 04:43:40,835 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052251, Speed: 1224.857238 images/s
2022-08-04 04:43:41,663 [distributed_optimizer.py:142] INFO Average number of selected gradients: 4214.000000, exact k: 1078
2022-08-04 04:43:41,663 [distributed_optimizer.py:143] INFO The number of selected gradients: [4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214]
2022-08-04 04:43:41,872 [dl_trainer.py:634] INFO train iter: 98, num_batches_per_epoch: 98
2022-08-04 04:43:41,872 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 21.364796, lr: 0.020163, avg loss: 2.334276
2022-08-04 04:43:43,544 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020163, val loss: 1.792706, val top-1 acc: 31.050955, top-5 acc: 85.947452
2022-08-04 04:43:44,618 [dl_trainer.py:731] WARNING [  1][  120/   98][rank:0] loss: 1.813, average forward (0.010034) and backward (0.023032) time: 0.080488, iotime: 0.005280 
2022-08-04 04:43:45,516 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082114, Speed: 779.402158 images/s
2022-08-04 04:43:46,574 [dl_trainer.py:731] WARNING [  1][  160/   98][rank:0] loss: 1.787, average forward (0.009654) and backward (0.022498) time: 0.034069, iotime: 0.001660 
2022-08-04 04:43:47,460 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048577, Speed: 1317.484977 images/s
2022-08-04 04:43:48,255 [distributed_optimizer.py:142] INFO Average number of selected gradients: 1078.000000, exact k: 269
2022-08-04 04:43:48,256 [distributed_optimizer.py:143] INFO The number of selected gradients: [1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]
2022-08-04 04:43:48,574 [dl_trainer.py:634] INFO train iter: 196, num_batches_per_epoch: 98
2022-08-04 04:43:48,575 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 32.955995, lr: 0.040122, avg loss: 1.797900
2022-08-04 04:43:50,231 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040122, val loss: 2.008488, val top-1 acc: 29.637739, top-5 acc: 86.037022
2022-08-04 04:43:50,453 [dl_trainer.py:731] WARNING [  2][  200/   98][rank:0] loss: 1.953, average forward (0.009319) and backward (0.023488) time: 0.081750, iotime: 0.006040 
2022-08-04 04:43:52,180 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082768, Speed: 773.249245 images/s
2022-08-04 04:43:52,417 [dl_trainer.py:731] WARNING [  2][  240/   98][rank:0] loss: 2.149, average forward (0.008678) and backward (0.023072) time: 0.033497, iotime: 0.001514 
2022-08-04 04:43:54,138 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048960, Speed: 1307.194468 images/s
2022-08-04 04:43:54,372 [dl_trainer.py:731] WARNING [  2][  280/   98][rank:0] loss: 1.880, average forward (0.009336) and backward (0.022960) time: 0.034200, iotime: 0.001631 
2022-08-04 04:43:54,916 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:43:54,916 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:43:55,263 [dl_trainer.py:634] INFO train iter: 294, num_batches_per_epoch: 98
2022-08-04 04:43:55,264 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 30.245536, lr: 0.060082, avg loss: 1.930392
2022-08-04 04:43:56,983 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060082, val loss: 2.367447, val top-1 acc: 25.865844, top-5 acc: 81.767516
2022-08-04 04:43:58,195 [dl_trainer.py:731] WARNING [  3][  320/   98][rank:0] loss: 2.077, average forward (0.010983) and backward (0.023760) time: 0.084552, iotime: 0.006469 
2022-08-04 04:43:58,786 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081507, Speed: 785.209630 images/s
2022-08-04 04:44:00,128 [dl_trainer.py:731] WARNING [  3][  360/   98][rank:0] loss: 1.670, average forward (0.010219) and backward (0.022290) time: 0.034486, iotime: 0.001718 
2022-08-04 04:44:00,765 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049471, Speed: 1293.697424 images/s
2022-08-04 04:44:01,553 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:44:01,554 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:44:01,929 [dl_trainer.py:634] INFO train iter: 392, num_batches_per_epoch: 98
2022-08-04 04:44:01,930 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 33.482143, lr: 0.080041, avg loss: 1.800913
2022-08-04 04:44:03,521 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080041, val loss: 1.831844, val top-1 acc: 33.668392, top-5 acc: 87.908041
2022-08-04 04:44:03,923 [dl_trainer.py:731] WARNING [  4][  400/   98][rank:0] loss: 1.663, average forward (0.009390) and backward (0.023895) time: 0.080321, iotime: 0.006439 
2022-08-04 04:44:05,346 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080342, Speed: 796.593604 images/s
2022-08-04 04:44:05,868 [dl_trainer.py:731] WARNING [  4][  440/   98][rank:0] loss: 1.516, average forward (0.008715) and backward (0.024035) time: 0.034534, iotime: 0.001545 
2022-08-04 04:44:07,308 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049027, Speed: 1305.401667 images/s
2022-08-04 04:44:07,853 [dl_trainer.py:731] WARNING [  4][  480/   98][rank:0] loss: 1.571, average forward (0.009448) and backward (0.023405) time: 0.034712, iotime: 0.001595 
2022-08-04 04:44:08,115 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:44:08,115 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:44:08,516 [dl_trainer.py:634] INFO train iter: 490, num_batches_per_epoch: 98
2022-08-04 04:44:08,516 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 36.495536, lr: 0.100000, avg loss: 1.666480
2022-08-04 04:44:10,137 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 1.795737, val top-1 acc: 37.878185, top-5 acc: 88.445462
2022-08-04 04:44:11,611 [dl_trainer.py:731] WARNING [  5][  520/   98][rank:0] loss: 1.576, average forward (0.009261) and backward (0.024782) time: 0.081035, iotime: 0.006073 
2022-08-04 04:44:11,929 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081044, Speed: 789.690569 images/s
2022-08-04 04:44:13,577 [dl_trainer.py:731] WARNING [  5][  560/   98][rank:0] loss: 1.308, average forward (0.009662) and backward (0.025005) time: 0.036553, iotime: 0.001614 
2022-08-04 04:44:13,885 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048904, Speed: 1308.686839 images/s
2022-08-04 04:44:14,653 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:44:14,653 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:44:15,108 [dl_trainer.py:634] INFO train iter: 588, num_batches_per_epoch: 98
2022-08-04 04:44:15,108 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 43.558673, lr: 0.100000, avg loss: 1.535229
2022-08-04 04:44:16,690 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 1.531991, val top-1 acc: 44.496417, top-5 acc: 92.028264
2022-08-04 04:44:17,275 [dl_trainer.py:731] WARNING [  6][  600/   98][rank:0] loss: 1.658, average forward (0.010030) and backward (0.021082) time: 0.077752, iotime: 0.006346 
2022-08-04 04:44:18,414 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079442, Speed: 805.620687 images/s
2022-08-04 04:44:19,231 [dl_trainer.py:731] WARNING [  6][  640/   98][rank:0] loss: 1.518, average forward (0.008334) and backward (0.022846) time: 0.032849, iotime: 0.001458 
2022-08-04 04:44:20,370 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048873, Speed: 1309.513272 images/s
2022-08-04 04:44:21,188 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:44:21,189 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:44:21,413 [dl_trainer.py:731] WARNING [  6][  680/   98][rank:0] loss: 1.431, average forward (0.009533) and backward (0.023576) time: 0.039624, iotime: 0.006258 
2022-08-04 04:44:21,732 [dl_trainer.py:634] INFO train iter: 686, num_batches_per_epoch: 98
2022-08-04 04:44:21,733 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 47.480867, lr: 0.100000, avg loss: 1.442792
2022-08-04 04:44:23,377 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 1.585028, val top-1 acc: 43.073248, top-5 acc: 90.744427
2022-08-04 04:52:31,367 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=64, compressor='topk', data_dir='./data', dataset='cifar10', density=0.001, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=8, nwpernode=8, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2022-08-04 04:52:54,954 [dl_trainer.py:254] INFO num_batches_per_epoch: 98
2022-08-04 04:52:55,058 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2022-08-04 04:52:55,060 [distributed_optimizer.py:323] INFO # of parameters: 269722
2022-08-04 04:52:55,061 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2022-08-04 04:52:55,061 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2022-08-04 04:52:55,827 [dist_trainer.py:62] INFO max_epochs: 141
2022-08-04 04:52:59,684 [dl_trainer.py:731] WARNING [  0][   40/   98][rank:0] loss: 2.132, average forward (0.051634) and backward (0.028518) time: 0.082338, iotime: 0.001731 
2022-08-04 04:52:59,755 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.095775, Speed: 668.234464 images/s
2022-08-04 04:53:01,743 [dl_trainer.py:731] WARNING [  0][   80/   98][rank:0] loss: 1.882, average forward (0.009343) and backward (0.022408) time: 0.033538, iotime: 0.001561 
2022-08-04 04:53:01,824 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051711, Speed: 1237.651571 images/s
2022-08-04 04:53:02,670 [distributed_optimizer.py:142] INFO Average number of selected gradients: 4214.000000, exact k: 1078
2022-08-04 04:53:02,670 [distributed_optimizer.py:143] INFO The number of selected gradients: [4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214]
2022-08-04 04:53:02,869 [dl_trainer.py:634] INFO train iter: 98, num_batches_per_epoch: 98
2022-08-04 04:53:02,870 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 24.441964, lr: 0.020163, avg loss: 2.097614
2022-08-04 04:53:04,485 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020163, val loss: 2.106532, val top-1 acc: 28.473328, top-5 acc: 82.832404
2022-08-04 04:53:05,576 [dl_trainer.py:731] WARNING [  1][  120/   98][rank:0] loss: 1.648, average forward (0.011171) and backward (0.024039) time: 0.081217, iotime: 0.005264 
2022-08-04 04:53:06,470 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081496, Speed: 785.310422 images/s
2022-08-04 04:53:07,551 [dl_trainer.py:731] WARNING [  1][  160/   98][rank:0] loss: 1.975, average forward (0.008875) and backward (0.022639) time: 0.033287, iotime: 0.001509 
2022-08-04 04:53:08,471 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050016, Speed: 1279.583723 images/s
2022-08-04 04:53:09,243 [distributed_optimizer.py:142] INFO Average number of selected gradients: 1078.000000, exact k: 269
2022-08-04 04:53:09,243 [distributed_optimizer.py:143] INFO The number of selected gradients: [1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]
2022-08-04 04:53:09,522 [dl_trainer.py:634] INFO train iter: 196, num_batches_per_epoch: 98
2022-08-04 04:53:09,524 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 35.172194, lr: 0.040122, avg loss: 1.746095
2022-08-04 04:53:11,203 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040122, val loss: 2.323675, val top-1 acc: 28.941083, top-5 acc: 83.449443
2022-08-04 04:53:11,403 [dl_trainer.py:731] WARNING [  2][  200/   98][rank:0] loss: 1.973, average forward (0.010544) and backward (0.023265) time: 0.082809, iotime: 0.006008 
2022-08-04 04:53:13,171 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082422, Speed: 776.488470 images/s
2022-08-04 04:53:13,402 [dl_trainer.py:731] WARNING [  2][  240/   98][rank:0] loss: 2.509, average forward (0.009547) and backward (0.023336) time: 0.034683, iotime: 0.001560 
2022-08-04 04:53:15,168 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049917, Speed: 1282.122502 images/s
2022-08-04 04:53:15,397 [dl_trainer.py:731] WARNING [  2][  280/   98][rank:0] loss: 2.077, average forward (0.008590) and backward (0.022056) time: 0.032307, iotime: 0.001449 
2022-08-04 04:53:15,966 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:15,966 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:16,287 [dl_trainer.py:634] INFO train iter: 294, num_batches_per_epoch: 98
2022-08-04 04:53:16,288 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 32.095026, lr: 0.060082, avg loss: 1.965516
2022-08-04 04:53:17,911 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060082, val loss: 3.237901, val top-1 acc: 23.298169, top-5 acc: 77.149682
2022-08-04 04:53:19,172 [dl_trainer.py:731] WARNING [  3][  320/   98][rank:0] loss: 1.769, average forward (0.010627) and backward (0.024167) time: 0.081394, iotime: 0.005695 
2022-08-04 04:53:19,757 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080487, Speed: 795.159794 images/s
2022-08-04 04:53:21,133 [dl_trainer.py:731] WARNING [  3][  360/   98][rank:0] loss: 1.744, average forward (0.009587) and backward (0.025676) time: 0.037179, iotime: 0.001669 
2022-08-04 04:53:21,743 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049648, Speed: 1289.075120 images/s
2022-08-04 04:53:22,528 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:22,528 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:22,898 [dl_trainer.py:634] INFO train iter: 392, num_batches_per_epoch: 98
2022-08-04 04:53:22,899 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 35.921556, lr: 0.080041, avg loss: 1.738862
2022-08-04 04:53:24,516 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080041, val loss: 2.020493, val top-1 acc: 35.638933, top-5 acc: 87.679140
2022-08-04 04:53:24,899 [dl_trainer.py:731] WARNING [  4][  400/   98][rank:0] loss: 1.611, average forward (0.009848) and backward (0.025758) time: 0.082531, iotime: 0.005721 
2022-08-04 04:53:26,342 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080658, Speed: 793.476922 images/s
2022-08-04 04:53:26,886 [dl_trainer.py:731] WARNING [  4][  440/   98][rank:0] loss: 1.537, average forward (0.009764) and backward (0.024806) time: 0.036490, iotime: 0.001656 
2022-08-04 04:53:28,317 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049371, Speed: 1296.299147 images/s
2022-08-04 04:53:28,848 [dl_trainer.py:731] WARNING [  4][  480/   98][rank:0] loss: 1.333, average forward (0.009861) and backward (0.024757) time: 0.036540, iotime: 0.001649 
2022-08-04 04:53:29,096 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:29,096 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:29,488 [dl_trainer.py:634] INFO train iter: 490, num_batches_per_epoch: 98
2022-08-04 04:53:29,489 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 43.160077, lr: 0.100000, avg loss: 1.530678
2022-08-04 04:53:31,150 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 1.481275, val top-1 acc: 47.123806, top-5 acc: 93.083201
2022-08-04 04:53:32,629 [dl_trainer.py:731] WARNING [  5][  520/   98][rank:0] loss: 1.330, average forward (0.010441) and backward (0.025492) time: 0.083890, iotime: 0.006040 
2022-08-04 04:53:32,930 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080903, Speed: 791.068386 images/s
2022-08-04 04:53:34,611 [dl_trainer.py:731] WARNING [  5][  560/   98][rank:0] loss: 1.140, average forward (0.011501) and backward (0.023414) time: 0.037004, iotime: 0.001801 
2022-08-04 04:53:34,912 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049544, Speed: 1291.775121 images/s
2022-08-04 04:53:35,723 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:35,724 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:36,188 [dl_trainer.py:634] INFO train iter: 588, num_batches_per_epoch: 98
2022-08-04 04:53:36,189 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 48.485332, lr: 0.100000, avg loss: 1.391431
2022-08-04 04:53:37,808 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 1.576618, val top-1 acc: 48.029459, top-5 acc: 90.425955
2022-08-04 04:53:38,384 [dl_trainer.py:731] WARNING [  6][  600/   98][rank:0] loss: 1.212, average forward (0.009989) and backward (0.024540) time: 0.081525, iotime: 0.005722 
2022-08-04 04:53:39,555 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081407, Speed: 786.171492 images/s
2022-08-04 04:53:40,371 [dl_trainer.py:731] WARNING [  6][  640/   98][rank:0] loss: 1.411, average forward (0.010339) and backward (0.025261) time: 0.037547, iotime: 0.001677 
2022-08-04 04:53:41,515 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049004, Speed: 1306.007881 images/s
2022-08-04 04:53:42,307 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:42,307 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:42,524 [dl_trainer.py:731] WARNING [  6][  680/   98][rank:0] loss: 1.261, average forward (0.010169) and backward (0.023899) time: 0.040396, iotime: 0.006066 
2022-08-04 04:53:42,841 [dl_trainer.py:634] INFO train iter: 686, num_batches_per_epoch: 98
2022-08-04 04:53:42,842 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 54.830995, lr: 0.100000, avg loss: 1.263377
2022-08-04 04:53:44,573 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 1.223171, val top-1 acc: 56.827229, top-5 acc: 94.834793
2022-08-04 04:53:46,276 [dl_trainer.py:731] WARNING [  7][  720/   98][rank:0] loss: 1.143, average forward (0.010204) and backward (0.023969) time: 0.079486, iotime: 0.001684 
2022-08-04 04:53:46,284 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083640, Speed: 765.187774 images/s
2022-08-04 04:53:48,227 [dl_trainer.py:731] WARNING [  7][  760/   98][rank:0] loss: 1.279, average forward (0.009529) and backward (0.024058) time: 0.035455, iotime: 0.001617 
2022-08-04 04:53:48,233 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048703, Speed: 1314.080112 images/s
2022-08-04 04:53:49,007 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:49,007 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:49,579 [dl_trainer.py:634] INFO train iter: 784, num_batches_per_epoch: 98
2022-08-04 04:53:49,580 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 57.669005, lr: 0.100000, avg loss: 1.164242
2022-08-04 04:53:51,231 [dl_trainer.py:822] INFO Epoch 8, lr: 0.100000, val loss: 1.117555, val top-1 acc: 59.593949, top-5 acc: 96.188296
2022-08-04 04:53:52,038 [dl_trainer.py:731] WARNING [  8][  800/   98][rank:0] loss: 1.406, average forward (0.009649) and backward (0.024512) time: 0.082438, iotime: 0.006183 
2022-08-04 04:53:52,906 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081967, Speed: 780.802325 images/s
2022-08-04 04:53:54,033 [dl_trainer.py:731] WARNING [  8][  840/   98][rank:0] loss: 1.197, average forward (0.009514) and backward (0.023437) time: 0.034799, iotime: 0.001593 
2022-08-04 04:53:54,861 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048857, Speed: 1309.947016 images/s
2022-08-04 04:53:55,670 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:53:55,670 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:53:56,159 [dl_trainer.py:731] WARNING [  8][  880/   98][rank:0] loss: 0.883, average forward (0.009771) and backward (0.022979) time: 0.038716, iotime: 0.005712 
2022-08-04 04:53:56,273 [dl_trainer.py:634] INFO train iter: 882, num_batches_per_epoch: 98
2022-08-04 04:53:56,274 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 60.778061, lr: 0.100000, avg loss: 1.089887
2022-08-04 04:53:58,007 [dl_trainer.py:822] INFO Epoch 9, lr: 0.100000, val loss: 1.215279, val top-1 acc: 57.434315, top-5 acc: 95.770303
2022-08-04 04:53:59,607 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083244, Speed: 768.828298 images/s
2022-08-04 04:53:59,902 [dl_trainer.py:731] WARNING [  9][  920/   98][rank:0] loss: 1.152, average forward (0.009557) and backward (0.025465) time: 0.080357, iotime: 0.001666 
2022-08-04 04:54:01,601 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049847, Speed: 1283.928954 images/s
2022-08-04 04:54:01,893 [dl_trainer.py:731] WARNING [  9][  960/   98][rank:0] loss: 0.875, average forward (0.009233) and backward (0.024972) time: 0.036045, iotime: 0.001587 
2022-08-04 04:54:02,391 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:02,391 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:03,066 [dl_trainer.py:634] INFO train iter: 980, num_batches_per_epoch: 98
2022-08-04 04:54:03,067 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 62.691327, lr: 0.100000, avg loss: 1.042709
2022-08-04 04:54:04,736 [dl_trainer.py:822] INFO Epoch 10, lr: 0.100000, val loss: 1.070913, val top-1 acc: 62.400478, top-5 acc: 96.775478
2022-08-04 04:54:05,718 [dl_trainer.py:731] WARNING [ 10][ 1000/   98][rank:0] loss: 1.154, average forward (0.008997) and backward (0.020900) time: 0.078418, iotime: 0.005917 
2022-08-04 04:54:06,283 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082106, Speed: 779.482721 images/s
2022-08-04 04:54:07,653 [dl_trainer.py:731] WARNING [ 10][ 1040/   98][rank:0] loss: 0.778, average forward (0.009449) and backward (0.023088) time: 0.034448, iotime: 0.001629 
2022-08-04 04:54:08,199 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047905, Speed: 1335.981000 images/s
2022-08-04 04:54:08,973 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:08,974 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:09,691 [dl_trainer.py:634] INFO train iter: 1078, num_batches_per_epoch: 98
2022-08-04 04:54:09,692 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 64.190051, lr: 0.100000, avg loss: 0.993643
2022-08-04 04:54:11,323 [dl_trainer.py:822] INFO Epoch 11, lr: 0.100000, val loss: 1.054379, val top-1 acc: 63.684315, top-5 acc: 96.865048
2022-08-04 04:54:11,426 [dl_trainer.py:731] WARNING [ 11][ 1080/   98][rank:0] loss: 0.952, average forward (0.010668) and backward (0.024715) time: 0.082465, iotime: 0.005930 
2022-08-04 04:54:12,850 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081562, Speed: 784.676882 images/s
2022-08-04 04:54:13,450 [dl_trainer.py:731] WARNING [ 11][ 1120/   98][rank:0] loss: 0.849, average forward (0.011052) and backward (0.026291) time: 0.039455, iotime: 0.001821 
2022-08-04 04:54:14,866 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050387, Speed: 1270.173825 images/s
2022-08-04 04:54:15,463 [dl_trainer.py:731] WARNING [ 11][ 1160/   98][rank:0] loss: 0.735, average forward (0.009133) and backward (0.025363) time: 0.036306, iotime: 0.001562 
2022-08-04 04:54:15,672 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:15,673 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:16,451 [dl_trainer.py:634] INFO train iter: 1176, num_batches_per_epoch: 98
2022-08-04 04:54:16,451 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 65.975765, lr: 0.100000, avg loss: 0.940076
2022-08-04 04:54:18,030 [dl_trainer.py:822] INFO Epoch 12, lr: 0.100000, val loss: 1.009930, val top-1 acc: 64.769108, top-5 acc: 96.646099
2022-08-04 04:54:19,182 [dl_trainer.py:731] WARNING [ 12][ 1200/   98][rank:0] loss: 0.926, average forward (0.010504) and backward (0.024373) time: 0.081187, iotime: 0.005994 
2022-08-04 04:54:19,397 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.079472, Speed: 805.312980 images/s
2022-08-04 04:54:21,033 [dl_trainer.py:731] WARNING [ 12][ 1240/   98][rank:0] loss: 0.730, average forward (0.009073) and backward (0.023420) time: 0.034299, iotime: 0.001575 
2022-08-04 04:54:21,287 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047236, Speed: 1354.912783 images/s
2022-08-04 04:54:22,074 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:22,074 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:22,911 [dl_trainer.py:634] INFO train iter: 1274, num_batches_per_epoch: 98
2022-08-04 04:54:22,912 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 68.287628, lr: 0.100000, avg loss: 0.906357
2022-08-04 04:54:24,637 [dl_trainer.py:822] INFO Epoch 13, lr: 0.100000, val loss: 1.087780, val top-1 acc: 63.714172, top-5 acc: 97.263137
2022-08-04 04:54:24,930 [dl_trainer.py:731] WARNING [ 13][ 1280/   98][rank:0] loss: 0.837, average forward (0.009879) and backward (0.023619) time: 0.082763, iotime: 0.005789 
2022-08-04 04:54:26,058 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083693, Speed: 764.698847 images/s
2022-08-04 04:54:26,947 [dl_trainer.py:731] WARNING [ 13][ 1320/   98][rank:0] loss: 0.968, average forward (0.010025) and backward (0.024330) time: 0.036294, iotime: 0.001682 
2022-08-04 04:54:28,034 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049389, Speed: 1295.829977 images/s
2022-08-04 04:54:28,823 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:28,823 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:29,091 [dl_trainer.py:731] WARNING [ 13][ 1360/   98][rank:0] loss: 0.884, average forward (0.009406) and backward (0.022537) time: 0.037862, iotime: 0.005680 
2022-08-04 04:54:29,685 [dl_trainer.py:634] INFO train iter: 1372, num_batches_per_epoch: 98
2022-08-04 04:54:29,686 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 69.435587, lr: 0.100000, avg loss: 0.859182
2022-08-04 04:54:31,334 [dl_trainer.py:822] INFO Epoch 14, lr: 0.100000, val loss: 0.964614, val top-1 acc: 67.774682, top-5 acc: 97.691083
2022-08-04 04:54:32,605 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080161, Speed: 798.389708 images/s
2022-08-04 04:54:32,635 [dl_trainer.py:731] WARNING [ 14][ 1400/   98][rank:0] loss: 0.756, average forward (0.009564) and backward (0.022546) time: 0.075689, iotime: 0.001590 
2022-08-04 04:54:34,594 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049726, Speed: 1287.044169 images/s
2022-08-04 04:54:34,629 [dl_trainer.py:731] WARNING [ 14][ 1440/   98][rank:0] loss: 0.696, average forward (0.008285) and backward (0.023903) time: 0.033921, iotime: 0.001509 
2022-08-04 04:54:35,365 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:35,365 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:36,244 [dl_trainer.py:634] INFO train iter: 1470, num_batches_per_epoch: 98
2022-08-04 04:54:36,244 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 72.082270, lr: 0.100000, avg loss: 0.802235
2022-08-04 04:54:37,899 [dl_trainer.py:822] INFO Epoch 15, lr: 0.100000, val loss: 0.992408, val top-1 acc: 66.431131, top-5 acc: 97.312898
2022-08-04 04:54:38,416 [dl_trainer.py:731] WARNING [ 15][ 1480/   98][rank:0] loss: 0.443, average forward (0.010528) and backward (0.023937) time: 0.082176, iotime: 0.005992 
2022-08-04 04:54:39,173 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080302, Speed: 796.995467 images/s
2022-08-04 04:54:40,348 [dl_trainer.py:731] WARNING [ 15][ 1520/   98][rank:0] loss: 0.779, average forward (0.009945) and backward (0.024563) time: 0.036374, iotime: 0.001605 
2022-08-04 04:54:41,149 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049391, Speed: 1295.771023 images/s
2022-08-04 04:54:41,940 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:41,940 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:42,487 [dl_trainer.py:731] WARNING [ 15][ 1560/   98][rank:0] loss: 0.640, average forward (0.009965) and backward (0.024570) time: 0.040635, iotime: 0.005836 
2022-08-04 04:54:42,920 [dl_trainer.py:634] INFO train iter: 1568, num_batches_per_epoch: 98
2022-08-04 04:54:42,921 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 71.492347, lr: 0.100000, avg loss: 0.799509
2022-08-04 04:54:44,605 [dl_trainer.py:822] INFO Epoch 16, lr: 0.100000, val loss: 0.948379, val top-1 acc: 66.918790, top-5 acc: 97.929936
2022-08-04 04:54:45,862 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082664, Speed: 774.219538 images/s
2022-08-04 04:54:46,193 [dl_trainer.py:731] WARNING [ 16][ 1600/   98][rank:0] loss: 0.875, average forward (0.008843) and backward (0.025509) time: 0.078768, iotime: 0.001514 
2022-08-04 04:54:47,864 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050033, Speed: 1279.151259 images/s
2022-08-04 04:54:48,165 [dl_trainer.py:731] WARNING [ 16][ 1640/   98][rank:0] loss: 0.593, average forward (0.008933) and backward (0.022941) time: 0.033603, iotime: 0.001497 
2022-08-04 04:54:48,627 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:48,627 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:49,618 [dl_trainer.py:634] INFO train iter: 1666, num_batches_per_epoch: 98
2022-08-04 04:54:49,618 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 73.485332, lr: 0.100000, avg loss: 0.754743
2022-08-04 04:54:51,273 [dl_trainer.py:822] INFO Epoch 17, lr: 0.100000, val loss: 0.896862, val top-1 acc: 68.779857, top-5 acc: 97.820462
2022-08-04 04:54:51,994 [dl_trainer.py:731] WARNING [ 17][ 1680/   98][rank:0] loss: 0.697, average forward (0.009700) and backward (0.024061) time: 0.081168, iotime: 0.005699 
2022-08-04 04:54:52,510 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081480, Speed: 785.467445 images/s
2022-08-04 04:54:53,963 [dl_trainer.py:731] WARNING [ 17][ 1720/   98][rank:0] loss: 0.777, average forward (0.009932) and backward (0.023673) time: 0.035467, iotime: 0.001599 
2022-08-04 04:54:54,509 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049970, Speed: 1280.776227 images/s
2022-08-04 04:54:55,288 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:54:55,288 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:54:56,135 [dl_trainer.py:731] WARNING [ 17][ 1760/   98][rank:0] loss: 0.979, average forward (0.009884) and backward (0.024507) time: 0.040583, iotime: 0.005936 
2022-08-04 04:54:56,344 [dl_trainer.py:634] INFO train iter: 1764, num_batches_per_epoch: 98
2022-08-04 04:54:56,344 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 73.979592, lr: 0.100000, avg loss: 0.755963
2022-08-04 04:54:58,020 [dl_trainer.py:822] INFO Epoch 18, lr: 0.100000, val loss: 0.758675, val top-1 acc: 73.895303, top-5 acc: 98.278264
2022-08-04 04:54:59,186 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082038, Speed: 780.128262 images/s
2022-08-04 04:54:59,818 [dl_trainer.py:731] WARNING [ 18][ 1800/   98][rank:0] loss: 0.788, average forward (0.010743) and backward (0.024515) time: 0.079736, iotime: 0.001732 
2022-08-04 04:55:01,174 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049682, Speed: 1288.185559 images/s
2022-08-04 04:55:01,806 [dl_trainer.py:731] WARNING [ 18][ 1840/   98][rank:0] loss: 0.828, average forward (0.009592) and backward (0.023914) time: 0.035363, iotime: 0.001596 
2022-08-04 04:55:01,972 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:01,972 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:03,054 [dl_trainer.py:634] INFO train iter: 1862, num_batches_per_epoch: 98
2022-08-04 04:55:03,054 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 75.940689, lr: 0.100000, avg loss: 0.698219
2022-08-04 04:55:04,799 [dl_trainer.py:822] INFO Epoch 19, lr: 0.100000, val loss: 0.694923, val top-1 acc: 76.363455, top-5 acc: 98.636545
2022-08-04 04:55:05,659 [dl_trainer.py:731] WARNING [ 19][ 1880/   98][rank:0] loss: 0.773, average forward (0.010684) and backward (0.023195) time: 0.084143, iotime: 0.006304 
2022-08-04 04:55:05,868 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082317, Speed: 777.479833 images/s
2022-08-04 04:55:07,654 [dl_trainer.py:731] WARNING [ 19][ 1920/   98][rank:0] loss: 0.933, average forward (0.008681) and backward (0.023185) time: 0.033603, iotime: 0.001497 
2022-08-04 04:55:07,878 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050230, Speed: 1274.128165 images/s
2022-08-04 04:55:08,646 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:08,647 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:09,770 [dl_trainer.py:731] WARNING [ 19][ 1960/   98][rank:0] loss: 0.651, average forward (0.009448) and backward (0.024178) time: 0.039403, iotime: 0.005534 
2022-08-04 04:55:09,784 [dl_trainer.py:634] INFO train iter: 1960, num_batches_per_epoch: 98
2022-08-04 04:55:09,784 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 76.116071, lr: 0.100000, avg loss: 0.671543
2022-08-04 04:55:11,407 [dl_trainer.py:822] INFO Epoch 20, lr: 0.100000, val loss: 0.777966, val top-1 acc: 74.412818, top-5 acc: 98.527070
2022-08-04 04:55:12,469 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080522, Speed: 794.813450 images/s
2022-08-04 04:55:13,425 [dl_trainer.py:731] WARNING [ 20][ 2000/   98][rank:0] loss: 0.618, average forward (0.009684) and backward (0.023886) time: 0.076592, iotime: 0.001580 
2022-08-04 04:55:14,493 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050591, Speed: 1265.045853 images/s
2022-08-04 04:55:15,276 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:15,276 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:15,573 [dl_trainer.py:731] WARNING [ 20][ 2040/   98][rank:0] loss: 0.677, average forward (0.009946) and backward (0.024187) time: 0.040225, iotime: 0.005843 
2022-08-04 04:55:16,494 [dl_trainer.py:634] INFO train iter: 2058, num_batches_per_epoch: 98
2022-08-04 04:55:16,495 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 77.838010, lr: 0.100000, avg loss: 0.641584
2022-08-04 04:55:18,118 [dl_trainer.py:822] INFO Epoch 21, lr: 0.100000, val loss: 0.722144, val top-1 acc: 75.835987, top-5 acc: 98.397691
2022-08-04 04:55:19,128 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081290, Speed: 787.305457 images/s
2022-08-04 04:55:19,224 [dl_trainer.py:731] WARNING [ 21][ 2080/   98][rank:0] loss: 0.821, average forward (0.009928) and backward (0.026632) time: 0.079133, iotime: 0.001630 
2022-08-04 04:55:21,109 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049532, Speed: 1292.084147 images/s
2022-08-04 04:55:21,198 [dl_trainer.py:731] WARNING [ 21][ 2120/   98][rank:0] loss: 0.614, average forward (0.009467) and backward (0.025891) time: 0.037204, iotime: 0.001589 
2022-08-04 04:55:21,876 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:21,877 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:23,164 [dl_trainer.py:634] INFO train iter: 2156, num_batches_per_epoch: 98
2022-08-04 04:55:23,165 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 78.762755, lr: 0.100000, avg loss: 0.616024
2022-08-04 04:55:24,773 [dl_trainer.py:822] INFO Epoch 22, lr: 0.100000, val loss: 0.688167, val top-1 acc: 77.219347, top-5 acc: 98.676354
2022-08-04 04:55:24,966 [dl_trainer.py:731] WARNING [ 22][ 2160/   98][rank:0] loss: 0.543, average forward (0.008982) and backward (0.023622) time: 0.079303, iotime: 0.005719 
2022-08-04 04:55:25,718 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080841, Speed: 791.681242 images/s
2022-08-04 04:55:27,007 [dl_trainer.py:731] WARNING [ 22][ 2200/   98][rank:0] loss: 0.453, average forward (0.009380) and backward (0.024262) time: 0.035455, iotime: 0.001568 
2022-08-04 04:55:27,729 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050265, Speed: 1273.260315 images/s
2022-08-04 04:55:28,519 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:28,519 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:29,145 [dl_trainer.py:731] WARNING [ 22][ 2240/   98][rank:0] loss: 0.821, average forward (0.009148) and backward (0.023762) time: 0.038816, iotime: 0.005666 
2022-08-04 04:55:29,881 [dl_trainer.py:634] INFO train iter: 2254, num_batches_per_epoch: 98
2022-08-04 04:55:29,881 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 78.555485, lr: 0.100000, avg loss: 0.607729
2022-08-04 04:55:31,496 [dl_trainer.py:822] INFO Epoch 23, lr: 0.100000, val loss: 0.705398, val top-1 acc: 76.821258, top-5 acc: 98.527070
2022-08-04 04:55:32,400 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081930, Speed: 781.158099 images/s
2022-08-04 04:55:32,803 [dl_trainer.py:731] WARNING [ 23][ 2280/   98][rank:0] loss: 0.744, average forward (0.009968) and backward (0.023997) time: 0.076313, iotime: 0.001650 
2022-08-04 04:55:34,384 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049572, Speed: 1291.055206 images/s
2022-08-04 04:55:34,775 [dl_trainer.py:731] WARNING [ 23][ 2320/   98][rank:0] loss: 0.594, average forward (0.009026) and backward (0.024726) time: 0.035537, iotime: 0.001546 
2022-08-04 04:55:35,185 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:35,185 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:36,532 [dl_trainer.py:634] INFO train iter: 2352, num_batches_per_epoch: 98
2022-08-04 04:55:36,533 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 80.293367, lr: 0.100000, avg loss: 0.569208
2022-08-04 04:55:38,128 [dl_trainer.py:822] INFO Epoch 24, lr: 0.100000, val loss: 0.838696, val top-1 acc: 73.248408, top-5 acc: 98.188694
2022-08-04 04:55:38,526 [dl_trainer.py:731] WARNING [ 24][ 2360/   98][rank:0] loss: 0.558, average forward (0.008749) and backward (0.023509) time: 0.078447, iotime: 0.005599 
2022-08-04 04:55:38,984 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080685, Speed: 793.209711 images/s
2022-08-04 04:55:40,497 [dl_trainer.py:731] WARNING [ 24][ 2400/   98][rank:0] loss: 0.497, average forward (0.008871) and backward (0.024445) time: 0.035044, iotime: 0.001501 
2022-08-04 04:55:40,953 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049202, Speed: 1300.766721 images/s
2022-08-04 04:55:41,765 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:41,765 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:42,674 [dl_trainer.py:731] WARNING [ 24][ 2440/   98][rank:0] loss: 0.675, average forward (0.008774) and backward (0.023586) time: 0.038126, iotime: 0.005544 
2022-08-04 04:55:43,169 [dl_trainer.py:634] INFO train iter: 2450, num_batches_per_epoch: 98
2022-08-04 04:55:43,170 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 80.931122, lr: 0.100000, avg loss: 0.567615
2022-08-04 04:55:44,795 [dl_trainer.py:822] INFO Epoch 25, lr: 0.100000, val loss: 0.637014, val top-1 acc: 79.000796, top-5 acc: 98.706210
2022-08-04 04:55:45,588 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081297, Speed: 787.236352 images/s
2022-08-04 04:55:46,244 [dl_trainer.py:731] WARNING [ 25][ 2480/   98][rank:0] loss: 0.661, average forward (0.010409) and backward (0.024116) time: 0.077208, iotime: 0.001709 
2022-08-04 04:55:47,487 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047466, Speed: 1348.322597 images/s
2022-08-04 04:55:48,187 [dl_trainer.py:731] WARNING [ 25][ 2520/   98][rank:0] loss: 0.521, average forward (0.009488) and backward (0.024788) time: 0.036154, iotime: 0.001631 
2022-08-04 04:55:48,300 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:48,300 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:49,757 [dl_trainer.py:634] INFO train iter: 2548, num_batches_per_epoch: 98
2022-08-04 04:55:49,758 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 82.110969, lr: 0.100000, avg loss: 0.531506
2022-08-04 04:55:51,405 [dl_trainer.py:822] INFO Epoch 26, lr: 0.100000, val loss: 0.684015, val top-1 acc: 77.697054, top-5 acc: 98.736067
2022-08-04 04:55:51,976 [dl_trainer.py:731] WARNING [ 26][ 2560/   98][rank:0] loss: 0.318, average forward (0.010234) and backward (0.022589) time: 0.080854, iotime: 0.005951 
2022-08-04 04:55:52,134 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081500, Speed: 785.275398 images/s
2022-08-04 04:55:53,990 [dl_trainer.py:731] WARNING [ 26][ 2600/   98][rank:0] loss: 0.519, average forward (0.008782) and backward (0.024411) time: 0.034965, iotime: 0.001536 
2022-08-04 04:55:54,150 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050408, Speed: 1269.650705 images/s
2022-08-04 04:55:54,921 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:55:54,921 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:55:56,042 [dl_trainer.py:731] WARNING [ 26][ 2640/   98][rank:0] loss: 0.375, average forward (0.009783) and backward (0.023192) time: 0.038993, iotime: 0.005759 
2022-08-04 04:55:56,340 [dl_trainer.py:634] INFO train iter: 2646, num_batches_per_epoch: 98
2022-08-04 04:55:56,341 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 82.126913, lr: 0.100000, avg loss: 0.513383
2022-08-04 04:55:58,050 [dl_trainer.py:822] INFO Epoch 27, lr: 0.100000, val loss: 0.742907, val top-1 acc: 76.861067, top-5 acc: 98.546975
2022-08-04 04:55:58,729 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080309, Speed: 796.922907 images/s
2022-08-04 04:55:59,700 [dl_trainer.py:731] WARNING [ 27][ 2680/   98][rank:0] loss: 0.614, average forward (0.009842) and backward (0.022920) time: 0.077495, iotime: 0.001679 
2022-08-04 04:56:00,728 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049971, Speed: 1280.751173 images/s
2022-08-04 04:56:01,505 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:01,505 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:01,841 [dl_trainer.py:731] WARNING [ 27][ 2720/   98][rank:0] loss: 0.508, average forward (0.009704) and backward (0.022096) time: 0.038054, iotime: 0.006017 
2022-08-04 04:56:03,036 [dl_trainer.py:634] INFO train iter: 2744, num_batches_per_epoch: 98
2022-08-04 04:56:03,036 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 82.477679, lr: 0.100000, avg loss: 0.498073
2022-08-04 04:56:04,748 [dl_trainer.py:822] INFO Epoch 28, lr: 0.100000, val loss: 0.770367, val top-1 acc: 75.766322, top-5 acc: 98.158838
2022-08-04 04:56:05,438 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082600, Speed: 774.818876 images/s
2022-08-04 04:56:05,573 [dl_trainer.py:731] WARNING [ 28][ 2760/   98][rank:0] loss: 0.447, average forward (0.009253) and backward (0.024600) time: 0.079466, iotime: 0.001599 
2022-08-04 04:56:07,345 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047665, Speed: 1342.711679 images/s
2022-08-04 04:56:07,479 [dl_trainer.py:731] WARNING [ 28][ 2800/   98][rank:0] loss: 0.559, average forward (0.009025) and backward (0.021488) time: 0.032239, iotime: 0.001493 
2022-08-04 04:56:08,150 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:08,151 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:09,610 [dl_trainer.py:731] WARNING [ 28][ 2840/   98][rank:0] loss: 0.566, average forward (0.009482) and backward (0.022201) time: 0.037731, iotime: 0.005808 
2022-08-04 04:56:09,731 [dl_trainer.py:634] INFO train iter: 2842, num_batches_per_epoch: 98
2022-08-04 04:56:09,732 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 81.345663, lr: 0.100000, avg loss: 0.534693
2022-08-04 04:56:11,400 [dl_trainer.py:822] INFO Epoch 29, lr: 0.100000, val loss: 0.823619, val top-1 acc: 75.228901, top-5 acc: 98.517118
2022-08-04 04:56:11,983 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081336, Speed: 786.858309 images/s
2022-08-04 04:56:13,246 [dl_trainer.py:731] WARNING [ 29][ 2880/   98][rank:0] loss: 0.531, average forward (0.010615) and backward (0.023059) time: 0.077435, iotime: 0.001717 
2022-08-04 04:56:13,945 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049057, Speed: 1304.592302 images/s
2022-08-04 04:56:14,740 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:14,740 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:15,400 [dl_trainer.py:731] WARNING [ 29][ 2920/   98][rank:0] loss: 0.607, average forward (0.009683) and backward (0.023368) time: 0.038862, iotime: 0.005561 
2022-08-04 04:56:16,410 [dl_trainer.py:634] INFO train iter: 2940, num_batches_per_epoch: 98
2022-08-04 04:56:16,411 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 82.254464, lr: 0.100000, avg loss: 0.511093
2022-08-04 04:56:18,108 [dl_trainer.py:822] INFO Epoch 30, lr: 0.100000, val loss: 0.871396, val top-1 acc: 74.263535, top-5 acc: 98.208599
2022-08-04 04:56:18,667 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082817, Speed: 772.792155 images/s
2022-08-04 04:56:19,086 [dl_trainer.py:731] WARNING [ 30][ 2960/   98][rank:0] loss: 0.659, average forward (0.009900) and backward (0.024995) time: 0.079778, iotime: 0.001649 
2022-08-04 04:56:20,621 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048832, Speed: 1310.615848 images/s
2022-08-04 04:56:21,047 [dl_trainer.py:731] WARNING [ 30][ 3000/   98][rank:0] loss: 0.379, average forward (0.009862) and backward (0.024753) time: 0.036505, iotime: 0.001627 
2022-08-04 04:56:21,396 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:21,396 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:23,123 [dl_trainer.py:634] INFO train iter: 3038, num_batches_per_epoch: 98
2022-08-04 04:56:23,123 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 82.589286, lr: 0.100000, avg loss: 0.489988
2022-08-04 04:56:24,797 [dl_trainer.py:822] INFO Epoch 31, lr: 0.100000, val loss: 0.684842, val top-1 acc: 78.224522, top-5 acc: 98.875398
2022-08-04 04:56:24,900 [dl_trainer.py:731] WARNING [ 31][ 3040/   98][rank:0] loss: 0.498, average forward (0.009756) and backward (0.023754) time: 0.081546, iotime: 0.005851 
2022-08-04 04:56:25,299 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082040, Speed: 780.107420 images/s
2022-08-04 04:56:26,908 [dl_trainer.py:731] WARNING [ 31][ 3080/   98][rank:0] loss: 0.555, average forward (0.009381) and backward (0.021744) time: 0.032881, iotime: 0.001526 
2022-08-04 04:56:27,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050812, Speed: 1259.537010 images/s
2022-08-04 04:56:28,118 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:28,118 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:29,044 [dl_trainer.py:731] WARNING [ 31][ 3120/   98][rank:0] loss: 0.329, average forward (0.009104) and backward (0.024705) time: 0.039753, iotime: 0.005699 
2022-08-04 04:56:29,866 [dl_trainer.py:634] INFO train iter: 3136, num_batches_per_epoch: 98
2022-08-04 04:56:29,867 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 83.960459, lr: 0.100000, avg loss: 0.456939
2022-08-04 04:56:31,606 [dl_trainer.py:822] INFO Epoch 32, lr: 0.100000, val loss: 0.660674, val top-1 acc: 78.493232, top-5 acc: 98.755971
2022-08-04 04:56:32,047 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082706, Speed: 773.823951 images/s
2022-08-04 04:56:32,747 [dl_trainer.py:731] WARNING [ 32][ 3160/   98][rank:0] loss: 0.655, average forward (0.009767) and backward (0.024731) time: 0.080330, iotime: 0.001688 
2022-08-04 04:56:34,003 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048871, Speed: 1309.580831 images/s
2022-08-04 04:56:34,734 [dl_trainer.py:731] WARNING [ 32][ 3200/   98][rank:0] loss: 0.507, average forward (0.009888) and backward (0.023947) time: 0.035808, iotime: 0.001720 
2022-08-04 04:56:34,801 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:34,802 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:36,570 [dl_trainer.py:634] INFO train iter: 3234, num_batches_per_epoch: 98
2022-08-04 04:56:36,571 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 82.796556, lr: 0.100000, avg loss: 0.494064
2022-08-04 04:56:38,267 [dl_trainer.py:822] INFO Epoch 33, lr: 0.100000, val loss: 0.681493, val top-1 acc: 78.433519, top-5 acc: 97.949841
2022-08-04 04:56:38,564 [dl_trainer.py:731] WARNING [ 33][ 3240/   98][rank:0] loss: 0.442, average forward (0.010685) and backward (0.024786) time: 0.084106, iotime: 0.005886 
2022-08-04 04:56:38,668 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081830, Speed: 782.109814 images/s
2022-08-04 04:56:40,497 [dl_trainer.py:731] WARNING [ 33][ 3280/   98][rank:0] loss: 0.479, average forward (0.010010) and backward (0.023344) time: 0.035305, iotime: 0.001680 
2022-08-04 04:56:40,620 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048787, Speed: 1311.823328 images/s
2022-08-04 04:56:41,406 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:41,406 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:42,670 [dl_trainer.py:731] WARNING [ 33][ 3320/   98][rank:0] loss: 0.714, average forward (0.010295) and backward (0.024438) time: 0.041007, iotime: 0.005993 
2022-08-04 04:56:43,269 [dl_trainer.py:634] INFO train iter: 3332, num_batches_per_epoch: 98
2022-08-04 04:56:43,270 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 83.864796, lr: 0.100000, avg loss: 0.461941
2022-08-04 04:56:44,995 [dl_trainer.py:822] INFO Epoch 34, lr: 0.100000, val loss: 0.585369, val top-1 acc: 80.881768, top-5 acc: 98.775876
2022-08-04 04:56:45,361 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083151, Speed: 769.682814 images/s
2022-08-04 04:56:46,407 [dl_trainer.py:731] WARNING [ 34][ 3360/   98][rank:0] loss: 0.295, average forward (0.009840) and backward (0.025793) time: 0.081225, iotime: 0.001675 
2022-08-04 04:56:47,368 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050163, Speed: 1275.838767 images/s
2022-08-04 04:56:48,172 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:48,173 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:48,583 [dl_trainer.py:731] WARNING [ 34][ 3400/   98][rank:0] loss: 0.313, average forward (0.010053) and backward (0.023760) time: 0.039886, iotime: 0.005814 
2022-08-04 04:56:50,096 [dl_trainer.py:634] INFO train iter: 3430, num_batches_per_epoch: 98
2022-08-04 04:56:50,096 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 84.566327, lr: 0.100000, avg loss: 0.439490
2022-08-04 04:56:51,738 [dl_trainer.py:822] INFO Epoch 35, lr: 0.100000, val loss: 0.533477, val top-1 acc: 82.145701, top-5 acc: 99.144108
2022-08-04 04:56:52,032 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081800, Speed: 782.396201 images/s
2022-08-04 04:56:52,219 [dl_trainer.py:731] WARNING [ 35][ 3440/   98][rank:0] loss: 0.371, average forward (0.009336) and backward (0.025215) time: 0.077456, iotime: 0.001566 
2022-08-04 04:56:54,001 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049218, Speed: 1300.339192 images/s
2022-08-04 04:56:54,178 [dl_trainer.py:731] WARNING [ 35][ 3480/   98][rank:0] loss: 0.379, average forward (0.008842) and backward (0.024486) time: 0.035256, iotime: 0.001699 
2022-08-04 04:56:54,788 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:56:54,788 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:56:56,310 [dl_trainer.py:731] WARNING [ 35][ 3520/   98][rank:0] loss: 0.538, average forward (0.009545) and backward (0.023617) time: 0.039273, iotime: 0.005856 
2022-08-04 04:56:56,719 [dl_trainer.py:634] INFO train iter: 3528, num_batches_per_epoch: 98
2022-08-04 04:56:56,719 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 84.821429, lr: 0.100000, avg loss: 0.435592
2022-08-04 04:56:58,425 [dl_trainer.py:822] INFO Epoch 36, lr: 0.100000, val loss: 0.576703, val top-1 acc: 81.648089, top-5 acc: 99.104299
2022-08-04 04:56:58,701 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082428, Speed: 776.433109 images/s
2022-08-04 04:56:59,992 [dl_trainer.py:731] WARNING [ 36][ 3560/   98][rank:0] loss: 0.407, average forward (0.008972) and backward (0.021662) time: 0.075587, iotime: 0.001543 
2022-08-04 04:57:00,631 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048245, Speed: 1326.571001 images/s
2022-08-04 04:57:01,419 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:01,419 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:02,132 [dl_trainer.py:731] WARNING [ 36][ 3600/   98][rank:0] loss: 0.320, average forward (0.008789) and backward (0.023554) time: 0.038398, iotime: 0.005824 
2022-08-04 04:57:03,455 [dl_trainer.py:634] INFO train iter: 3626, num_batches_per_epoch: 98
2022-08-04 04:57:03,456 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 86.288265, lr: 0.100000, avg loss: 0.401824
2022-08-04 04:57:05,197 [dl_trainer.py:822] INFO Epoch 37, lr: 0.100000, val loss: 0.541722, val top-1 acc: 81.717755, top-5 acc: 99.164013
2022-08-04 04:57:05,394 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083551, Speed: 766.000772 images/s
2022-08-04 04:57:05,831 [dl_trainer.py:731] WARNING [ 37][ 3640/   98][rank:0] loss: 0.452, average forward (0.009514) and backward (0.022299) time: 0.077252, iotime: 0.001600 
2022-08-04 04:57:07,332 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048441, Speed: 1321.186760 images/s
2022-08-04 04:57:07,821 [dl_trainer.py:731] WARNING [ 37][ 3680/   98][rank:0] loss: 0.475, average forward (0.008716) and backward (0.023842) time: 0.034292, iotime: 0.001502 
2022-08-04 04:57:08,141 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:08,141 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:09,903 [dl_trainer.py:731] WARNING [ 37][ 3720/   98][rank:0] loss: 0.401, average forward (0.008768) and backward (0.022787) time: 0.037457, iotime: 0.005656 
2022-08-04 04:57:10,129 [dl_trainer.py:634] INFO train iter: 3724, num_batches_per_epoch: 98
2022-08-04 04:57:10,130 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 85.634566, lr: 0.100000, avg loss: 0.412973
2022-08-04 04:57:11,802 [dl_trainer.py:822] INFO Epoch 38, lr: 0.100000, val loss: 0.642228, val top-1 acc: 79.657643, top-5 acc: 98.855494
2022-08-04 04:57:11,976 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081441, Speed: 785.846694 images/s
2022-08-04 04:57:13,512 [dl_trainer.py:731] WARNING [ 38][ 3760/   98][rank:0] loss: 0.527, average forward (0.009163) and backward (0.023559) time: 0.076844, iotime: 0.001547 
2022-08-04 04:57:13,860 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047104, Speed: 1358.699748 images/s
2022-08-04 04:57:14,674 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:14,675 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:15,651 [dl_trainer.py:731] WARNING [ 38][ 3800/   98][rank:0] loss: 0.342, average forward (0.008439) and backward (0.023801) time: 0.038283, iotime: 0.005812 
2022-08-04 04:57:16,736 [dl_trainer.py:634] INFO train iter: 3822, num_batches_per_epoch: 98
2022-08-04 04:57:16,737 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 85.012755, lr: 0.100000, avg loss: 0.430131
2022-08-04 04:57:18,425 [dl_trainer.py:822] INFO Epoch 39, lr: 0.100000, val loss: 0.711469, val top-1 acc: 77.935908, top-5 acc: 99.134156
2022-08-04 04:57:18,517 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081689, Speed: 783.460720 images/s
2022-08-04 04:57:19,303 [dl_trainer.py:731] WARNING [ 39][ 3840/   98][rank:0] loss: 0.381, average forward (0.009122) and backward (0.021953) time: 0.075107, iotime: 0.001532 
2022-08-04 04:57:20,524 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050143, Speed: 1276.339690 images/s
2022-08-04 04:57:21,284 [dl_trainer.py:731] WARNING [ 39][ 3880/   98][rank:0] loss: 0.244, average forward (0.010396) and backward (0.023422) time: 0.035816, iotime: 0.001720 
2022-08-04 04:57:21,297 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:21,297 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:23,466 [dl_trainer.py:731] WARNING [ 39][ 3920/   98][rank:0] loss: 0.218, average forward (0.010669) and backward (0.024580) time: 0.041759, iotime: 0.006238 
2022-08-04 04:57:23,478 [dl_trainer.py:634] INFO train iter: 3920, num_batches_per_epoch: 98
2022-08-04 04:57:23,479 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 86.065051, lr: 0.100000, avg loss: 0.405571
2022-08-04 04:57:25,215 [dl_trainer.py:822] INFO Epoch 40, lr: 0.100000, val loss: 0.603798, val top-1 acc: 80.593153, top-5 acc: 99.283439
2022-08-04 04:57:25,286 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083528, Speed: 766.213433 images/s
2022-08-04 04:57:27,222 [dl_trainer.py:731] WARNING [ 40][ 3960/   98][rank:0] loss: 0.352, average forward (0.010078) and backward (0.024275) time: 0.080062, iotime: 0.001703 
2022-08-04 04:57:27,299 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050302, Speed: 1272.305446 images/s
2022-08-04 04:57:28,100 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:28,101 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:29,409 [dl_trainer.py:731] WARNING [ 40][ 4000/   98][rank:0] loss: 0.436, average forward (0.009135) and backward (0.025494) time: 0.040718, iotime: 0.005844 
2022-08-04 04:57:30,328 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053115, Speed: 1204.927953 images/s
2022-08-04 04:57:30,329 [dl_trainer.py:634] INFO train iter: 4018, num_batches_per_epoch: 98
2022-08-04 04:57:30,329 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 86.096939, lr: 0.100000, avg loss: 0.395455
2022-08-04 04:57:32,027 [dl_trainer.py:822] INFO Epoch 41, lr: 0.100000, val loss: 0.593289, val top-1 acc: 80.672771, top-5 acc: 99.134156
2022-08-04 04:57:33,133 [dl_trainer.py:731] WARNING [ 41][ 4040/   98][rank:0] loss: 0.511, average forward (0.010484) and backward (0.023267) time: 0.078301, iotime: 0.001775 
2022-08-04 04:57:34,002 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091831, Speed: 696.935088 images/s
2022-08-04 04:57:34,799 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:34,799 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:35,269 [dl_trainer.py:731] WARNING [ 41][ 4080/   98][rank:0] loss: 0.377, average forward (0.009015) and backward (0.022379) time: 0.037571, iotime: 0.005940 
2022-08-04 04:57:36,951 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051716, Speed: 1237.538814 images/s
2022-08-04 04:57:37,025 [dl_trainer.py:634] INFO train iter: 4116, num_batches_per_epoch: 98
2022-08-04 04:57:37,025 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 86.383929, lr: 0.100000, avg loss: 0.391093
2022-08-04 04:57:38,727 [dl_trainer.py:822] INFO Epoch 42, lr: 0.100000, val loss: 0.607877, val top-1 acc: 80.792197, top-5 acc: 99.193869
2022-08-04 04:57:38,914 [dl_trainer.py:731] WARNING [ 42][ 4120/   98][rank:0] loss: 0.251, average forward (0.009293) and backward (0.025212) time: 0.079603, iotime: 0.001906 
2022-08-04 04:57:40,641 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.092229, Speed: 693.927889 images/s
2022-08-04 04:57:40,874 [dl_trainer.py:731] WARNING [ 42][ 4160/   98][rank:0] loss: 0.397, average forward (0.008402) and backward (0.022358) time: 0.032494, iotime: 0.001504 
2022-08-04 04:57:41,436 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:41,437 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:43,024 [dl_trainer.py:731] WARNING [ 42][ 4200/   98][rank:0] loss: 0.417, average forward (0.009341) and backward (0.023155) time: 0.039050, iotime: 0.006317 
2022-08-04 04:57:43,637 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052534, Speed: 1218.255187 images/s
2022-08-04 04:57:43,738 [dl_trainer.py:634] INFO train iter: 4214, num_batches_per_epoch: 98
2022-08-04 04:57:43,739 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 86.814413, lr: 0.100000, avg loss: 0.378644
2022-08-04 04:57:45,415 [dl_trainer.py:822] INFO Epoch 43, lr: 0.100000, val loss: 0.500787, val top-1 acc: 83.568869, top-5 acc: 99.392914
2022-08-04 04:57:46,701 [dl_trainer.py:731] WARNING [ 43][ 4240/   98][rank:0] loss: 0.344, average forward (0.009669) and backward (0.024120) time: 0.077617, iotime: 0.001588 
2022-08-04 04:57:47,314 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091914, Speed: 696.300732 images/s
2022-08-04 04:57:48,106 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:48,107 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:48,858 [dl_trainer.py:731] WARNING [ 43][ 4280/   98][rank:0] loss: 0.489, average forward (0.009462) and backward (0.024104) time: 0.039752, iotime: 0.005928 
2022-08-04 04:57:50,304 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052432, Speed: 1220.634038 images/s
2022-08-04 04:57:50,441 [dl_trainer.py:634] INFO train iter: 4312, num_batches_per_epoch: 98
2022-08-04 04:57:50,441 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 87.181122, lr: 0.100000, avg loss: 0.372487
2022-08-04 04:57:52,150 [dl_trainer.py:822] INFO Epoch 44, lr: 0.100000, val loss: 0.555599, val top-1 acc: 82.673169, top-5 acc: 99.223726
2022-08-04 04:57:52,543 [dl_trainer.py:731] WARNING [ 44][ 4320/   98][rank:0] loss: 0.349, average forward (0.009529) and backward (0.024112) time: 0.078740, iotime: 0.001588 
2022-08-04 04:57:53,957 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091318, Speed: 700.847409 images/s
2022-08-04 04:57:54,492 [dl_trainer.py:731] WARNING [ 44][ 4360/   98][rank:0] loss: 0.428, average forward (0.008740) and backward (0.022969) time: 0.033474, iotime: 0.001532 
2022-08-04 04:57:54,755 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:57:54,755 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:57:56,633 [dl_trainer.py:731] WARNING [ 44][ 4400/   98][rank:0] loss: 0.269, average forward (0.010468) and backward (0.024318) time: 0.040918, iotime: 0.005860 
2022-08-04 04:57:56,953 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052542, Speed: 1218.081101 images/s
2022-08-04 04:57:57,147 [dl_trainer.py:634] INFO train iter: 4410, num_batches_per_epoch: 98
2022-08-04 04:57:57,147 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 87.260842, lr: 0.100000, avg loss: 0.350368
2022-08-04 04:57:58,834 [dl_trainer.py:822] INFO Epoch 45, lr: 0.100000, val loss: 0.468865, val top-1 acc: 84.733280, top-5 acc: 99.343153
2022-08-04 04:58:00,298 [dl_trainer.py:731] WARNING [ 45][ 4440/   98][rank:0] loss: 0.519, average forward (0.010423) and backward (0.022628) time: 0.077274, iotime: 0.001674 
2022-08-04 04:58:00,604 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091267, Speed: 701.238201 images/s
2022-08-04 04:58:01,351 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:01,351 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:02,410 [dl_trainer.py:731] WARNING [ 45][ 4480/   98][rank:0] loss: 0.350, average forward (0.009977) and backward (0.024751) time: 0.040780, iotime: 0.005804 
2022-08-04 04:58:03,556 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051766, Speed: 1236.321483 images/s
2022-08-04 04:58:03,799 [dl_trainer.py:634] INFO train iter: 4508, num_batches_per_epoch: 98
2022-08-04 04:58:03,799 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 86.941964, lr: 0.100000, avg loss: 0.386427
2022-08-04 04:58:05,492 [dl_trainer.py:822] INFO Epoch 46, lr: 0.100000, val loss: 0.704667, val top-1 acc: 79.100318, top-5 acc: 98.945064
2022-08-04 04:58:06,096 [dl_trainer.py:731] WARNING [ 46][ 4520/   98][rank:0] loss: 0.243, average forward (0.008888) and backward (0.022961) time: 0.077376, iotime: 0.001521 
2022-08-04 04:58:07,245 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.092220, Speed: 693.989379 images/s
2022-08-04 04:58:08,055 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:08,056 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:08,272 [dl_trainer.py:731] WARNING [ 46][ 4560/   98][rank:0] loss: 0.363, average forward (0.009223) and backward (0.024276) time: 0.039592, iotime: 0.005860 
2022-08-04 04:58:10,235 [dl_trainer.py:731] WARNING [ 46][ 4600/   98][rank:0] loss: 0.309, average forward (0.010120) and backward (0.025688) time: 0.037657, iotime: 0.001594 
2022-08-04 04:58:10,243 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052566, Speed: 1217.507599 images/s
2022-08-04 04:58:10,544 [dl_trainer.py:634] INFO train iter: 4606, num_batches_per_epoch: 98
2022-08-04 04:58:10,545 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 87.579719, lr: 0.100000, avg loss: 0.347884
2022-08-04 04:58:12,239 [dl_trainer.py:822] INFO Epoch 47, lr: 0.100000, val loss: 0.584568, val top-1 acc: 81.837182, top-5 acc: 98.805732
2022-08-04 04:58:13,905 [dl_trainer.py:731] WARNING [ 47][ 4640/   98][rank:0] loss: 0.657, average forward (0.010206) and backward (0.022432) time: 0.077091, iotime: 0.001729 
2022-08-04 04:58:13,931 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.092188, Speed: 694.233069 images/s
2022-08-04 04:58:14,748 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:14,749 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:16,111 [dl_trainer.py:731] WARNING [ 47][ 4680/   98][rank:0] loss: 0.257, average forward (0.009622) and backward (0.023503) time: 0.039996, iotime: 0.006623 
2022-08-04 04:58:16,964 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053186, Speed: 1203.316839 images/s
2022-08-04 04:58:17,298 [dl_trainer.py:634] INFO train iter: 4704, num_batches_per_epoch: 98
2022-08-04 04:58:17,299 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 87.675383, lr: 0.100000, avg loss: 0.339027
2022-08-04 04:58:18,905 [dl_trainer.py:822] INFO Epoch 48, lr: 0.100000, val loss: 0.531086, val top-1 acc: 83.011545, top-5 acc: 99.273487
2022-08-04 04:58:19,702 [dl_trainer.py:731] WARNING [ 48][ 4720/   98][rank:0] loss: 0.381, average forward (0.010532) and backward (0.023434) time: 0.076610, iotime: 0.001670 
2022-08-04 04:58:20,553 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089724, Speed: 713.302127 images/s
2022-08-04 04:58:21,336 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:21,336 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:21,852 [dl_trainer.py:731] WARNING [ 48][ 4760/   98][rank:0] loss: 0.675, average forward (0.009460) and backward (0.021691) time: 0.036988, iotime: 0.005614 
2022-08-04 04:58:23,565 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052810, Speed: 1211.893326 images/s
2022-08-04 04:58:23,867 [dl_trainer.py:731] WARNING [ 48][ 4800/   98][rank:0] loss: 0.271, average forward (0.010394) and backward (0.025185) time: 0.037579, iotime: 0.001724 
2022-08-04 04:58:23,982 [dl_trainer.py:634] INFO train iter: 4802, num_batches_per_epoch: 98
2022-08-04 04:58:23,982 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 87.452168, lr: 0.100000, avg loss: 0.361141
2022-08-04 04:58:25,631 [dl_trainer.py:822] INFO Epoch 49, lr: 0.100000, val loss: 0.584468, val top-1 acc: 82.215366, top-5 acc: 98.974920
2022-08-04 04:58:27,219 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091346, Speed: 700.635398 images/s
2022-08-04 04:58:27,495 [dl_trainer.py:731] WARNING [ 49][ 4840/   98][rank:0] loss: 0.388, average forward (0.011087) and backward (0.025178) time: 0.079605, iotime: 0.001778 
2022-08-04 04:58:28,001 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:28,001 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:29,645 [dl_trainer.py:731] WARNING [ 49][ 4880/   98][rank:0] loss: 0.466, average forward (0.009130) and backward (0.024975) time: 0.040286, iotime: 0.005943 
2022-08-04 04:58:30,194 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052173, Speed: 1226.690109 images/s
2022-08-04 04:58:30,643 [dl_trainer.py:634] INFO train iter: 4900, num_batches_per_epoch: 98
2022-08-04 04:58:30,643 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 88.647959, lr: 0.100000, avg loss: 0.341615
2022-08-04 04:58:32,289 [dl_trainer.py:822] INFO Epoch 50, lr: 0.100000, val loss: 0.525672, val top-1 acc: 83.459395, top-5 acc: 99.243631
2022-08-04 04:58:33,292 [dl_trainer.py:731] WARNING [ 50][ 4920/   98][rank:0] loss: 0.198, average forward (0.010316) and backward (0.025172) time: 0.079653, iotime: 0.001786 
2022-08-04 04:58:33,848 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091336, Speed: 700.706039 images/s
2022-08-04 04:58:34,650 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:34,651 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:35,472 [dl_trainer.py:731] WARNING [ 50][ 4960/   98][rank:0] loss: 0.278, average forward (0.010131) and backward (0.024212) time: 0.040727, iotime: 0.006122 
2022-08-04 04:58:36,861 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052831, Speed: 1211.418375 images/s
2022-08-04 04:58:37,335 [dl_trainer.py:634] INFO train iter: 4998, num_batches_per_epoch: 98
2022-08-04 04:58:37,335 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 88.887117, lr: 0.100000, avg loss: 0.320100
2022-08-04 04:58:39,023 [dl_trainer.py:822] INFO Epoch 51, lr: 0.100000, val loss: 0.582189, val top-1 acc: 82.503981, top-5 acc: 99.253583
2022-08-04 04:58:39,124 [dl_trainer.py:731] WARNING [ 51][ 5000/   98][rank:0] loss: 0.231, average forward (0.009646) and backward (0.023824) time: 0.077631, iotime: 0.001634 
2022-08-04 04:58:40,507 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091141, Speed: 702.208043 images/s
2022-08-04 04:58:41,092 [dl_trainer.py:731] WARNING [ 51][ 5040/   98][rank:0] loss: 0.307, average forward (0.009963) and backward (0.025166) time: 0.037032, iotime: 0.001639 
2022-08-04 04:58:41,302 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:41,302 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:43,278 [dl_trainer.py:731] WARNING [ 51][ 5080/   98][rank:0] loss: 0.143, average forward (0.009811) and backward (0.025902) time: 0.042047, iotime: 0.006069 
2022-08-04 04:58:43,541 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053203, Speed: 1202.929345 images/s
2022-08-04 04:58:44,079 [dl_trainer.py:634] INFO train iter: 5096, num_batches_per_epoch: 98
2022-08-04 04:58:44,080 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 88.360969, lr: 0.100000, avg loss: 0.324247
2022-08-04 04:58:45,741 [dl_trainer.py:822] INFO Epoch 52, lr: 0.100000, val loss: 0.489907, val top-1 acc: 84.036624, top-5 acc: 99.353105
2022-08-04 04:58:46,943 [dl_trainer.py:731] WARNING [ 52][ 5120/   98][rank:0] loss: 0.374, average forward (0.010498) and backward (0.023650) time: 0.078191, iotime: 0.001656 
2022-08-04 04:58:47,202 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091506, Speed: 699.409797 images/s
2022-08-04 04:58:48,015 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:48,016 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:49,140 [dl_trainer.py:731] WARNING [ 52][ 5160/   98][rank:0] loss: 0.183, average forward (0.009145) and backward (0.023757) time: 0.038626, iotime: 0.005487 
2022-08-04 04:58:50,235 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053180, Speed: 1203.462498 images/s
2022-08-04 04:58:50,816 [dl_trainer.py:634] INFO train iter: 5194, num_batches_per_epoch: 98
2022-08-04 04:58:50,817 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 88.488520, lr: 0.100000, avg loss: 0.333054
2022-08-04 04:58:52,527 [dl_trainer.py:822] INFO Epoch 53, lr: 0.100000, val loss: 0.494364, val top-1 acc: 83.927150, top-5 acc: 99.323248
2022-08-04 04:58:52,812 [dl_trainer.py:731] WARNING [ 53][ 5200/   98][rank:0] loss: 0.448, average forward (0.009976) and backward (0.020013) time: 0.074799, iotime: 0.001730 
2022-08-04 04:58:53,853 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090436, Speed: 707.686655 images/s
2022-08-04 04:58:54,631 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:58:54,631 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:58:54,887 [dl_trainer.py:731] WARNING [ 53][ 5240/   98][rank:0] loss: 0.247, average forward (0.009546) and backward (0.022745) time: 0.038780, iotime: 0.006222 
2022-08-04 04:58:56,840 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052361, Speed: 1222.279391 images/s
2022-08-04 04:58:56,881 [dl_trainer.py:731] WARNING [ 53][ 5280/   98][rank:0] loss: 0.208, average forward (0.010511) and backward (0.024469) time: 0.037077, iotime: 0.001800 
2022-08-04 04:58:57,470 [dl_trainer.py:634] INFO train iter: 5292, num_batches_per_epoch: 98
2022-08-04 04:58:57,470 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 88.982781, lr: 0.100000, avg loss: 0.316398
2022-08-04 04:58:59,126 [dl_trainer.py:822] INFO Epoch 54, lr: 0.100000, val loss: 0.540135, val top-1 acc: 82.822452, top-5 acc: 99.412818
2022-08-04 04:59:00,502 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091522, Speed: 699.285811 images/s
2022-08-04 04:59:00,540 [dl_trainer.py:731] WARNING [ 54][ 5320/   98][rank:0] loss: 0.403, average forward (0.010549) and backward (0.021819) time: 0.076660, iotime: 0.001677 
2022-08-04 04:59:01,298 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:01,298 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:02,664 [dl_trainer.py:731] WARNING [ 54][ 5360/   98][rank:0] loss: 0.248, average forward (0.010339) and backward (0.023148) time: 0.039855, iotime: 0.006076 
2022-08-04 04:59:03,460 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051866, Speed: 1233.949616 images/s
2022-08-04 04:59:04,118 [dl_trainer.py:634] INFO train iter: 5390, num_batches_per_epoch: 98
2022-08-04 04:59:04,118 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 89.716199, lr: 0.100000, avg loss: 0.301370
2022-08-04 04:59:05,778 [dl_trainer.py:822] INFO Epoch 55, lr: 0.100000, val loss: 0.610270, val top-1 acc: 81.299761, top-5 acc: 99.154061
2022-08-04 04:59:06,258 [dl_trainer.py:731] WARNING [ 55][ 5400/   98][rank:0] loss: 0.393, average forward (0.009516) and backward (0.023170) time: 0.076150, iotime: 0.001636 
2022-08-04 04:59:07,065 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090120, Speed: 710.164416 images/s
2022-08-04 04:59:07,872 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:07,872 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:08,394 [dl_trainer.py:731] WARNING [ 55][ 5440/   98][rank:0] loss: 0.343, average forward (0.010515) and backward (0.026203) time: 0.042698, iotime: 0.005708 
2022-08-04 04:59:10,024 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051890, Speed: 1233.378476 images/s
2022-08-04 04:59:10,348 [dl_trainer.py:731] WARNING [ 55][ 5480/   98][rank:0] loss: 0.169, average forward (0.010111) and backward (0.025596) time: 0.037619, iotime: 0.001661 
2022-08-04 04:59:10,764 [dl_trainer.py:634] INFO train iter: 5488, num_batches_per_epoch: 98
2022-08-04 04:59:10,764 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 89.365434, lr: 0.100000, avg loss: 0.302030
2022-08-04 04:59:12,376 [dl_trainer.py:822] INFO Epoch 56, lr: 0.100000, val loss: 0.643377, val top-1 acc: 80.075637, top-5 acc: 99.323248
2022-08-04 04:59:13,653 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090692, Speed: 705.682567 images/s
2022-08-04 04:59:13,999 [dl_trainer.py:731] WARNING [ 56][ 5520/   98][rank:0] loss: 0.211, average forward (0.010649) and backward (0.025855) time: 0.079353, iotime: 0.001718 
2022-08-04 04:59:14,461 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:14,462 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:16,124 [dl_trainer.py:731] WARNING [ 56][ 5560/   98][rank:0] loss: 0.223, average forward (0.010750) and backward (0.024270) time: 0.041261, iotime: 0.005951 
2022-08-04 04:59:16,609 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051842, Speed: 1234.523476 images/s
2022-08-04 04:59:17,350 [dl_trainer.py:634] INFO train iter: 5586, num_batches_per_epoch: 98
2022-08-04 04:59:17,351 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 90.082908, lr: 0.100000, avg loss: 0.291275
2022-08-04 04:59:19,008 [dl_trainer.py:822] INFO Epoch 57, lr: 0.100000, val loss: 0.488801, val top-1 acc: 84.544188, top-5 acc: 99.333201
2022-08-04 04:59:19,674 [dl_trainer.py:731] WARNING [ 57][ 5600/   98][rank:0] loss: 0.181, average forward (0.010846) and backward (0.023199) time: 0.077545, iotime: 0.001734 
2022-08-04 04:59:20,172 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089069, Speed: 718.544460 images/s
2022-08-04 04:59:20,953 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:20,954 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:21,781 [dl_trainer.py:731] WARNING [ 57][ 5640/   98][rank:0] loss: 0.270, average forward (0.010090) and backward (0.024953) time: 0.041088, iotime: 0.005789 
2022-08-04 04:59:23,112 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051546, Speed: 1241.613452 images/s
2022-08-04 04:59:23,744 [dl_trainer.py:731] WARNING [ 57][ 5680/   98][rank:0] loss: 0.266, average forward (0.009089) and backward (0.024359) time: 0.035342, iotime: 0.001649 
2022-08-04 04:59:23,953 [dl_trainer.py:634] INFO train iter: 5684, num_batches_per_epoch: 98
2022-08-04 04:59:23,953 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 89.413265, lr: 0.100000, avg loss: 0.300489
2022-08-04 04:59:25,608 [dl_trainer.py:822] INFO Epoch 58, lr: 0.100000, val loss: 0.610871, val top-1 acc: 81.847134, top-5 acc: 99.114252
2022-08-04 04:59:26,738 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090637, Speed: 706.116242 images/s
2022-08-04 04:59:27,328 [dl_trainer.py:731] WARNING [ 58][ 5720/   98][rank:0] loss: 0.280, average forward (0.010103) and backward (0.022296) time: 0.076176, iotime: 0.001599 
2022-08-04 04:59:27,478 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:27,478 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:29,484 [dl_trainer.py:731] WARNING [ 58][ 5760/   98][rank:0] loss: 0.230, average forward (0.011190) and backward (0.025781) time: 0.043037, iotime: 0.005767 
2022-08-04 04:59:29,683 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051652, Speed: 1239.059689 images/s
2022-08-04 04:59:30,553 [dl_trainer.py:634] INFO train iter: 5782, num_batches_per_epoch: 98
2022-08-04 04:59:30,553 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 89.748087, lr: 0.100000, avg loss: 0.293784
2022-08-04 04:59:32,176 [dl_trainer.py:822] INFO Epoch 59, lr: 0.100000, val loss: 0.544168, val top-1 acc: 83.160828, top-5 acc: 99.382962
2022-08-04 04:59:33,072 [dl_trainer.py:731] WARNING [ 59][ 5800/   98][rank:0] loss: 0.266, average forward (0.010397) and backward (0.024461) time: 0.077512, iotime: 0.001724 
2022-08-04 04:59:33,289 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090147, Speed: 709.949876 images/s
2022-08-04 04:59:34,067 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:34,067 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:35,210 [dl_trainer.py:731] WARNING [ 59][ 5840/   98][rank:0] loss: 0.256, average forward (0.009313) and backward (0.024898) time: 0.040290, iotime: 0.005841 
2022-08-04 04:59:36,244 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051804, Speed: 1235.434942 images/s
2022-08-04 04:59:37,145 [dl_trainer.py:731] WARNING [ 59][ 5880/   98][rank:0] loss: 0.269, average forward (0.009428) and backward (0.023001) time: 0.034230, iotime: 0.001558 
2022-08-04 04:59:37,155 [dl_trainer.py:634] INFO train iter: 5880, num_batches_per_epoch: 98
2022-08-04 04:59:37,155 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 89.174107, lr: 0.100000, avg loss: 0.296668
2022-08-04 04:59:38,832 [dl_trainer.py:822] INFO Epoch 60, lr: 0.100000, val loss: 0.455014, val top-1 acc: 85.907643, top-5 acc: 99.402866
2022-08-04 04:59:39,844 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089995, Speed: 711.151587 images/s
2022-08-04 04:59:40,653 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:40,654 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:40,960 [dl_trainer.py:731] WARNING [ 60][ 5920/   98][rank:0] loss: 0.491, average forward (0.009409) and backward (0.023836) time: 0.081673, iotime: 0.005702 
2022-08-04 04:59:42,873 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053119, Speed: 1204.852238 images/s
2022-08-04 04:59:42,965 [dl_trainer.py:731] WARNING [ 60][ 5960/   98][rank:0] loss: 0.274, average forward (0.009326) and backward (0.024793) time: 0.035926, iotime: 0.001567 
2022-08-04 04:59:43,884 [dl_trainer.py:634] INFO train iter: 5978, num_batches_per_epoch: 98
2022-08-04 04:59:43,885 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 89.827806, lr: 0.100000, avg loss: 0.289955
2022-08-04 04:59:45,559 [dl_trainer.py:822] INFO Epoch 61, lr: 0.100000, val loss: 0.500882, val top-1 acc: 84.494427, top-5 acc: 99.363057
2022-08-04 04:59:46,534 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091502, Speed: 699.435083 images/s
2022-08-04 04:59:46,626 [dl_trainer.py:731] WARNING [ 61][ 6000/   98][rank:0] loss: 0.289, average forward (0.009276) and backward (0.024463) time: 0.077614, iotime: 0.001682 
2022-08-04 04:59:47,313 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:47,314 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:48,776 [dl_trainer.py:731] WARNING [ 61][ 6040/   98][rank:0] loss: 0.121, average forward (0.009620) and backward (0.025277) time: 0.040868, iotime: 0.005710 
2022-08-04 04:59:49,510 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052185, Speed: 1226.397994 images/s
2022-08-04 04:59:50,572 [dl_trainer.py:634] INFO train iter: 6076, num_batches_per_epoch: 98
2022-08-04 04:59:50,572 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 90.513393, lr: 0.100000, avg loss: 0.269691
2022-08-04 04:59:52,292 [dl_trainer.py:822] INFO Epoch 62, lr: 0.100000, val loss: 0.540268, val top-1 acc: 84.146099, top-5 acc: 99.104299
2022-08-04 04:59:52,500 [dl_trainer.py:731] WARNING [ 62][ 6080/   98][rank:0] loss: 0.364, average forward (0.010307) and backward (0.025005) time: 0.080832, iotime: 0.001717 
2022-08-04 04:59:53,256 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.093637, Speed: 683.489315 images/s
2022-08-04 04:59:54,055 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 04:59:54,055 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 04:59:54,637 [dl_trainer.py:731] WARNING [ 62][ 6120/   98][rank:0] loss: 0.345, average forward (0.010993) and backward (0.024506) time: 0.041595, iotime: 0.005813 
2022-08-04 04:59:56,259 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052655, Speed: 1215.467269 images/s
2022-08-04 04:59:56,647 [dl_trainer.py:731] WARNING [ 62][ 6160/   98][rank:0] loss: 0.403, average forward (0.008341) and backward (0.023682) time: 0.033756, iotime: 0.001514 
2022-08-04 04:59:57,360 [dl_trainer.py:634] INFO train iter: 6174, num_batches_per_epoch: 98
2022-08-04 04:59:57,360 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 91.039541, lr: 0.100000, avg loss: 0.249016
2022-08-04 04:59:58,985 [dl_trainer.py:822] INFO Epoch 63, lr: 0.100000, val loss: 0.443766, val top-1 acc: 86.136545, top-5 acc: 99.452627
2022-08-04 04:59:59,875 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090395, Speed: 708.004340 images/s
2022-08-04 05:00:00,253 [dl_trainer.py:731] WARNING [ 63][ 6200/   98][rank:0] loss: 0.265, average forward (0.009859) and backward (0.024637) time: 0.077075, iotime: 0.001626 
2022-08-04 05:00:00,670 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:00,670 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:02,419 [dl_trainer.py:731] WARNING [ 63][ 6240/   98][rank:0] loss: 0.376, average forward (0.009736) and backward (0.024702) time: 0.040920, iotime: 0.006226 
2022-08-04 05:00:02,905 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053137, Speed: 1204.438629 images/s
2022-08-04 05:00:04,072 [dl_trainer.py:634] INFO train iter: 6272, num_batches_per_epoch: 98
2022-08-04 05:00:04,073 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 90.449617, lr: 0.100000, avg loss: 0.266671
2022-08-04 05:00:05,796 [dl_trainer.py:822] INFO Epoch 64, lr: 0.100000, val loss: 0.505232, val top-1 acc: 84.434713, top-5 acc: 99.442675
2022-08-04 05:00:06,218 [dl_trainer.py:731] WARNING [ 64][ 6280/   98][rank:0] loss: 0.177, average forward (0.009344) and backward (0.024462) time: 0.079212, iotime: 0.001524 
2022-08-04 05:00:06,694 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.094705, Speed: 675.781435 images/s
2022-08-04 05:00:07,502 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:07,502 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:08,425 [dl_trainer.py:731] WARNING [ 64][ 6320/   98][rank:0] loss: 0.220, average forward (0.009301) and backward (0.025063) time: 0.040201, iotime: 0.005586 
2022-08-04 05:00:09,687 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052492, Speed: 1219.231475 images/s
2022-08-04 05:00:10,372 [dl_trainer.py:731] WARNING [ 64][ 6360/   98][rank:0] loss: 0.301, average forward (0.009011) and backward (0.023765) time: 0.034516, iotime: 0.001505 
2022-08-04 05:00:10,892 [dl_trainer.py:634] INFO train iter: 6370, num_batches_per_epoch: 98
2022-08-04 05:00:10,892 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 90.561224, lr: 0.100000, avg loss: 0.262720
2022-08-04 05:00:12,538 [dl_trainer.py:822] INFO Epoch 65, lr: 0.100000, val loss: 0.472219, val top-1 acc: 85.499602, top-5 acc: 99.343153
2022-08-04 05:00:13,341 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091341, Speed: 700.670740 images/s
2022-08-04 05:00:14,028 [dl_trainer.py:731] WARNING [ 65][ 6400/   98][rank:0] loss: 0.297, average forward (0.010049) and backward (0.023397) time: 0.076610, iotime: 0.001668 
2022-08-04 05:00:14,141 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:14,141 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:16,188 [dl_trainer.py:731] WARNING [ 65][ 6440/   98][rank:0] loss: 0.188, average forward (0.009874) and backward (0.023789) time: 0.040126, iotime: 0.006186 
2022-08-04 05:00:16,355 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052849, Speed: 1210.986253 images/s
2022-08-04 05:00:17,582 [dl_trainer.py:634] INFO train iter: 6468, num_batches_per_epoch: 98
2022-08-04 05:00:17,582 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 90.720663, lr: 0.100000, avg loss: 0.254751
2022-08-04 05:00:19,216 [dl_trainer.py:822] INFO Epoch 66, lr: 0.100000, val loss: 0.465862, val top-1 acc: 86.096736, top-5 acc: 99.482484
2022-08-04 05:00:19,836 [dl_trainer.py:731] WARNING [ 66][ 6480/   98][rank:0] loss: 0.264, average forward (0.010787) and backward (0.024011) time: 0.078188, iotime: 0.001741 
2022-08-04 05:00:19,992 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090904, Speed: 704.041339 images/s
2022-08-04 05:00:20,782 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:20,782 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:21,969 [dl_trainer.py:731] WARNING [ 66][ 6520/   98][rank:0] loss: 0.189, average forward (0.010607) and backward (0.024397) time: 0.041118, iotime: 0.005839 
2022-08-04 05:00:22,938 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051663, Speed: 1238.791942 images/s
2022-08-04 05:00:23,921 [dl_trainer.py:731] WARNING [ 66][ 6560/   98][rank:0] loss: 0.146, average forward (0.010012) and backward (0.023497) time: 0.035412, iotime: 0.001628 
2022-08-04 05:00:24,210 [dl_trainer.py:634] INFO train iter: 6566, num_batches_per_epoch: 98
2022-08-04 05:00:24,210 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 91.900510, lr: 0.100000, avg loss: 0.242645
2022-08-04 05:00:25,842 [dl_trainer.py:822] INFO Epoch 67, lr: 0.100000, val loss: 0.467844, val top-1 acc: 86.365446, top-5 acc: 99.452627
2022-08-04 05:00:26,554 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090379, Speed: 708.127982 images/s
2022-08-04 05:00:27,341 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:27,342 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:27,696 [dl_trainer.py:731] WARNING [ 67][ 6600/   98][rank:0] loss: 0.209, average forward (0.009958) and backward (0.025013) time: 0.082138, iotime: 0.006035 
2022-08-04 05:00:29,545 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052436, Speed: 1220.537155 images/s
2022-08-04 05:00:29,686 [dl_trainer.py:731] WARNING [ 67][ 6640/   98][rank:0] loss: 0.421, average forward (0.009196) and backward (0.024026) time: 0.035013, iotime: 0.001539 
2022-08-04 05:00:30,881 [dl_trainer.py:634] INFO train iter: 6664, num_batches_per_epoch: 98
2022-08-04 05:00:30,881 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 91.581633, lr: 0.100000, avg loss: 0.240892
2022-08-04 05:00:32,519 [dl_trainer.py:822] INFO Epoch 68, lr: 0.100000, val loss: 0.518289, val top-1 acc: 85.061704, top-5 acc: 99.313296
2022-08-04 05:00:33,175 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090741, Speed: 705.303809 images/s
2022-08-04 05:00:33,306 [dl_trainer.py:731] WARNING [ 68][ 6680/   98][rank:0] loss: 0.259, average forward (0.010268) and backward (0.023479) time: 0.077135, iotime: 0.001637 
2022-08-04 05:00:33,977 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:33,977 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:35,436 [dl_trainer.py:731] WARNING [ 68][ 6720/   98][rank:0] loss: 0.233, average forward (0.010389) and backward (0.022454) time: 0.038900, iotime: 0.005778 
2022-08-04 05:00:36,150 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052173, Speed: 1226.682340 images/s
2022-08-04 05:00:37,428 [dl_trainer.py:731] WARNING [ 68][ 6760/   98][rank:0] loss: 0.335, average forward (0.009193) and backward (0.022950) time: 0.033942, iotime: 0.001528 
2022-08-04 05:00:37,535 [dl_trainer.py:634] INFO train iter: 6762, num_batches_per_epoch: 98
2022-08-04 05:00:37,536 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 91.007653, lr: 0.100000, avg loss: 0.252595
2022-08-04 05:00:39,144 [dl_trainer.py:822] INFO Epoch 69, lr: 0.100000, val loss: 0.470542, val top-1 acc: 85.210987, top-5 acc: 99.512341
2022-08-04 05:00:39,727 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089398, Speed: 715.902432 images/s
2022-08-04 05:00:40,538 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:40,538 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:41,166 [dl_trainer.py:731] WARNING [ 69][ 6800/   98][rank:0] loss: 0.391, average forward (0.010146) and backward (0.025004) time: 0.081753, iotime: 0.006078 
2022-08-04 05:00:42,685 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051872, Speed: 1233.805637 images/s
2022-08-04 05:00:43,123 [dl_trainer.py:731] WARNING [ 69][ 6840/   98][rank:0] loss: 0.330, average forward (0.009977) and backward (0.023396) time: 0.035285, iotime: 0.001648 
2022-08-04 05:00:44,121 [dl_trainer.py:634] INFO train iter: 6860, num_batches_per_epoch: 98
2022-08-04 05:00:44,121 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 90.911990, lr: 0.100000, avg loss: 0.251385
2022-08-04 05:00:45,740 [dl_trainer.py:822] INFO Epoch 70, lr: 0.100000, val loss: 0.484255, val top-1 acc: 85.330414, top-5 acc: 99.402866
2022-08-04 05:00:46,302 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090432, Speed: 707.716601 images/s
2022-08-04 05:00:46,711 [dl_trainer.py:731] WARNING [ 70][ 6880/   98][rank:0] loss: 0.291, average forward (0.010264) and backward (0.023764) time: 0.076985, iotime: 0.001684 
2022-08-04 05:00:47,081 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:47,082 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:48,876 [dl_trainer.py:731] WARNING [ 70][ 6920/   98][rank:0] loss: 0.242, average forward (0.009993) and backward (0.024260) time: 0.040403, iotime: 0.005868 
2022-08-04 05:00:49,288 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052351, Speed: 1222.509083 images/s
2022-08-04 05:00:50,759 [dl_trainer.py:634] INFO train iter: 6958, num_batches_per_epoch: 98
2022-08-04 05:00:50,759 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 90.991709, lr: 0.100000, avg loss: 0.242519
2022-08-04 05:00:52,407 [dl_trainer.py:822] INFO Epoch 71, lr: 0.100000, val loss: 0.492063, val top-1 acc: 84.852707, top-5 acc: 99.353105
2022-08-04 05:00:52,504 [dl_trainer.py:731] WARNING [ 71][ 6960/   98][rank:0] loss: 0.285, average forward (0.009980) and backward (0.022830) time: 0.076132, iotime: 0.001778 
2022-08-04 05:00:52,914 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090624, Speed: 706.212099 images/s
2022-08-04 05:00:53,712 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:00:53,712 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:00:54,705 [dl_trainer.py:731] WARNING [ 71][ 7000/   98][rank:0] loss: 0.358, average forward (0.009279) and backward (0.020396) time: 0.036428, iotime: 0.006508 
2022-08-04 05:00:55,932 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052927, Speed: 1209.207996 images/s
2022-08-04 05:00:56,636 [dl_trainer.py:731] WARNING [ 71][ 7040/   98][rank:0] loss: 0.244, average forward (0.008658) and backward (0.025323) time: 0.035743, iotime: 0.001542 
2022-08-04 05:00:57,437 [dl_trainer.py:634] INFO train iter: 7056, num_batches_per_epoch: 98
2022-08-04 05:00:57,438 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 91.852679, lr: 0.100000, avg loss: 0.239546
2022-08-04 05:00:59,077 [dl_trainer.py:822] INFO Epoch 72, lr: 0.100000, val loss: 0.449610, val top-1 acc: 86.385350, top-5 acc: 99.392914
2022-08-04 05:00:59,565 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090808, Speed: 704.783965 images/s
2022-08-04 05:01:00,287 [dl_trainer.py:731] WARNING [ 72][ 7080/   98][rank:0] loss: 0.255, average forward (0.008896) and backward (0.022832) time: 0.075066, iotime: 0.001567 
2022-08-04 05:01:00,353 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:00,353 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:02,458 [dl_trainer.py:731] WARNING [ 72][ 7120/   98][rank:0] loss: 0.408, average forward (0.010027) and backward (0.022961) time: 0.039196, iotime: 0.005941 
2022-08-04 05:01:02,570 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052695, Speed: 1214.533534 images/s
2022-08-04 05:01:04,113 [dl_trainer.py:634] INFO train iter: 7154, num_batches_per_epoch: 98
2022-08-04 05:01:04,113 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 91.549745, lr: 0.100000, avg loss: 0.232089
2022-08-04 05:01:05,760 [dl_trainer.py:822] INFO Epoch 73, lr: 0.100000, val loss: 0.479656, val top-1 acc: 85.230892, top-5 acc: 99.552150
2022-08-04 05:01:06,054 [dl_trainer.py:731] WARNING [ 73][ 7160/   98][rank:0] loss: 0.163, average forward (0.009253) and backward (0.021941) time: 0.074239, iotime: 0.001589 
2022-08-04 05:01:06,170 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089996, Speed: 711.146170 images/s
2022-08-04 05:01:06,940 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:06,940 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:08,218 [dl_trainer.py:731] WARNING [ 73][ 7200/   98][rank:0] loss: 0.459, average forward (0.010756) and backward (0.024935) time: 0.041971, iotime: 0.006002 
2022-08-04 05:01:09,202 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053167, Speed: 1203.759319 images/s
2022-08-04 05:01:10,190 [dl_trainer.py:731] WARNING [ 73][ 7240/   98][rank:0] loss: 0.164, average forward (0.010962) and backward (0.023431) time: 0.036456, iotime: 0.001793 
2022-08-04 05:01:10,814 [dl_trainer.py:634] INFO train iter: 7252, num_batches_per_epoch: 98
2022-08-04 05:01:10,815 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 91.932398, lr: 0.100000, avg loss: 0.232410
2022-08-04 05:01:12,427 [dl_trainer.py:822] INFO Epoch 74, lr: 0.100000, val loss: 0.500381, val top-1 acc: 84.802946, top-5 acc: 99.422771
2022-08-04 05:01:12,784 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089547, Speed: 714.711456 images/s
2022-08-04 05:01:13,598 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:13,599 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:14,018 [dl_trainer.py:731] WARNING [ 74][ 7280/   98][rank:0] loss: 0.110, average forward (0.009863) and backward (0.023776) time: 0.080353, iotime: 0.005743 
2022-08-04 05:01:15,850 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053756, Speed: 1190.572216 images/s
2022-08-04 05:01:16,032 [dl_trainer.py:731] WARNING [ 74][ 7320/   98][rank:0] loss: 0.111, average forward (0.009382) and backward (0.024263) time: 0.035500, iotime: 0.001611 
2022-08-04 05:01:17,463 [dl_trainer.py:634] INFO train iter: 7350, num_batches_per_epoch: 98
2022-08-04 05:01:17,463 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 92.538265, lr: 0.100000, avg loss: 0.207904
2022-08-04 05:01:19,106 [dl_trainer.py:822] INFO Epoch 75, lr: 0.100000, val loss: 0.443919, val top-1 acc: 86.435111, top-5 acc: 99.611863
2022-08-04 05:01:19,427 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089423, Speed: 715.702873 images/s
2022-08-04 05:01:19,617 [dl_trainer.py:731] WARNING [ 75][ 7360/   98][rank:0] loss: 0.292, average forward (0.009868) and backward (0.023233) time: 0.076121, iotime: 0.001634 
2022-08-04 05:01:20,234 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:20,234 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:21,785 [dl_trainer.py:731] WARNING [ 75][ 7400/   98][rank:0] loss: 0.109, average forward (0.010463) and backward (0.023981) time: 0.040753, iotime: 0.006036 
2022-08-04 05:01:22,454 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053050, Speed: 1206.420287 images/s
2022-08-04 05:01:23,780 [dl_trainer.py:731] WARNING [ 75][ 7440/   98][rank:0] loss: 0.158, average forward (0.009387) and backward (0.022259) time: 0.033594, iotime: 0.001685 
2022-08-04 05:01:24,186 [dl_trainer.py:634] INFO train iter: 7448, num_batches_per_epoch: 98
2022-08-04 05:01:24,186 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 92.330995, lr: 0.100000, avg loss: 0.232050
2022-08-04 05:01:25,849 [dl_trainer.py:822] INFO Epoch 76, lr: 0.100000, val loss: 0.442902, val top-1 acc: 85.847930, top-5 acc: 99.512341
2022-08-04 05:01:26,129 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091871, Speed: 696.625675 images/s
2022-08-04 05:01:26,961 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:26,961 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:27,668 [dl_trainer.py:731] WARNING [ 76][ 7480/   98][rank:0] loss: 0.235, average forward (0.009076) and backward (0.023907) time: 0.080957, iotime: 0.005628 
2022-08-04 05:01:29,130 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052613, Speed: 1216.424780 images/s
2022-08-04 05:01:29,606 [dl_trainer.py:731] WARNING [ 76][ 7520/   98][rank:0] loss: 0.219, average forward (0.009283) and backward (0.021918) time: 0.033072, iotime: 0.001635 
2022-08-04 05:01:30,841 [dl_trainer.py:634] INFO train iter: 7546, num_batches_per_epoch: 98
2022-08-04 05:01:30,841 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 91.629464, lr: 0.100000, avg loss: 0.232306
2022-08-04 05:01:32,459 [dl_trainer.py:822] INFO Epoch 77, lr: 0.100000, val loss: 0.562908, val top-1 acc: 83.588774, top-5 acc: 99.333201
2022-08-04 05:01:32,674 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.088592, Speed: 722.411216 images/s
2022-08-04 05:01:33,142 [dl_trainer.py:731] WARNING [ 77][ 7560/   98][rank:0] loss: 0.152, average forward (0.010291) and backward (0.021537) time: 0.074221, iotime: 0.001623 
2022-08-04 05:01:33,461 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:33,461 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:35,273 [dl_trainer.py:731] WARNING [ 77][ 7600/   98][rank:0] loss: 0.287, average forward (0.008923) and backward (0.023581) time: 0.038269, iotime: 0.005518 
2022-08-04 05:01:35,628 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051802, Speed: 1235.476840 images/s
2022-08-04 05:01:37,209 [dl_trainer.py:731] WARNING [ 77][ 7640/   98][rank:0] loss: 0.232, average forward (0.010103) and backward (0.023872) time: 0.035887, iotime: 0.001635 
2022-08-04 05:01:37,399 [dl_trainer.py:634] INFO train iter: 7644, num_batches_per_epoch: 98
2022-08-04 05:01:37,400 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 91.916454, lr: 0.100000, avg loss: 0.227350
2022-08-04 05:01:39,069 [dl_trainer.py:822] INFO Epoch 78, lr: 0.100000, val loss: 0.473361, val top-1 acc: 85.887739, top-5 acc: 99.591959
2022-08-04 05:01:39,229 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.090000, Speed: 711.111836 images/s
2022-08-04 05:01:40,033 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:40,033 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:41,025 [dl_trainer.py:731] WARNING [ 78][ 7680/   98][rank:0] loss: 0.148, average forward (0.009677) and backward (0.023809) time: 0.081689, iotime: 0.005663 
2022-08-04 05:01:42,240 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052799, Speed: 1212.148706 images/s
2022-08-04 05:01:42,988 [dl_trainer.py:731] WARNING [ 78][ 7720/   98][rank:0] loss: 0.181, average forward (0.009819) and backward (0.023797) time: 0.035457, iotime: 0.001609 
2022-08-04 05:01:44,142 [dl_trainer.py:634] INFO train iter: 7742, num_batches_per_epoch: 98
2022-08-04 05:01:44,142 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 92.123724, lr: 0.100000, avg loss: 0.210863
2022-08-04 05:01:45,786 [dl_trainer.py:822] INFO Epoch 79, lr: 0.100000, val loss: 0.446788, val top-1 acc: 86.843153, top-5 acc: 99.343153
2022-08-04 05:01:45,896 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091399, Speed: 700.230162 images/s
2022-08-04 05:01:46,645 [dl_trainer.py:731] WARNING [ 79][ 7760/   98][rank:0] loss: 0.202, average forward (0.008994) and backward (0.024262) time: 0.076246, iotime: 0.001582 
2022-08-04 05:01:46,661 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:46,661 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:48,827 [dl_trainer.py:731] WARNING [ 79][ 7800/   98][rank:0] loss: 0.280, average forward (0.009570) and backward (0.023428) time: 0.039049, iotime: 0.005803 
2022-08-04 05:01:48,887 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052442, Speed: 1220.396192 images/s
2022-08-04 05:01:50,756 [dl_trainer.py:731] WARNING [ 79][ 7840/   98][rank:0] loss: 0.256, average forward (0.009581) and backward (0.024820) time: 0.036290, iotime: 0.001629 
2022-08-04 05:01:50,770 [dl_trainer.py:634] INFO train iter: 7840, num_batches_per_epoch: 98
2022-08-04 05:01:50,770 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 91.836735, lr: 0.100000, avg loss: 0.230233
2022-08-04 05:01:52,401 [dl_trainer.py:822] INFO Epoch 80, lr: 0.100000, val loss: 0.507718, val top-1 acc: 84.922373, top-5 acc: 99.502389
2022-08-04 05:01:52,470 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089572, Speed: 714.512323 images/s
2022-08-04 05:01:53,275 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:53,275 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:01:54,589 [dl_trainer.py:731] WARNING [ 80][ 7880/   98][rank:0] loss: 0.083, average forward (0.011852) and backward (0.026254) time: 0.085696, iotime: 0.005937 
2022-08-04 05:01:55,518 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.053442, Speed: 1197.563503 images/s
2022-08-04 05:01:56,549 [dl_trainer.py:731] WARNING [ 80][ 7920/   98][rank:0] loss: 0.193, average forward (0.010377) and backward (0.024586) time: 0.036954, iotime: 0.001712 
2022-08-04 05:01:57,477 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048972, Speed: 1306.862904 images/s
2022-08-04 05:01:57,478 [dl_trainer.py:634] INFO train iter: 7938, num_batches_per_epoch: 98
2022-08-04 05:01:57,479 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 92.825255, lr: 0.100000, avg loss: 0.201893
2022-08-04 05:01:59,129 [dl_trainer.py:822] INFO Epoch 81, lr: 0.100000, val loss: 0.458227, val top-1 acc: 86.375398, top-5 acc: 99.412818
2022-08-04 05:01:59,948 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:01:59,948 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:00,418 [dl_trainer.py:731] WARNING [ 81][ 7960/   98][rank:0] loss: 0.234, average forward (0.011060) and backward (0.022974) time: 0.082114, iotime: 0.006459 
2022-08-04 05:02:02,146 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081889, Speed: 781.550523 images/s
2022-08-04 05:02:02,392 [dl_trainer.py:731] WARNING [ 81][ 8000/   98][rank:0] loss: 0.237, average forward (0.009727) and backward (0.024903) time: 0.036490, iotime: 0.001606 
2022-08-04 05:02:04,113 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049149, Speed: 1302.149687 images/s
2022-08-04 05:02:04,162 [dl_trainer.py:634] INFO train iter: 8036, num_batches_per_epoch: 98
2022-08-04 05:02:04,163 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 94.068878, lr: 0.010000, avg loss: 0.168760
2022-08-04 05:02:05,828 [dl_trainer.py:822] INFO Epoch 82, lr: 0.010000, val loss: 0.354167, val top-1 acc: 89.072452, top-5 acc: 99.611863
2022-08-04 05:02:06,021 [dl_trainer.py:731] WARNING [ 82][ 8040/   98][rank:0] loss: 0.155, average forward (0.010164) and backward (0.022603) time: 0.076807, iotime: 0.001642 
2022-08-04 05:02:06,594 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:06,594 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:08,170 [dl_trainer.py:731] WARNING [ 82][ 8080/   98][rank:0] loss: 0.204, average forward (0.010191) and backward (0.023689) time: 0.040019, iotime: 0.005885 
2022-08-04 05:02:08,751 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081343, Speed: 786.796322 images/s
2022-08-04 05:02:10,139 [dl_trainer.py:731] WARNING [ 82][ 8120/   98][rank:0] loss: 0.068, average forward (0.009297) and backward (0.024102) time: 0.035218, iotime: 0.001575 
2022-08-04 05:02:10,714 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049068, Speed: 1304.319884 images/s
2022-08-04 05:02:10,812 [dl_trainer.py:634] INFO train iter: 8134, num_batches_per_epoch: 98
2022-08-04 05:02:10,812 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 94.132653, lr: 0.010000, avg loss: 0.155637
2022-08-04 05:02:12,444 [dl_trainer.py:822] INFO Epoch 83, lr: 0.010000, val loss: 0.346987, val top-1 acc: 89.450637, top-5 acc: 99.651672
2022-08-04 05:02:13,156 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:13,157 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:13,900 [dl_trainer.py:731] WARNING [ 83][ 8160/   98][rank:0] loss: 0.105, average forward (0.010730) and backward (0.024303) time: 0.082709, iotime: 0.006496 
2022-08-04 05:02:15,375 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081735, Speed: 783.022576 images/s
2022-08-04 05:02:15,905 [dl_trainer.py:731] WARNING [ 83][ 8200/   98][rank:0] loss: 0.205, average forward (0.009543) and backward (0.025120) time: 0.036587, iotime: 0.001660 
2022-08-04 05:02:17,363 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049679, Speed: 1288.275665 images/s
2022-08-04 05:02:17,508 [dl_trainer.py:634] INFO train iter: 8232, num_batches_per_epoch: 98
2022-08-04 05:02:17,508 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 95.647321, lr: 0.010000, avg loss: 0.134187
2022-08-04 05:02:19,167 [dl_trainer.py:822] INFO Epoch 84, lr: 0.010000, val loss: 0.341834, val top-1 acc: 89.709395, top-5 acc: 99.611863
2022-08-04 05:02:19,588 [dl_trainer.py:731] WARNING [ 84][ 8240/   98][rank:0] loss: 0.139, average forward (0.010512) and backward (0.024314) time: 0.079194, iotime: 0.001740 
2022-08-04 05:02:19,859 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:19,859 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:21,782 [dl_trainer.py:731] WARNING [ 84][ 8280/   98][rank:0] loss: 0.172, average forward (0.009583) and backward (0.024626) time: 0.040512, iotime: 0.006061 
2022-08-04 05:02:22,067 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082478, Speed: 775.961466 images/s
2022-08-04 05:02:23,732 [dl_trainer.py:731] WARNING [ 84][ 8320/   98][rank:0] loss: 0.148, average forward (0.009884) and backward (0.023727) time: 0.035543, iotime: 0.001658 
2022-08-04 05:02:24,036 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049213, Speed: 1300.472902 images/s
2022-08-04 05:02:24,234 [dl_trainer.py:634] INFO train iter: 8330, num_batches_per_epoch: 98
2022-08-04 05:02:24,234 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 95.312500, lr: 0.010000, avg loss: 0.131959
2022-08-04 05:02:25,912 [dl_trainer.py:822] INFO Epoch 85, lr: 0.010000, val loss: 0.344234, val top-1 acc: 89.689490, top-5 acc: 99.651672
2022-08-04 05:02:26,496 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:26,496 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:27,551 [dl_trainer.py:731] WARNING [ 85][ 8360/   98][rank:0] loss: 0.174, average forward (0.010466) and backward (0.023649) time: 0.082402, iotime: 0.005965 
2022-08-04 05:02:28,742 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082532, Speed: 775.454080 images/s
2022-08-04 05:02:29,569 [dl_trainer.py:731] WARNING [ 85][ 8400/   98][rank:0] loss: 0.064, average forward (0.009575) and backward (0.024970) time: 0.036399, iotime: 0.001585 
2022-08-04 05:02:30,693 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048775, Speed: 1312.145228 images/s
2022-08-04 05:02:30,941 [dl_trainer.py:634] INFO train iter: 8428, num_batches_per_epoch: 98
2022-08-04 05:02:30,941 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 95.822704, lr: 0.010000, avg loss: 0.124252
2022-08-04 05:02:32,538 [dl_trainer.py:822] INFO Epoch 86, lr: 0.010000, val loss: 0.343019, val top-1 acc: 89.739252, top-5 acc: 99.641720
2022-08-04 05:02:33,104 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:33,104 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:33,306 [dl_trainer.py:731] WARNING [ 86][ 8440/   98][rank:0] loss: 0.119, average forward (0.008886) and backward (0.023371) time: 0.078538, iotime: 0.005619 
2022-08-04 05:02:35,321 [dl_trainer.py:731] WARNING [ 86][ 8480/   98][rank:0] loss: 0.115, average forward (0.010097) and backward (0.024080) time: 0.036134, iotime: 0.001692 
2022-08-04 05:02:35,331 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081347, Speed: 786.748827 images/s
2022-08-04 05:02:37,307 [dl_trainer.py:731] WARNING [ 86][ 8520/   98][rank:0] loss: 0.043, average forward (0.010387) and backward (0.024928) time: 0.037254, iotime: 0.001665 
2022-08-04 05:02:37,329 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049927, Speed: 1281.860764 images/s
2022-08-04 05:02:37,638 [dl_trainer.py:634] INFO train iter: 8526, num_batches_per_epoch: 98
2022-08-04 05:02:37,638 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 95.695153, lr: 0.010000, avg loss: 0.126216
2022-08-04 05:02:39,277 [dl_trainer.py:822] INFO Epoch 87, lr: 0.010000, val loss: 0.344979, val top-1 acc: 89.729299, top-5 acc: 99.651672
2022-08-04 05:02:39,792 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:39,793 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:41,156 [dl_trainer.py:731] WARNING [ 87][ 8560/   98][rank:0] loss: 0.132, average forward (0.009569) and backward (0.024730) time: 0.081326, iotime: 0.005733 
2022-08-04 05:02:42,032 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082479, Speed: 775.954540 images/s
2022-08-04 05:02:43,107 [dl_trainer.py:731] WARNING [ 87][ 8600/   98][rank:0] loss: 0.124, average forward (0.010209) and backward (0.023118) time: 0.035213, iotime: 0.001647 
2022-08-04 05:02:43,927 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047375, Speed: 1350.914658 images/s
2022-08-04 05:02:44,267 [dl_trainer.py:634] INFO train iter: 8624, num_batches_per_epoch: 98
2022-08-04 05:02:44,267 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 96.301020, lr: 0.010000, avg loss: 0.109069
2022-08-04 05:02:45,923 [dl_trainer.py:822] INFO Epoch 88, lr: 0.010000, val loss: 0.344337, val top-1 acc: 89.759156, top-5 acc: 99.641720
2022-08-04 05:02:46,413 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:46,413 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:46,923 [dl_trainer.py:731] WARNING [ 88][ 8640/   98][rank:0] loss: 0.070, average forward (0.009806) and backward (0.023683) time: 0.082073, iotime: 0.006380 
2022-08-04 05:02:48,641 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082667, Speed: 774.190589 images/s
2022-08-04 05:02:48,929 [dl_trainer.py:731] WARNING [ 88][ 8680/   98][rank:0] loss: 0.104, average forward (0.010984) and backward (0.024636) time: 0.037723, iotime: 0.001814 
2022-08-04 05:02:50,639 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049948, Speed: 1281.332410 images/s
2022-08-04 05:02:50,916 [dl_trainer.py:731] WARNING [ 88][ 8720/   98][rank:0] loss: 0.050, average forward (0.009542) and backward (0.025140) time: 0.036548, iotime: 0.001606 
2022-08-04 05:02:51,032 [dl_trainer.py:634] INFO train iter: 8722, num_batches_per_epoch: 98
2022-08-04 05:02:51,033 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 95.647321, lr: 0.010000, avg loss: 0.124418
2022-08-04 05:02:52,754 [dl_trainer.py:822] INFO Epoch 89, lr: 0.010000, val loss: 0.344846, val top-1 acc: 89.898487, top-5 acc: 99.691481
2022-08-04 05:02:53,168 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:53,168 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:02:54,816 [dl_trainer.py:731] WARNING [ 89][ 8760/   98][rank:0] loss: 0.048, average forward (0.011249) and backward (0.022264) time: 0.083189, iotime: 0.006283 
2022-08-04 05:02:55,384 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083216, Speed: 769.086560 images/s
2022-08-04 05:02:56,821 [dl_trainer.py:731] WARNING [ 89][ 8800/   98][rank:0] loss: 0.163, average forward (0.009190) and backward (0.024214) time: 0.035229, iotime: 0.001580 
2022-08-04 05:02:57,380 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049897, Speed: 1282.634041 images/s
2022-08-04 05:02:57,816 [dl_trainer.py:634] INFO train iter: 8820, num_batches_per_epoch: 98
2022-08-04 05:02:57,816 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 95.982143, lr: 0.010000, avg loss: 0.112840
2022-08-04 05:02:59,480 [dl_trainer.py:822] INFO Epoch 90, lr: 0.010000, val loss: 0.354093, val top-1 acc: 89.808917, top-5 acc: 99.611863
2022-08-04 05:02:59,866 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:02:59,866 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:00,683 [dl_trainer.py:731] WARNING [ 90][ 8840/   98][rank:0] loss: 0.090, average forward (0.010554) and backward (0.025629) time: 0.084386, iotime: 0.005771 
2022-08-04 05:03:02,070 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082261, Speed: 778.010405 images/s
2022-08-04 05:03:02,647 [dl_trainer.py:731] WARNING [ 90][ 8880/   98][rank:0] loss: 0.084, average forward (0.009964) and backward (0.024647) time: 0.036516, iotime: 0.001651 
2022-08-04 05:03:04,080 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050228, Speed: 1274.179723 images/s
2022-08-04 05:03:04,578 [dl_trainer.py:634] INFO train iter: 8918, num_batches_per_epoch: 98
2022-08-04 05:03:04,579 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 95.838648, lr: 0.010000, avg loss: 0.123095
2022-08-04 05:03:06,210 [dl_trainer.py:822] INFO Epoch 91, lr: 0.010000, val loss: 0.350579, val top-1 acc: 90.007962, top-5 acc: 99.641720
2022-08-04 05:03:06,298 [dl_trainer.py:731] WARNING [ 91][ 8920/   98][rank:0] loss: 0.068, average forward (0.008672) and backward (0.024530) time: 0.075812, iotime: 0.001532 
2022-08-04 05:03:06,512 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:06,512 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:08,431 [dl_trainer.py:731] WARNING [ 91][ 8960/   98][rank:0] loss: 0.117, average forward (0.008785) and backward (0.022878) time: 0.037616, iotime: 0.005713 
2022-08-04 05:03:08,700 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081041, Speed: 789.721383 images/s
2022-08-04 05:03:10,401 [dl_trainer.py:731] WARNING [ 91][ 9000/   98][rank:0] loss: 0.065, average forward (0.008804) and backward (0.022564) time: 0.033046, iotime: 0.001458 
2022-08-04 05:03:10,652 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048793, Speed: 1311.657470 images/s
2022-08-04 05:03:11,168 [dl_trainer.py:634] INFO train iter: 9016, num_batches_per_epoch: 98
2022-08-04 05:03:11,168 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 96.540179, lr: 0.010000, avg loss: 0.105782
2022-08-04 05:03:12,852 [dl_trainer.py:822] INFO Epoch 92, lr: 0.010000, val loss: 0.352842, val top-1 acc: 89.858678, top-5 acc: 99.651672
2022-08-04 05:03:13,121 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:13,122 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:14,212 [dl_trainer.py:731] WARNING [ 92][ 9040/   98][rank:0] loss: 0.154, average forward (0.009463) and backward (0.023101) time: 0.081230, iotime: 0.005816 
2022-08-04 05:03:15,327 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081984, Speed: 780.635572 images/s
2022-08-04 05:03:16,285 [dl_trainer.py:731] WARNING [ 92][ 9080/   98][rank:0] loss: 0.053, average forward (0.009341) and backward (0.024000) time: 0.035221, iotime: 0.001634 
2022-08-04 05:03:17,398 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051765, Speed: 1236.366842 images/s
2022-08-04 05:03:17,981 [dl_trainer.py:634] INFO train iter: 9114, num_batches_per_epoch: 98
2022-08-04 05:03:17,981 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 96.524235, lr: 0.010000, avg loss: 0.106259
2022-08-04 05:03:19,595 [dl_trainer.py:822] INFO Epoch 93, lr: 0.010000, val loss: 0.350458, val top-1 acc: 89.749204, top-5 acc: 99.661624
2022-08-04 05:03:19,815 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:19,816 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:20,068 [dl_trainer.py:731] WARNING [ 93][ 9120/   98][rank:0] loss: 0.074, average forward (0.009900) and backward (0.021281) time: 0.077587, iotime: 0.005724 
2022-08-04 05:03:21,974 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080237, Speed: 797.636588 images/s
2022-08-04 05:03:22,001 [dl_trainer.py:731] WARNING [ 93][ 9160/   98][rank:0] loss: 0.151, average forward (0.009324) and backward (0.024355) time: 0.035527, iotime: 0.001600 
2022-08-04 05:03:23,984 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050234, Speed: 1274.028991 images/s
2022-08-04 05:03:24,019 [dl_trainer.py:731] WARNING [ 93][ 9200/   98][rank:0] loss: 0.121, average forward (0.009798) and backward (0.021193) time: 0.032770, iotime: 0.001530 
2022-08-04 05:03:24,634 [dl_trainer.py:634] INFO train iter: 9212, num_batches_per_epoch: 98
2022-08-04 05:03:24,634 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 96.093750, lr: 0.010000, avg loss: 0.113208
2022-08-04 05:03:26,322 [dl_trainer.py:822] INFO Epoch 94, lr: 0.010000, val loss: 0.355435, val top-1 acc: 89.798965, top-5 acc: 99.651672
2022-08-04 05:03:26,519 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:26,520 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:27,905 [dl_trainer.py:731] WARNING [ 94][ 9240/   98][rank:0] loss: 0.174, average forward (0.009294) and backward (0.022806) time: 0.081363, iotime: 0.005684 
2022-08-04 05:03:28,717 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083017, Speed: 770.923232 images/s
2022-08-04 05:03:29,862 [dl_trainer.py:731] WARNING [ 94][ 9280/   98][rank:0] loss: 0.082, average forward (0.009849) and backward (0.023655) time: 0.035451, iotime: 0.001685 
2022-08-04 05:03:30,655 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048431, Speed: 1321.471149 images/s
2022-08-04 05:03:31,365 [dl_trainer.py:634] INFO train iter: 9310, num_batches_per_epoch: 98
2022-08-04 05:03:31,365 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 96.109694, lr: 0.010000, avg loss: 0.109023
2022-08-04 05:03:33,028 [dl_trainer.py:822] INFO Epoch 95, lr: 0.010000, val loss: 0.357648, val top-1 acc: 89.580016, top-5 acc: 99.641720
2022-08-04 05:03:33,145 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:33,145 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:33,717 [dl_trainer.py:731] WARNING [ 95][ 9320/   98][rank:0] loss: 0.101, average forward (0.011549) and backward (0.025359) time: 0.084646, iotime: 0.005765 
2022-08-04 05:03:35,408 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083356, Speed: 767.794245 images/s
2022-08-04 05:03:35,739 [dl_trainer.py:731] WARNING [ 95][ 9360/   98][rank:0] loss: 0.035, average forward (0.010352) and backward (0.025424) time: 0.037754, iotime: 0.001690 
2022-08-04 05:03:37,421 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050326, Speed: 1271.695918 images/s
2022-08-04 05:03:37,754 [dl_trainer.py:731] WARNING [ 95][ 9400/   98][rank:0] loss: 0.104, average forward (0.010487) and backward (0.025297) time: 0.037715, iotime: 0.001655 
2022-08-04 05:03:38,172 [dl_trainer.py:634] INFO train iter: 9408, num_batches_per_epoch: 98
2022-08-04 05:03:38,173 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 96.492347, lr: 0.010000, avg loss: 0.105654
2022-08-04 05:03:39,835 [dl_trainer.py:822] INFO Epoch 96, lr: 0.010000, val loss: 0.356486, val top-1 acc: 89.868631, top-5 acc: 99.611863
2022-08-04 05:03:39,906 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:39,906 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:41,606 [dl_trainer.py:731] WARNING [ 96][ 9440/   98][rank:0] loss: 0.075, average forward (0.010280) and backward (0.025047) time: 0.083565, iotime: 0.005882 
2022-08-04 05:03:42,109 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082188, Speed: 778.705026 images/s
2022-08-04 05:03:43,564 [dl_trainer.py:731] WARNING [ 96][ 9480/   98][rank:0] loss: 0.086, average forward (0.009688) and backward (0.023876) time: 0.035392, iotime: 0.001574 
2022-08-04 05:03:44,080 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049248, Speed: 1299.545212 images/s
2022-08-04 05:03:44,895 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:44,895 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:44,896 [dl_trainer.py:634] INFO train iter: 9506, num_batches_per_epoch: 98
2022-08-04 05:03:44,896 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 96.588010, lr: 0.010000, avg loss: 0.103375
2022-08-04 05:03:46,605 [dl_trainer.py:822] INFO Epoch 97, lr: 0.010000, val loss: 0.363037, val top-1 acc: 89.749204, top-5 acc: 99.611863
2022-08-04 05:03:47,503 [dl_trainer.py:731] WARNING [ 97][ 9520/   98][rank:0] loss: 0.066, average forward (0.009754) and backward (0.025180) time: 0.084036, iotime: 0.006032 
2022-08-04 05:03:48,842 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083520, Speed: 766.285382 images/s
2022-08-04 05:03:49,428 [dl_trainer.py:731] WARNING [ 97][ 9560/   98][rank:0] loss: 0.065, average forward (0.008722) and backward (0.022825) time: 0.033327, iotime: 0.001540 
2022-08-04 05:03:50,695 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046299, Speed: 1382.319315 images/s
2022-08-04 05:03:51,320 [dl_trainer.py:731] WARNING [ 97][ 9600/   98][rank:0] loss: 0.205, average forward (0.009200) and backward (0.022801) time: 0.033823, iotime: 0.001563 
2022-08-04 05:03:51,484 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:51,484 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:51,701 [dl_trainer.py:634] INFO train iter: 9604, num_batches_per_epoch: 98
2022-08-04 05:03:51,701 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 96.827168, lr: 0.010000, avg loss: 0.099448
2022-08-04 05:03:53,354 [dl_trainer.py:822] INFO Epoch 98, lr: 0.010000, val loss: 0.359886, val top-1 acc: 89.828822, top-5 acc: 99.611863
2022-08-04 05:03:55,114 [dl_trainer.py:731] WARNING [ 98][ 9640/   98][rank:0] loss: 0.137, average forward (0.009131) and backward (0.025384) time: 0.082142, iotime: 0.005499 
2022-08-04 05:03:55,324 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081192, Speed: 788.251753 images/s
2022-08-04 05:03:57,042 [dl_trainer.py:731] WARNING [ 98][ 9680/   98][rank:0] loss: 0.080, average forward (0.009535) and backward (0.024423) time: 0.035987, iotime: 0.001771 
2022-08-04 05:03:57,264 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048498, Speed: 1319.650029 images/s
2022-08-04 05:03:58,079 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:03:58,079 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:03:58,351 [dl_trainer.py:634] INFO train iter: 9702, num_batches_per_epoch: 98
2022-08-04 05:03:58,352 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 96.811224, lr: 0.010000, avg loss: 0.098168
2022-08-04 05:03:59,999 [dl_trainer.py:822] INFO Epoch 99, lr: 0.010000, val loss: 0.361915, val top-1 acc: 89.749204, top-5 acc: 99.631768
2022-08-04 05:04:00,918 [dl_trainer.py:731] WARNING [ 99][ 9720/   98][rank:0] loss: 0.137, average forward (0.009485) and backward (0.025020) time: 0.082001, iotime: 0.005985 
2022-08-04 05:04:01,962 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082396, Speed: 776.738222 images/s
2022-08-04 05:04:02,887 [dl_trainer.py:731] WARNING [ 99][ 9760/   98][rank:0] loss: 0.058, average forward (0.009736) and backward (0.025200) time: 0.036859, iotime: 0.001653 
2022-08-04 05:04:03,931 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049205, Speed: 1300.680058 images/s
2022-08-04 05:04:04,708 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:04,709 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:05,025 [dl_trainer.py:731] WARNING [ 99][ 9800/   98][rank:0] loss: 0.037, average forward (0.010163) and backward (0.025540) time: 0.041569, iotime: 0.005595 
2022-08-04 05:04:05,038 [dl_trainer.py:634] INFO train iter: 9800, num_batches_per_epoch: 98
2022-08-04 05:04:05,038 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 96.396684, lr: 0.010000, avg loss: 0.103881
2022-08-04 05:04:06,731 [dl_trainer.py:822] INFO Epoch 100, lr: 0.010000, val loss: 0.368074, val top-1 acc: 89.699443, top-5 acc: 99.631768
2022-08-04 05:04:08,652 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082785, Speed: 773.088085 images/s
2022-08-04 05:04:08,732 [dl_trainer.py:731] WARNING [100][ 9840/   98][rank:0] loss: 0.096, average forward (0.010443) and backward (0.021453) time: 0.077520, iotime: 0.001789 
2022-08-04 05:04:10,647 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049876, Speed: 1283.194905 images/s
2022-08-04 05:04:10,731 [dl_trainer.py:731] WARNING [100][ 9880/   98][rank:0] loss: 0.055, average forward (0.008942) and backward (0.023153) time: 0.033985, iotime: 0.001625 
2022-08-04 05:04:11,444 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:11,444 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:11,805 [dl_trainer.py:634] INFO train iter: 9898, num_batches_per_epoch: 98
2022-08-04 05:04:11,806 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 96.875000, lr: 0.010000, avg loss: 0.096164
2022-08-04 05:04:13,415 [dl_trainer.py:822] INFO Epoch 101, lr: 0.010000, val loss: 0.365750, val top-1 acc: 89.848726, top-5 acc: 99.621815
2022-08-04 05:04:14,496 [dl_trainer.py:731] WARNING [101][ 9920/   98][rank:0] loss: 0.059, average forward (0.010096) and backward (0.024778) time: 0.081318, iotime: 0.005866 
2022-08-04 05:04:15,257 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080852, Speed: 791.565253 images/s
2022-08-04 05:04:16,483 [dl_trainer.py:731] WARNING [101][ 9960/   98][rank:0] loss: 0.034, average forward (0.009395) and backward (0.024304) time: 0.035549, iotime: 0.001617 
2022-08-04 05:04:17,258 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050019, Speed: 1279.510380 images/s
2022-08-04 05:04:18,045 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:18,045 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:18,473 [dl_trainer.py:634] INFO train iter: 9996, num_batches_per_epoch: 98
2022-08-04 05:04:18,474 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 96.922832, lr: 0.010000, avg loss: 0.091484
2022-08-04 05:04:20,133 [dl_trainer.py:822] INFO Epoch 102, lr: 0.010000, val loss: 0.364339, val top-1 acc: 89.779061, top-5 acc: 99.631768
2022-08-04 05:04:20,343 [dl_trainer.py:731] WARNING [102][10000/   98][rank:0] loss: 0.030, average forward (0.010777) and backward (0.025017) time: 0.084193, iotime: 0.006100 
2022-08-04 05:04:21,944 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082189, Speed: 778.688540 images/s
2022-08-04 05:04:22,335 [dl_trainer.py:731] WARNING [102][10040/   98][rank:0] loss: 0.060, average forward (0.009714) and backward (0.023643) time: 0.035271, iotime: 0.001630 
2022-08-04 05:04:23,895 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048758, Speed: 1312.602861 images/s
2022-08-04 05:04:24,262 [dl_trainer.py:731] WARNING [102][10080/   98][rank:0] loss: 0.107, average forward (0.010064) and backward (0.024090) time: 0.036094, iotime: 0.001672 
2022-08-04 05:04:24,657 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:24,657 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:25,142 [dl_trainer.py:634] INFO train iter: 10094, num_batches_per_epoch: 98
2022-08-04 05:04:25,142 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 96.699617, lr: 0.010000, avg loss: 0.096849
2022-08-04 05:04:26,771 [dl_trainer.py:822] INFO Epoch 103, lr: 0.010000, val loss: 0.368495, val top-1 acc: 89.709395, top-5 acc: 99.671576
2022-08-04 05:04:28,103 [dl_trainer.py:731] WARNING [103][10120/   98][rank:0] loss: 0.142, average forward (0.010153) and backward (0.025007) time: 0.082152, iotime: 0.005894 
2022-08-04 05:04:28,565 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081913, Speed: 781.319450 images/s
2022-08-04 05:04:30,093 [dl_trainer.py:731] WARNING [103][10160/   98][rank:0] loss: 0.039, average forward (0.011551) and backward (0.026219) time: 0.040067, iotime: 0.001982 
2022-08-04 05:04:30,541 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049373, Speed: 1296.245000 images/s
2022-08-04 05:04:31,351 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:31,352 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:31,844 [dl_trainer.py:634] INFO train iter: 10192, num_batches_per_epoch: 98
2022-08-04 05:04:31,844 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 97.034439, lr: 0.010000, avg loss: 0.090385
2022-08-04 05:04:33,468 [dl_trainer.py:822] INFO Epoch 104, lr: 0.010000, val loss: 0.367032, val top-1 acc: 89.798965, top-5 acc: 99.671576
2022-08-04 05:04:33,869 [dl_trainer.py:731] WARNING [104][10200/   98][rank:0] loss: 0.112, average forward (0.009973) and backward (0.025227) time: 0.082329, iotime: 0.005679 
2022-08-04 05:04:35,228 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082200, Speed: 778.591104 images/s
2022-08-04 05:04:35,915 [dl_trainer.py:731] WARNING [104][10240/   98][rank:0] loss: 0.095, average forward (0.008686) and backward (0.022772) time: 0.033258, iotime: 0.001546 
2022-08-04 05:04:37,219 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049760, Speed: 1286.162184 images/s
2022-08-04 05:04:37,919 [dl_trainer.py:731] WARNING [104][10280/   98][rank:0] loss: 0.049, average forward (0.010501) and backward (0.026074) time: 0.038633, iotime: 0.001766 
2022-08-04 05:04:38,030 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:38,030 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:38,620 [dl_trainer.py:634] INFO train iter: 10290, num_batches_per_epoch: 98
2022-08-04 05:04:38,621 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 97.002551, lr: 0.010000, avg loss: 0.087266
2022-08-04 05:04:40,236 [dl_trainer.py:822] INFO Epoch 105, lr: 0.010000, val loss: 0.370319, val top-1 acc: 89.878583, top-5 acc: 99.671576
2022-08-04 05:04:41,648 [dl_trainer.py:731] WARNING [105][10320/   98][rank:0] loss: 0.097, average forward (0.009552) and backward (0.022531) time: 0.079173, iotime: 0.006371 
2022-08-04 05:04:41,806 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080445, Speed: 795.576013 images/s
2022-08-04 05:04:43,661 [dl_trainer.py:731] WARNING [105][10360/   98][rank:0] loss: 0.033, average forward (0.009158) and backward (0.024332) time: 0.035374, iotime: 0.001622 
2022-08-04 05:04:43,821 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050368, Speed: 1270.640385 images/s
2022-08-04 05:04:44,621 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:44,621 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:45,238 [dl_trainer.py:634] INFO train iter: 10388, num_batches_per_epoch: 98
2022-08-04 05:04:45,239 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 96.683673, lr: 0.010000, avg loss: 0.093385
2022-08-04 05:04:46,881 [dl_trainer.py:822] INFO Epoch 106, lr: 0.010000, val loss: 0.371046, val top-1 acc: 89.858678, top-5 acc: 99.661624
2022-08-04 05:04:47,446 [dl_trainer.py:731] WARNING [106][10400/   98][rank:0] loss: 0.091, average forward (0.011069) and backward (0.024015) time: 0.083341, iotime: 0.006292 
2022-08-04 05:04:48,428 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080799, Speed: 792.086035 images/s
2022-08-04 05:04:49,406 [dl_trainer.py:731] WARNING [106][10440/   98][rank:0] loss: 0.086, average forward (0.010046) and backward (0.024616) time: 0.036613, iotime: 0.001687 
2022-08-04 05:04:50,408 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049484, Speed: 1293.345719 images/s
2022-08-04 05:04:51,212 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:51,212 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:51,605 [dl_trainer.py:731] WARNING [106][10480/   98][rank:0] loss: 0.114, average forward (0.010172) and backward (0.026002) time: 0.042567, iotime: 0.006112 
2022-08-04 05:04:51,891 [dl_trainer.py:634] INFO train iter: 10486, num_batches_per_epoch: 98
2022-08-04 05:04:51,891 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 97.082270, lr: 0.010000, avg loss: 0.087362
2022-08-04 05:04:53,520 [dl_trainer.py:822] INFO Epoch 107, lr: 0.010000, val loss: 0.372074, val top-1 acc: 89.689490, top-5 acc: 99.671576
2022-08-04 05:04:55,082 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081971, Speed: 780.767662 images/s
2022-08-04 05:04:55,210 [dl_trainer.py:731] WARNING [107][10520/   98][rank:0] loss: 0.093, average forward (0.009048) and backward (0.024187) time: 0.075837, iotime: 0.001571 
2022-08-04 05:04:57,024 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048558, Speed: 1318.003449 images/s
2022-08-04 05:04:57,159 [dl_trainer.py:731] WARNING [107][10560/   98][rank:0] loss: 0.090, average forward (0.009279) and backward (0.025216) time: 0.036288, iotime: 0.001553 
2022-08-04 05:04:57,830 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:04:57,830 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:04:58,582 [dl_trainer.py:634] INFO train iter: 10584, num_batches_per_epoch: 98
2022-08-04 05:04:58,582 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 97.305485, lr: 0.010000, avg loss: 0.084598
2022-08-04 05:05:00,237 [dl_trainer.py:822] INFO Epoch 108, lr: 0.010000, val loss: 0.374720, val top-1 acc: 89.908439, top-5 acc: 99.651672
2022-08-04 05:05:01,027 [dl_trainer.py:731] WARNING [108][10600/   98][rank:0] loss: 0.116, average forward (0.009396) and backward (0.023390) time: 0.080975, iotime: 0.005985 
2022-08-04 05:05:01,765 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083141, Speed: 769.776406 images/s
2022-08-04 05:05:03,008 [dl_trainer.py:731] WARNING [108][10640/   98][rank:0] loss: 0.038, average forward (0.010118) and backward (0.024392) time: 0.036497, iotime: 0.001729 
2022-08-04 05:05:03,676 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047768, Speed: 1339.802814 images/s
2022-08-04 05:05:04,495 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:04,495 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:05,157 [dl_trainer.py:731] WARNING [108][10680/   98][rank:0] loss: 0.193, average forward (0.010247) and backward (0.023992) time: 0.040702, iotime: 0.006197 
2022-08-04 05:05:05,245 [dl_trainer.py:634] INFO train iter: 10682, num_batches_per_epoch: 98
2022-08-04 05:05:05,246 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 97.082270, lr: 0.010000, avg loss: 0.085978
2022-08-04 05:05:06,861 [dl_trainer.py:822] INFO Epoch 109, lr: 0.010000, val loss: 0.377573, val top-1 acc: 89.749204, top-5 acc: 99.661624
2022-08-04 05:05:08,289 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080907, Speed: 791.033501 images/s
2022-08-04 05:05:08,715 [dl_trainer.py:731] WARNING [109][10720/   98][rank:0] loss: 0.057, average forward (0.009500) and backward (0.024456) time: 0.076259, iotime: 0.001578 
2022-08-04 05:05:10,222 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048313, Speed: 1324.682692 images/s
2022-08-04 05:05:10,640 [dl_trainer.py:731] WARNING [109][10760/   98][rank:0] loss: 0.150, average forward (0.008892) and backward (0.024287) time: 0.034939, iotime: 0.001536 
2022-08-04 05:05:11,024 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:11,025 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:11,843 [dl_trainer.py:634] INFO train iter: 10780, num_batches_per_epoch: 98
2022-08-04 05:05:11,844 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 96.444515, lr: 0.010000, avg loss: 0.096936
2022-08-04 05:05:13,479 [dl_trainer.py:822] INFO Epoch 110, lr: 0.010000, val loss: 0.375974, val top-1 acc: 89.709395, top-5 acc: 99.641720
2022-08-04 05:05:14,453 [dl_trainer.py:731] WARNING [110][10800/   98][rank:0] loss: 0.036, average forward (0.010528) and backward (0.024148) time: 0.082300, iotime: 0.005909 
2022-08-04 05:05:14,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081127, Speed: 788.882539 images/s
2022-08-04 05:05:16,393 [dl_trainer.py:731] WARNING [110][10840/   98][rank:0] loss: 0.087, average forward (0.009595) and backward (0.023269) time: 0.034673, iotime: 0.001563 
2022-08-04 05:05:16,799 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048777, Speed: 1312.091193 images/s
2022-08-04 05:05:17,585 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:17,585 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:18,420 [dl_trainer.py:634] INFO train iter: 10878, num_batches_per_epoch: 98
2022-08-04 05:05:18,421 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 96.683673, lr: 0.010000, avg loss: 0.092553
2022-08-04 05:05:20,080 [dl_trainer.py:822] INFO Epoch 111, lr: 0.010000, val loss: 0.381055, val top-1 acc: 89.898487, top-5 acc: 99.641720
2022-08-04 05:05:20,175 [dl_trainer.py:731] WARNING [111][10880/   98][rank:0] loss: 0.107, average forward (0.009125) and backward (0.024831) time: 0.081505, iotime: 0.005756 
2022-08-04 05:05:21,475 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082024, Speed: 780.262528 images/s
2022-08-04 05:05:22,215 [dl_trainer.py:731] WARNING [111][10920/   98][rank:0] loss: 0.083, average forward (0.008582) and backward (0.023549) time: 0.033834, iotime: 0.001481 
2022-08-04 05:05:23,448 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049300, Speed: 1298.177187 images/s
2022-08-04 05:05:24,176 [dl_trainer.py:731] WARNING [111][10960/   98][rank:0] loss: 0.166, average forward (0.008686) and backward (0.023419) time: 0.033851, iotime: 0.001524 
2022-08-04 05:05:24,241 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:24,241 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:25,153 [dl_trainer.py:634] INFO train iter: 10976, num_batches_per_epoch: 98
2022-08-04 05:05:25,153 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 97.050383, lr: 0.010000, avg loss: 0.086133
2022-08-04 05:05:26,799 [dl_trainer.py:822] INFO Epoch 112, lr: 0.010000, val loss: 0.383147, val top-1 acc: 89.848726, top-5 acc: 99.681529
2022-08-04 05:05:27,948 [dl_trainer.py:731] WARNING [112][11000/   98][rank:0] loss: 0.036, average forward (0.009778) and backward (0.022792) time: 0.080080, iotime: 0.005558 
2022-08-04 05:05:28,067 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081026, Speed: 789.872182 images/s
2022-08-04 05:05:29,907 [dl_trainer.py:731] WARNING [112][11040/   98][rank:0] loss: 0.020, average forward (0.009298) and backward (0.020971) time: 0.032088, iotime: 0.001573 
2022-08-04 05:05:30,008 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048516, Speed: 1319.139498 images/s
2022-08-04 05:05:30,775 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:30,775 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:31,747 [dl_trainer.py:634] INFO train iter: 11074, num_batches_per_epoch: 98
2022-08-04 05:05:31,747 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 96.954719, lr: 0.010000, avg loss: 0.087427
2022-08-04 05:05:33,368 [dl_trainer.py:822] INFO Epoch 113, lr: 0.010000, val loss: 0.384033, val top-1 acc: 89.958201, top-5 acc: 99.661624
2022-08-04 05:05:33,652 [dl_trainer.py:731] WARNING [113][11080/   98][rank:0] loss: 0.125, average forward (0.009948) and backward (0.024714) time: 0.081517, iotime: 0.005990 
2022-08-04 05:05:34,602 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080568, Speed: 794.358065 images/s
2022-08-04 05:05:35,614 [dl_trainer.py:731] WARNING [113][11120/   98][rank:0] loss: 0.137, average forward (0.008710) and backward (0.023562) time: 0.034078, iotime: 0.001570 
2022-08-04 05:05:36,595 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049825, Speed: 1284.492950 images/s
2022-08-04 05:05:37,397 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:37,397 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:37,790 [dl_trainer.py:731] WARNING [113][11160/   98][rank:0] loss: 0.138, average forward (0.010233) and backward (0.021347) time: 0.037843, iotime: 0.006014 
2022-08-04 05:05:38,421 [dl_trainer.py:634] INFO train iter: 11172, num_batches_per_epoch: 98
2022-08-04 05:05:38,422 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 96.890944, lr: 0.010000, avg loss: 0.085710
2022-08-04 05:05:40,110 [dl_trainer.py:822] INFO Epoch 114, lr: 0.010000, val loss: 0.388234, val top-1 acc: 89.609873, top-5 acc: 99.651672
2022-08-04 05:05:41,326 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082970, Speed: 771.366992 images/s
2022-08-04 05:05:41,511 [dl_trainer.py:731] WARNING [114][11200/   98][rank:0] loss: 0.060, average forward (0.009219) and backward (0.023679) time: 0.077449, iotime: 0.001584 
2022-08-04 05:05:43,304 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049437, Speed: 1294.565897 images/s
2022-08-04 05:05:43,499 [dl_trainer.py:731] WARNING [114][11240/   98][rank:0] loss: 0.065, average forward (0.010959) and backward (0.025052) time: 0.038217, iotime: 0.001821 
2022-08-04 05:05:44,113 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:44,113 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:45,194 [dl_trainer.py:634] INFO train iter: 11270, num_batches_per_epoch: 98
2022-08-04 05:05:45,195 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 96.970663, lr: 0.010000, avg loss: 0.083452
2022-08-04 05:05:46,821 [dl_trainer.py:822] INFO Epoch 115, lr: 0.010000, val loss: 0.387652, val top-1 acc: 89.729299, top-5 acc: 99.681529
2022-08-04 05:05:47,325 [dl_trainer.py:731] WARNING [115][11280/   98][rank:0] loss: 0.130, average forward (0.011272) and backward (0.026301) time: 0.084549, iotime: 0.005906 
2022-08-04 05:05:47,967 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081776, Speed: 782.626110 images/s
2022-08-04 05:05:49,284 [dl_trainer.py:731] WARNING [115][11320/   98][rank:0] loss: 0.072, average forward (0.010451) and backward (0.023280) time: 0.035781, iotime: 0.001787 
2022-08-04 05:05:49,944 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049412, Speed: 1295.233485 images/s
2022-08-04 05:05:50,754 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:50,754 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:51,423 [dl_trainer.py:731] WARNING [115][11360/   98][rank:0] loss: 0.090, average forward (0.010755) and backward (0.024784) time: 0.042201, iotime: 0.006374 
2022-08-04 05:05:51,844 [dl_trainer.py:634] INFO train iter: 11368, num_batches_per_epoch: 98
2022-08-04 05:05:51,844 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 97.114158, lr: 0.010000, avg loss: 0.084317
2022-08-04 05:05:53,489 [dl_trainer.py:822] INFO Epoch 116, lr: 0.010000, val loss: 0.384849, val top-1 acc: 89.838774, top-5 acc: 99.691481
2022-08-04 05:05:54,621 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082027, Speed: 780.230897 images/s
2022-08-04 05:05:55,104 [dl_trainer.py:731] WARNING [116][11400/   98][rank:0] loss: 0.041, average forward (0.011085) and backward (0.024870) time: 0.079856, iotime: 0.001921 
2022-08-04 05:05:56,568 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048665, Speed: 1315.105182 images/s
2022-08-04 05:05:57,059 [dl_trainer.py:731] WARNING [116][11440/   98][rank:0] loss: 0.225, average forward (0.010350) and backward (0.024271) time: 0.036599, iotime: 0.001696 
2022-08-04 05:05:57,354 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:05:57,354 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:05:58,527 [dl_trainer.py:634] INFO train iter: 11466, num_batches_per_epoch: 98
2022-08-04 05:05:58,528 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 97.114158, lr: 0.010000, avg loss: 0.083176
2022-08-04 05:06:00,249 [dl_trainer.py:822] INFO Epoch 117, lr: 0.010000, val loss: 0.389418, val top-1 acc: 89.958201, top-5 acc: 99.691481
2022-08-04 05:06:00,959 [dl_trainer.py:731] WARNING [117][11480/   98][rank:0] loss: 0.047, average forward (0.010166) and backward (0.024202) time: 0.083909, iotime: 0.006149 
2022-08-04 05:06:01,323 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083414, Speed: 767.259273 images/s
2022-08-04 05:06:02,939 [dl_trainer.py:731] WARNING [117][11520/   98][rank:0] loss: 0.141, average forward (0.009840) and backward (0.024185) time: 0.035943, iotime: 0.001656 
2022-08-04 05:06:03,273 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048742, Speed: 1313.023561 images/s
2022-08-04 05:06:04,074 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:04,075 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:05,076 [dl_trainer.py:731] WARNING [117][11560/   98][rank:0] loss: 0.116, average forward (0.009133) and backward (0.023510) time: 0.038516, iotime: 0.005615 
2022-08-04 05:06:05,289 [dl_trainer.py:634] INFO train iter: 11564, num_batches_per_epoch: 98
2022-08-04 05:06:05,290 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 97.002551, lr: 0.010000, avg loss: 0.084353
2022-08-04 05:06:06,895 [dl_trainer.py:822] INFO Epoch 118, lr: 0.010000, val loss: 0.387565, val top-1 acc: 89.858678, top-5 acc: 99.582006
2022-08-04 05:06:07,906 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081254, Speed: 787.654978 images/s
2022-08-04 05:06:08,677 [dl_trainer.py:731] WARNING [118][11600/   98][rank:0] loss: 0.077, average forward (0.009305) and backward (0.024908) time: 0.076810, iotime: 0.001680 
2022-08-04 05:06:09,848 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048535, Speed: 1318.638754 images/s
2022-08-04 05:06:10,653 [dl_trainer.py:731] WARNING [118][11640/   98][rank:0] loss: 0.156, average forward (0.009795) and backward (0.025179) time: 0.036881, iotime: 0.001625 
2022-08-04 05:06:10,667 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:10,668 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:11,948 [dl_trainer.py:634] INFO train iter: 11662, num_batches_per_epoch: 98
2022-08-04 05:06:11,949 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 96.986607, lr: 0.010000, avg loss: 0.082773
2022-08-04 05:06:13,568 [dl_trainer.py:822] INFO Epoch 119, lr: 0.010000, val loss: 0.387703, val top-1 acc: 89.779061, top-5 acc: 99.641720
2022-08-04 05:06:14,447 [dl_trainer.py:731] WARNING [119][11680/   98][rank:0] loss: 0.075, average forward (0.009622) and backward (0.022075) time: 0.078970, iotime: 0.006493 
2022-08-04 05:06:14,502 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081631, Speed: 784.011662 images/s
2022-08-04 05:06:16,460 [dl_trainer.py:731] WARNING [119][11720/   98][rank:0] loss: 0.049, average forward (0.008998) and backward (0.023843) time: 0.034606, iotime: 0.001534 
2022-08-04 05:06:16,520 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050448, Speed: 1268.641881 images/s
2022-08-04 05:06:17,361 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:17,362 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:18,671 [dl_trainer.py:731] WARNING [119][11760/   98][rank:0] loss: 0.020, average forward (0.009772) and backward (0.024348) time: 0.040342, iotime: 0.005975 
2022-08-04 05:06:18,681 [dl_trainer.py:634] INFO train iter: 11760, num_batches_per_epoch: 98
2022-08-04 05:06:18,682 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 97.098214, lr: 0.010000, avg loss: 0.088165
2022-08-04 05:06:20,293 [dl_trainer.py:822] INFO Epoch 120, lr: 0.010000, val loss: 0.391837, val top-1 acc: 90.077627, top-5 acc: 99.631768
2022-08-04 05:06:21,215 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082336, Speed: 777.300003 images/s
2022-08-04 05:06:22,228 [dl_trainer.py:731] WARNING [120][11800/   98][rank:0] loss: 0.108, average forward (0.009966) and backward (0.024366) time: 0.076887, iotime: 0.001654 
2022-08-04 05:06:23,120 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047613, Speed: 1344.159916 images/s
2022-08-04 05:06:23,920 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:23,920 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:24,381 [dl_trainer.py:731] WARNING [120][11840/   98][rank:0] loss: 0.060, average forward (0.010318) and backward (0.024354) time: 0.040841, iotime: 0.005884 
2022-08-04 05:06:25,272 [dl_trainer.py:634] INFO train iter: 11858, num_batches_per_epoch: 98
2022-08-04 05:06:25,272 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 97.321429, lr: 0.010000, avg loss: 0.083786
2022-08-04 05:06:26,915 [dl_trainer.py:822] INFO Epoch 121, lr: 0.010000, val loss: 0.397661, val top-1 acc: 89.530255, top-5 acc: 99.621815
2022-08-04 05:06:27,762 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081406, Speed: 786.180500 images/s
2022-08-04 05:06:27,976 [dl_trainer.py:731] WARNING [121][11880/   98][rank:0] loss: 0.050, average forward (0.009580) and backward (0.024054) time: 0.076663, iotime: 0.001641 
2022-08-04 05:06:29,698 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048378, Speed: 1322.914325 images/s
2022-08-04 05:06:29,924 [dl_trainer.py:731] WARNING [121][11920/   98][rank:0] loss: 0.221, average forward (0.009930) and backward (0.023501) time: 0.035337, iotime: 0.001659 
2022-08-04 05:06:30,466 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:30,466 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:31,868 [dl_trainer.py:634] INFO train iter: 11956, num_batches_per_epoch: 98
2022-08-04 05:06:31,868 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 97.082270, lr: 0.010000, avg loss: 0.085676
2022-08-04 05:06:33,554 [dl_trainer.py:822] INFO Epoch 122, lr: 0.010000, val loss: 0.396557, val top-1 acc: 89.769108, top-5 acc: 99.621815
2022-08-04 05:06:33,745 [dl_trainer.py:731] WARNING [122][11960/   98][rank:0] loss: 0.061, average forward (0.009319) and backward (0.023995) time: 0.082358, iotime: 0.006107 
2022-08-04 05:06:34,336 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081366, Speed: 786.566342 images/s
2022-08-04 05:06:35,705 [dl_trainer.py:731] WARNING [122][12000/   98][rank:0] loss: 0.067, average forward (0.010164) and backward (0.024692) time: 0.036722, iotime: 0.001593 
2022-08-04 05:06:36,310 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049335, Speed: 1297.263896 images/s
2022-08-04 05:06:37,076 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:37,076 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:37,819 [dl_trainer.py:731] WARNING [122][12040/   98][rank:0] loss: 0.087, average forward (0.009284) and backward (0.023640) time: 0.039121, iotime: 0.005939 
2022-08-04 05:06:38,516 [dl_trainer.py:634] INFO train iter: 12054, num_batches_per_epoch: 98
2022-08-04 05:06:38,517 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 97.337372, lr: 0.001000, avg loss: 0.077945
2022-08-04 05:06:40,164 [dl_trainer.py:822] INFO Epoch 123, lr: 0.001000, val loss: 0.398683, val top-1 acc: 89.699443, top-5 acc: 99.671576
2022-08-04 05:06:40,929 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081004, Speed: 790.079866 images/s
2022-08-04 05:06:41,431 [dl_trainer.py:731] WARNING [123][12080/   98][rank:0] loss: 0.122, average forward (0.010889) and backward (0.022958) time: 0.077145, iotime: 0.001764 
2022-08-04 05:06:42,877 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048705, Speed: 1314.024792 images/s
2022-08-04 05:06:43,433 [dl_trainer.py:731] WARNING [123][12120/   98][rank:0] loss: 0.141, average forward (0.010333) and backward (0.025833) time: 0.038157, iotime: 0.001719 
2022-08-04 05:06:43,690 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:43,691 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:45,168 [dl_trainer.py:634] INFO train iter: 12152, num_batches_per_epoch: 98
2022-08-04 05:06:45,169 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 97.783801, lr: 0.001000, avg loss: 0.075367
2022-08-04 05:06:46,835 [dl_trainer.py:822] INFO Epoch 124, lr: 0.001000, val loss: 0.390774, val top-1 acc: 89.729299, top-5 acc: 99.651672
2022-08-04 05:06:47,233 [dl_trainer.py:731] WARNING [124][12160/   98][rank:0] loss: 0.037, average forward (0.010456) and backward (0.023072) time: 0.082309, iotime: 0.006283 
2022-08-04 05:06:47,527 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081547, Speed: 784.826567 images/s
2022-08-04 05:06:49,156 [dl_trainer.py:731] WARNING [124][12200/   98][rank:0] loss: 0.038, average forward (0.008916) and backward (0.023864) time: 0.034556, iotime: 0.001541 
2022-08-04 05:06:49,445 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047941, Speed: 1334.965629 images/s
2022-08-04 05:06:50,248 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:50,248 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:51,323 [dl_trainer.py:731] WARNING [124][12240/   98][rank:0] loss: 0.130, average forward (0.009099) and backward (0.023492) time: 0.039009, iotime: 0.006166 
2022-08-04 05:06:51,800 [dl_trainer.py:634] INFO train iter: 12250, num_batches_per_epoch: 98
2022-08-04 05:06:51,801 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 97.289541, lr: 0.001000, avg loss: 0.076561
2022-08-04 05:06:53,509 [dl_trainer.py:822] INFO Epoch 125, lr: 0.001000, val loss: 0.394372, val top-1 acc: 89.908439, top-5 acc: 99.621815
2022-08-04 05:06:54,147 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082472, Speed: 776.017113 images/s
2022-08-04 05:06:54,952 [dl_trainer.py:731] WARNING [125][12280/   98][rank:0] loss: 0.079, average forward (0.009810) and backward (0.024913) time: 0.079470, iotime: 0.001684 
2022-08-04 05:06:56,090 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.048569, Speed: 1317.716185 images/s
2022-08-04 05:06:56,870 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:06:56,871 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:06:57,085 [dl_trainer.py:731] WARNING [125][12320/   98][rank:0] loss: 0.123, average forward (0.010198) and backward (0.024638) time: 0.041278, iotime: 0.006129 
2022-08-04 05:06:58,463 [dl_trainer.py:634] INFO train iter: 12348, num_batches_per_epoch: 98
2022-08-04 05:06:58,464 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 97.624362, lr: 0.001000, avg loss: 0.075608
2022-08-04 05:07:00,106 [dl_trainer.py:822] INFO Epoch 126, lr: 0.001000, val loss: 0.394683, val top-1 acc: 89.858678, top-5 acc: 99.631768
2022-08-04 05:07:00,683 [dl_trainer.py:731] WARNING [126][12360/   98][rank:0] loss: 0.098, average forward (0.009614) and backward (0.022380) time: 0.075435, iotime: 0.001566 
2022-08-04 05:07:00,705 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080944, Speed: 790.674928 images/s
2022-08-04 05:07:02,656 [dl_trainer.py:731] WARNING [126][12400/   98][rank:0] loss: 0.113, average forward (0.009403) and backward (0.023685) time: 0.034948, iotime: 0.001598 
2022-08-04 05:07:02,668 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049058, Speed: 1304.565832 images/s
2022-08-04 05:07:03,444 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:03,444 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:04,766 [dl_trainer.py:731] WARNING [126][12440/   98][rank:0] loss: 0.070, average forward (0.009263) and backward (0.023854) time: 0.039367, iotime: 0.006013 
2022-08-04 05:07:05,079 [dl_trainer.py:634] INFO train iter: 12446, num_batches_per_epoch: 98
2022-08-04 05:07:05,080 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 97.464923, lr: 0.001000, avg loss: 0.072618
2022-08-04 05:07:06,712 [dl_trainer.py:822] INFO Epoch 127, lr: 0.001000, val loss: 0.395383, val top-1 acc: 89.828822, top-5 acc: 99.621815
2022-08-04 05:07:07,263 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080598, Speed: 794.067224 images/s
2022-08-04 05:07:08,403 [dl_trainer.py:731] WARNING [127][12480/   98][rank:0] loss: 0.034, average forward (0.011212) and backward (0.024959) time: 0.079139, iotime: 0.001789 
2022-08-04 05:07:09,303 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050986, Speed: 1255.237638 images/s
2022-08-04 05:07:10,092 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:10,092 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:10,586 [dl_trainer.py:731] WARNING [127][12520/   98][rank:0] loss: 0.061, average forward (0.009946) and backward (0.025192) time: 0.041248, iotime: 0.005841 
2022-08-04 05:07:11,795 [dl_trainer.py:634] INFO train iter: 12544, num_batches_per_epoch: 98
2022-08-04 05:07:11,796 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 97.544643, lr: 0.001000, avg loss: 0.073877
2022-08-04 05:07:13,436 [dl_trainer.py:822] INFO Epoch 128, lr: 0.001000, val loss: 0.394019, val top-1 acc: 89.689490, top-5 acc: 99.621815
2022-08-04 05:07:13,918 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080935, Speed: 790.753997 images/s
2022-08-04 05:07:14,193 [dl_trainer.py:731] WARNING [128][12560/   98][rank:0] loss: 0.034, average forward (0.009497) and backward (0.023844) time: 0.076788, iotime: 0.001627 
2022-08-04 05:07:15,828 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.047744, Speed: 1340.495294 images/s
2022-08-04 05:07:16,093 [dl_trainer.py:731] WARNING [128][12600/   98][rank:0] loss: 0.035, average forward (0.008947) and backward (0.023393) time: 0.034112, iotime: 0.001544 
2022-08-04 05:07:16,603 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:16,603 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:18,298 [dl_trainer.py:731] WARNING [128][12640/   98][rank:0] loss: 0.024, average forward (0.009821) and backward (0.022625) time: 0.038870, iotime: 0.006182 
2022-08-04 05:07:18,405 [dl_trainer.py:634] INFO train iter: 12642, num_batches_per_epoch: 98
2022-08-04 05:07:18,405 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 97.624362, lr: 0.001000, avg loss: 0.071721
2022-08-04 05:07:20,052 [dl_trainer.py:822] INFO Epoch 129, lr: 0.001000, val loss: 0.395940, val top-1 acc: 89.779061, top-5 acc: 99.671576
2022-08-04 05:07:20,504 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082009, Speed: 780.406154 images/s
2022-08-04 05:07:21,921 [dl_trainer.py:731] WARNING [129][12680/   98][rank:0] loss: 0.047, average forward (0.008863) and backward (0.021434) time: 0.073335, iotime: 0.001580 
2022-08-04 05:07:22,468 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049094, Speed: 1303.625806 images/s
2022-08-04 05:07:23,303 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:23,303 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:24,111 [dl_trainer.py:731] WARNING [129][12720/   98][rank:0] loss: 0.026, average forward (0.009394) and backward (0.021088) time: 0.037163, iotime: 0.006399 
2022-08-04 05:07:25,159 [dl_trainer.py:634] INFO train iter: 12740, num_batches_per_epoch: 98
2022-08-04 05:07:25,159 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 97.464923, lr: 0.001000, avg loss: 0.073547
2022-08-04 05:07:26,828 [dl_trainer.py:822] INFO Epoch 130, lr: 0.001000, val loss: 0.399930, val top-1 acc: 89.699443, top-5 acc: 99.611863
2022-08-04 05:07:27,238 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.083655, Speed: 765.047399 images/s
2022-08-04 05:07:27,802 [dl_trainer.py:731] WARNING [130][12760/   98][rank:0] loss: 0.099, average forward (0.009845) and backward (0.024302) time: 0.078481, iotime: 0.001714 
2022-08-04 05:07:29,229 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049763, Speed: 1286.101795 images/s
2022-08-04 05:07:29,820 [dl_trainer.py:731] WARNING [130][12800/   98][rank:0] loss: 0.079, average forward (0.008791) and backward (0.023801) time: 0.034459, iotime: 0.001622 
2022-08-04 05:07:30,041 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:30,042 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:31,856 [dl_trainer.py:634] INFO train iter: 12838, num_batches_per_epoch: 98
2022-08-04 05:07:31,857 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 97.512755, lr: 0.001000, avg loss: 0.071877
2022-08-04 05:07:33,512 [dl_trainer.py:822] INFO Epoch 131, lr: 0.001000, val loss: 0.394477, val top-1 acc: 89.669586, top-5 acc: 99.621815
2022-08-04 05:07:33,610 [dl_trainer.py:731] WARNING [131][12840/   98][rank:0] loss: 0.087, average forward (0.009684) and backward (0.021664) time: 0.079410, iotime: 0.006339 
2022-08-04 05:07:33,884 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081647, Speed: 783.866345 images/s
2022-08-04 05:07:35,606 [dl_trainer.py:731] WARNING [131][12880/   98][rank:0] loss: 0.012, average forward (0.008608) and backward (0.025069) time: 0.035425, iotime: 0.001517 
2022-08-04 05:07:35,853 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049192, Speed: 1301.015429 images/s
2022-08-04 05:07:36,611 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:36,611 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:37,691 [dl_trainer.py:731] WARNING [131][12920/   98][rank:0] loss: 0.015, average forward (0.009880) and backward (0.021269) time: 0.037252, iotime: 0.005828 
2022-08-04 05:07:38,498 [dl_trainer.py:634] INFO train iter: 12936, num_batches_per_epoch: 98
2022-08-04 05:07:38,498 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 97.480867, lr: 0.001000, avg loss: 0.072037
2022-08-04 05:07:40,135 [dl_trainer.py:822] INFO Epoch 132, lr: 0.001000, val loss: 0.397876, val top-1 acc: 89.699443, top-5 acc: 99.671576
2022-08-04 05:07:40,498 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.081464, Speed: 785.624611 images/s
2022-08-04 05:07:41,411 [dl_trainer.py:731] WARNING [132][12960/   98][rank:0] loss: 0.105, average forward (0.010230) and backward (0.026661) time: 0.080810, iotime: 0.001785 
2022-08-04 05:07:42,473 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049371, Speed: 1296.300555 images/s
2022-08-04 05:07:43,258 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:43,258 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:43,517 [dl_trainer.py:731] WARNING [132][13000/   98][rank:0] loss: 0.027, average forward (0.009721) and backward (0.021918) time: 0.037860, iotime: 0.005958 
2022-08-04 05:07:45,184 [dl_trainer.py:634] INFO train iter: 13034, num_batches_per_epoch: 98
2022-08-04 05:07:45,184 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 97.448980, lr: 0.001000, avg loss: 0.076682
2022-08-04 05:07:46,798 [dl_trainer.py:822] INFO Epoch 133, lr: 0.001000, val loss: 0.394162, val top-1 acc: 89.848726, top-5 acc: 99.631768
2022-08-04 05:07:47,054 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080339, Speed: 796.619774 images/s
2022-08-04 05:07:47,089 [dl_trainer.py:731] WARNING [133][13040/   98][rank:0] loss: 0.018, average forward (0.010269) and backward (0.024605) time: 0.077335, iotime: 0.001744 
2022-08-04 05:07:49,047 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049821, Speed: 1284.602058 images/s
2022-08-04 05:07:49,088 [dl_trainer.py:731] WARNING [133][13080/   98][rank:0] loss: 0.197, average forward (0.009813) and backward (0.025684) time: 0.037416, iotime: 0.001666 
2022-08-04 05:07:49,838 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:49,838 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:51,236 [dl_trainer.py:731] WARNING [133][13120/   98][rank:0] loss: 0.085, average forward (0.009610) and backward (0.023130) time: 0.039147, iotime: 0.006144 
2022-08-04 05:07:51,883 [dl_trainer.py:634] INFO train iter: 13132, num_batches_per_epoch: 98
2022-08-04 05:07:51,883 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 97.496811, lr: 0.001000, avg loss: 0.070439
2022-08-04 05:07:53,527 [dl_trainer.py:822] INFO Epoch 134, lr: 0.001000, val loss: 0.394955, val top-1 acc: 89.828822, top-5 acc: 99.621815
2022-08-04 05:07:53,756 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082599, Speed: 774.823310 images/s
2022-08-04 05:07:54,966 [dl_trainer.py:731] WARNING [134][13160/   98][rank:0] loss: 0.072, average forward (0.010855) and backward (0.026103) time: 0.080716, iotime: 0.001820 
2022-08-04 05:07:55,791 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050868, Speed: 1258.157373 images/s
2022-08-04 05:07:56,577 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:07:56,577 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:07:57,141 [dl_trainer.py:731] WARNING [134][13200/   98][rank:0] loss: 0.091, average forward (0.010953) and backward (0.025961) time: 0.043104, iotime: 0.005916 
2022-08-04 05:07:58,664 [dl_trainer.py:634] INFO train iter: 13230, num_batches_per_epoch: 98
2022-08-04 05:07:58,664 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 97.305485, lr: 0.001000, avg loss: 0.075543
2022-08-04 05:08:00,331 [dl_trainer.py:822] INFO Epoch 135, lr: 0.001000, val loss: 0.396679, val top-1 acc: 89.888535, top-5 acc: 99.631768
2022-08-04 05:08:00,493 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082456, Speed: 776.171425 images/s
2022-08-04 05:08:00,844 [dl_trainer.py:731] WARNING [135][13240/   98][rank:0] loss: 0.027, average forward (0.010003) and backward (0.023894) time: 0.077539, iotime: 0.001654 
2022-08-04 05:08:02,514 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.050523, Speed: 1266.762327 images/s
2022-08-04 05:08:02,869 [dl_trainer.py:731] WARNING [135][13280/   98][rank:0] loss: 0.077, average forward (0.008854) and backward (0.021969) time: 0.032584, iotime: 0.001519 
2022-08-04 05:08:03,373 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:08:03,373 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:08:05,025 [dl_trainer.py:731] WARNING [135][13320/   98][rank:0] loss: 0.090, average forward (0.010018) and backward (0.023054) time: 0.039060, iotime: 0.005725 
2022-08-04 05:08:05,419 [dl_trainer.py:634] INFO train iter: 13328, num_batches_per_epoch: 98
2022-08-04 05:08:05,420 [dl_trainer.py:635] INFO Epoch 136, avg train acc: 97.321429, lr: 0.001000, avg loss: 0.077380
2022-08-04 05:08:07,091 [dl_trainer.py:822] INFO Epoch 136, lr: 0.001000, val loss: 0.399525, val top-1 acc: 89.759156, top-5 acc: 99.611863
2022-08-04 05:08:07,215 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.082451, Speed: 776.218125 images/s
2022-08-04 05:08:08,671 [dl_trainer.py:731] WARNING [136][13360/   98][rank:0] loss: 0.094, average forward (0.009435) and backward (0.023507) time: 0.077122, iotime: 0.001608 
2022-08-04 05:08:09,190 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.049362, Speed: 1296.533624 images/s
2022-08-04 05:08:09,989 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:08:09,989 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:08:10,801 [dl_trainer.py:731] WARNING [136][13400/   98][rank:0] loss: 0.074, average forward (0.009424) and backward (0.023517) time: 0.039052, iotime: 0.005826 
2022-08-04 05:08:12,037 [dl_trainer.py:634] INFO train iter: 13426, num_batches_per_epoch: 98
2022-08-04 05:08:12,038 [dl_trainer.py:635] INFO Epoch 137, avg train acc: 97.528699, lr: 0.001000, avg loss: 0.075355
2022-08-04 05:08:13,716 [dl_trainer.py:822] INFO Epoch 137, lr: 0.001000, val loss: 0.400702, val top-1 acc: 89.818869, top-5 acc: 99.611863
2022-08-04 05:08:13,776 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.080436, Speed: 795.668312 images/s
2022-08-04 05:08:14,370 [dl_trainer.py:731] WARNING [137][13440/   98][rank:0] loss: 0.095, average forward (0.010875) and backward (0.023178) time: 0.078055, iotime: 0.001684 
2022-08-04 05:08:15,643 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.046658, Speed: 1371.676556 images/s
2022-08-04 05:08:16,235 [dl_trainer.py:731] WARNING [137][13480/   98][rank:0] loss: 0.061, average forward (0.008976) and backward (0.023975) time: 0.034764, iotime: 0.001581 
2022-08-04 05:08:16,422 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:08:16,422 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:08:18,441 [dl_trainer.py:731] WARNING [137][13520/   98][rank:0] loss: 0.062, average forward (0.011385) and backward (0.025231) time: 0.042954, iotime: 0.006059 
2022-08-04 05:08:18,647 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.052678, Speed: 1214.928831 images/s
2022-08-04 05:08:18,648 [dl_trainer.py:634] INFO train iter: 13524, num_batches_per_epoch: 98
2022-08-04 05:08:18,648 [dl_trainer.py:635] INFO Epoch 138, avg train acc: 97.592474, lr: 0.001000, avg loss: 0.072966
2022-08-04 05:08:20,302 [dl_trainer.py:822] INFO Epoch 138, lr: 0.001000, val loss: 0.394859, val top-1 acc: 89.918392, top-5 acc: 99.631768
2022-08-04 05:08:22,084 [dl_trainer.py:731] WARNING [138][13560/   98][rank:0] loss: 0.043, average forward (0.009402) and backward (0.024490) time: 0.077600, iotime: 0.001603 
2022-08-04 05:08:22,295 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.091181, Speed: 701.900630 images/s
2022-08-04 05:08:23,050 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:08:23,050 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2022-08-04 05:08:24,178 [dl_trainer.py:731] WARNING [138][13600/   98][rank:0] loss: 0.059, average forward (0.009401) and backward (0.023681) time: 0.039446, iotime: 0.006123 
2022-08-04 05:08:25,250 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.051828, Speed: 1234.863125 images/s
2022-08-04 05:08:25,299 [dl_trainer.py:634] INFO train iter: 13622, num_batches_per_epoch: 98
2022-08-04 05:08:25,300 [dl_trainer.py:635] INFO Epoch 139, avg train acc: 97.448980, lr: 0.001000, avg loss: 0.076176
2022-08-04 05:08:26,932 [dl_trainer.py:822] INFO Epoch 139, lr: 0.001000, val loss: 0.401374, val top-1 acc: 89.769108, top-5 acc: 99.661624
2022-08-04 05:08:27,799 [dl_trainer.py:731] WARNING [139][13640/   98][rank:0] loss: 0.050, average forward (0.009827) and backward (0.023742) time: 0.076347, iotime: 0.001656 
2022-08-04 05:08:28,849 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.089954, Speed: 711.476492 images/s
2022-08-04 05:08:29,647 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2022-08-04 05:08:29,647 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
